{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNFm2CsCReTE7f00MC607Mi"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git tqdm\n",
        "!pip install -U datasets huggingface_hub fsspec jiwer"
      ],
      "metadata": {
        "id": "tu9NLEr1p9Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"numpy<2.0\" --upgrade\n",
        "\n",
        "# Install FFmpeg for audio processing (fixes channel issues)\n",
        "!apt-get update -qq\n",
        "!apt-get install -y ffmpeg\n",
        "\n",
        "!pip install openai-whisper\n",
        "!pip install datasets==3.5.0\n",
        "!pip install torchcodec  # Required for audio decoding\n",
        "# Alternative audio libraries if torchcodec fails\n",
        "!pip install librosa soundfile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GdhKwBQHL7Di",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1753776190549,
          "user_tz": -180,
          "elapsed": 29900,
          "user": {
            "displayName": "MD.RAFIUL BISWAS",
            "userId": "15801257354586832441"
          }
        },
        "outputId": "bd8d93a0-8044-47d5-d5fc-45d686bf901b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy<2.0\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.3.2\n",
            "    Uninstalling numpy-2.3.2:\n",
            "      Successfully uninstalled numpy-2.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 3.5.0 requires fsspec[http]<=2024.12.0,>=2023.1.0, but you have fsspec 2025.7.0 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "ac78e489a17a4039b8e3ea2725d24172"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 43 not upgraded.\n",
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.11/dist-packages (20250625)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (1.26.4)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.3.1)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton>=2->openai-whisper) (80.9.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (4.14.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2025.7.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.11.1.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.7.14)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Requirement already satisfied: datasets==3.5.0 in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (0.70.16)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets==3.5.0)\n",
            "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (3.12.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (0.33.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.0) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.0) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets==3.5.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets==3.5.0) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.5.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.5.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.5.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.5.0) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.5.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.5.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.5.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.5.0) (1.17.0)\n",
            "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "Installing collected packages: fsspec\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.7.0\n",
            "    Uninstalling fsspec-2025.7.0:\n",
            "      Successfully uninstalled fsspec-2025.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fsspec-2024.12.0\n",
            "Requirement already satisfied: torchcodec in /usr/local/lib/python3.11/dist-packages (0.5)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.14.1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --force-reinstall\n",
        "!pip install torchcodec --force-reinstall\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JsKOeEvMe5b3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1753775974629,
          "user_tz": -180,
          "elapsed": 155438,
          "user": {
            "displayName": "MD.RAFIUL BISWAS",
            "userId": "15801257354586832441"
          }
        },
        "outputId": "08bcc303-38e4-4fd9-dcbb-cbc412fc8e68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch\n",
            "  Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio\n",
            "  Downloading torchaudio-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting filelock (from torch)\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting typing-extensions>=4.10.0 (from torch)\n",
            "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch)\n",
            "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n",
            "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.3.1 (from torch)\n",
            "  Downloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting setuptools>=40.8.0 (from triton==3.3.1->torch)\n",
            "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting numpy (from torchvision)\n",
            "  Downloading numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
            "  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
            "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (821.2 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m129.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m109.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl (7.5 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m132.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m119.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m133.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m137.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Downloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m199.6/199.6 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m123.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "Installing collected packages: nvidia-cusparselt-cu12, mpmath, typing-extensions, sympy, setuptools, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.3.0\n",
            "    Uninstalling mpmath-1.3.0:\n",
            "      Successfully uninstalled mpmath-1.3.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.14.1\n",
            "    Uninstalling typing_extensions-4.14.1:\n",
            "      Successfully uninstalled typing_extensions-4.14.1\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.3.0\n",
            "    Uninstalling pillow-11.3.0:\n",
            "      Successfully uninstalled pillow-11.3.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.5.147\n",
            "    Uninstalling nvidia-curand-cu12-10.3.5.147:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
            "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.5\n",
            "    Uninstalling networkx-3.5:\n",
            "      Successfully uninstalled networkx-3.5\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.12.0\n",
            "    Uninstalling fsspec-2024.12.0:\n",
            "      Successfully uninstalled fsspec-2024.12.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.18.0\n",
            "    Uninstalling filelock-3.18.0:\n",
            "      Successfully uninstalled filelock-3.18.0\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
            "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "datasets 3.5.0 requires fsspec[http]<=2024.12.0,>=2023.1.0, but you have fsspec 2025.7.0 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.2 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.7.0 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.2 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.2 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 filelock-3.18.0 fsspec-2025.7.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-2.3.2 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 pillow-11.3.0 setuptools-80.9.0 sympy-1.14.0 torch-2.7.1 torchaudio-2.7.1 torchvision-0.22.1 triton-3.3.1 typing-extensions-4.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "_distutils_hack",
                  "filelock",
                  "fsspec",
                  "markupsafe",
                  "mpmath",
                  "numpy",
                  "setuptools",
                  "sympy",
                  "torch",
                  "torchaudio",
                  "torchgen",
                  "torio",
                  "triton"
                ]
              },
              "id": "7632b257b7b8477f8ce6a8d001f910df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchcodec\n",
            "  Using cached torchcodec-0.5-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
            "Using cached torchcodec-0.5-cp311-cp311-manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "Installing collected packages: torchcodec\n",
            "  Attempting uninstall: torchcodec\n",
            "    Found existing installation: torchcodec 0.5\n",
            "    Uninstalling torchcodec-0.5:\n",
            "      Successfully uninstalled torchcodec-0.5\n",
            "Successfully installed torchcodec-0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3g0GbyIgIuRl",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1753776313371,
          "user_tz": -180,
          "elapsed": 6076,
          "user": {
            "displayName": "MD.RAFIUL BISWAS",
            "userId": "15801257354586832441"
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ce434ec-350a-4669-82e1-a7e5d7390e3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 torchcodec available\n",
            "\u2705 All libraries imported successfully\n",
            "PyTorch version: 2.7.1+cu126\n",
            "Torchaudio version: 2.7.1+cu126\n",
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim.lr_scheduler import LinearLR\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "import whisper\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm.notebook import tqdm\n",
        "import librosa\n",
        "import warnings\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import time  # Added for timing training\n",
        "\n",
        "# Import audio processing libraries\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "try:\n",
        "    import torchcodec\n",
        "    print(\"\u2705 torchcodec available\")\n",
        "except ImportError:\n",
        "    print(\"\u2139\ufe0f  torchcodec not available, using alternatives\")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"\u2705 All libraries imported successfully\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Torchaudio version: {torchaudio.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(new_session=False)"
      ],
      "metadata": {
        "id": "CmLeOfnGJgnO",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1753776318502,
          "user_tz": -180,
          "elapsed": 1680,
          "user": {
            "displayName": "MD.RAFIUL BISWAS",
            "userId": "15801257354586832441"
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "33087442eb004417b846836414910500",
            "c6b21f32fb2f4be5a921b54289fdac3c",
            "ea6d110981a14311be36c8bbdee0072f",
            "ecfef0e5c4c74f80932449c89a3585f6",
            "fcf9b20093b64db5b0fbb8ec6d9f5d30",
            "f3e99c226dfe45ceacb2df8e5882d61b",
            "688382542f1e4589b5832e161aad955e",
            "c6fcabfbed724e22b0b9e8b1c4fae746",
            "aebcd98182e840a48a44bd9cc4eb4aed",
            "e5e1e3f5094246c5b2fe0ae69dac04aa",
            "ca28a44d60b24da8b2450c61ca5d309e",
            "607c130d063746a69ae4948ec6adbdb9",
            "94e3f194a8fd4f8e99b6076d51db3f2b",
            "46baad0fbfd14e53a79ed85e3e782d48",
            "833e28d6d959448b8ad4509eb3edf655",
            "f7f5247324cf4798a27fcfcd3cbe4578",
            "ecb0f901ab8c4e77a989d97e86acde26",
            "d020b64df7964aa091abb7de986e7117",
            "98a0a850d41047448655eaa6c8de1ed7",
            "c67f3066359b42deb8bbf5b8a5338a53"
          ]
        },
        "outputId": "eb87b71d-4bb4-4715-c78d-05f9a21f1834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv\u2026"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33087442eb004417b846836414910500"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== CELL 4: Load Datasets (Memory Efficient Version) =====\n",
        "\n",
        "# Load training/validation dataset\n",
        "print(\"Loading training/validation dataset...\")\n",
        "ds = load_dataset(\"UBC-NLP/NADI2025_subtask2_SLID\")\n",
        "ds.set_format(\"torch\")\n",
        "\n",
        "print(\"Dataset structure:\")\n",
        "print(ds)\n",
        "\n",
        "# Define dialects but DON'T load test data yet (memory efficient approach)\n",
        "print(\"\\nSetting up test dataset configuration...\")\n",
        "dialects = ['Algeria', 'Egypt', 'Jordan', 'Mauritania', 'Morocco', 'Palestine', 'UAE', 'Yemen']\n",
        "\n",
        "# Instead of loading all data, we'll check dataset sizes first\n",
        "print(\"Checking test dataset sizes...\")\n",
        "total_test_samples = 0\n",
        "dialect_sizes = {}\n",
        "\n",
        "for dialect in dialects:\n",
        "    try:\n",
        "        print(f\"Checking {dialect}...\")\n",
        "        # Load just to get size info, then delete\n",
        "        dialect_ds = load_dataset(\"UBC-NLP/NADI2025_subtask2_ASR_Test\", dialect)\n",
        "        size = len(dialect_ds['test'])\n",
        "        dialect_sizes[dialect] = size\n",
        "        total_test_samples += size\n",
        "        print(f\"  {dialect}: {size:,} samples\")\n",
        "\n",
        "        # Clear from memory immediately\n",
        "        del dialect_ds\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  \u274c Error checking {dialect}: {e}\")\n",
        "        dialect_sizes[dialect] = 0\n",
        "\n",
        "print(f\"\\nTotal test samples across all dialects: {total_test_samples:,}\")\n",
        "print(f\"Memory efficient approach: Will process each dialect separately during transcription\")\n",
        "\n",
        "# Store configuration for later use\n",
        "TEST_CONFIG = {\n",
        "    'dialects': dialects,\n",
        "    'sizes': dialect_sizes,\n",
        "    'total_samples': total_test_samples\n",
        "}\n",
        "\n",
        "print(\"\u2705 Dataset configuration complete - using memory-efficient processing\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f82c92fb76a5444c9c021c2176da4b9e",
            "ea424737a4334e1aaac787ed42399461",
            "db7a5f0253464238b1a72f5928fbee4d",
            "7427ef529b4c4882b18154b3822974ba",
            "d9d86d291bf948538f976e61d1b323c9",
            "0d9ff470de25447190eb968966d460ec",
            "0cea7b9127b84578a3374c63b53e74ea",
            "13b168d2e50b4c9da2a01f20050d9a64",
            "7eadbc4eec564b37923255d5dc3030a4",
            "8f91a957fbc746e5a9ff6b1cb82624c8",
            "bb521d623c434a598ba83efbfb759755",
            "3e62a695c4e64a7e9fdd6aa595d6bb30",
            "bf033df7c8d4478f879e37beb6bb3d2c",
            "bfac393f80fa421aa4ea7f291c8c6122",
            "3edd872d9fee44aab4bd24f64dc87528",
            "3f5ded2b82874a1ba5bd065318d21b55",
            "313b3ebc54ee4a0883eeaa43f8090906",
            "89b9a145672f43569b6a50fa3800fe07",
            "410217bedcde463eb5aeb7a476a3e9d0",
            "d9a070e54e5e4cc0846d227d1480d407",
            "2eb3e38864b34a9fa36ea430a9d5369f",
            "a99a71c816764efcb1ae7212af04c41d",
            "f41ccc07ce50436e8f432e151ef0511a",
            "71c6ee67f20841b99b873d7b513b4eb0",
            "5d37645eaa994715b7c83f9bfd5b1a9d",
            "a40204d83d9e4bfcb17be35a2de38dba",
            "0b61dfd9936a4c2fb27de129d21c6d1f",
            "249259fb4ade4913b788f7c0eb34b4bd",
            "94f8de86027644b1b57abcbd1cab62c8",
            "dc749aa4528e4371a28232db7774a7b6",
            "1bb2c6629c9b4ee0ad1783cc13a68b94",
            "38a0715d9046429eabe9927cc59e4795",
            "53452f57b0634220813a4dee76bf321f",
            "6b1023e8a4b04baa96027a8e1bc84f6f",
            "3d1e4e2fed6b4c53992b038019ad9b94",
            "66433d4a66184f4eb6f2639560238493",
            "95318e2ce8714d19922e4cf6113d423e",
            "940b8171788c4bf4a5469a7ce88ceaa4",
            "d4d9da560f5e4af185aa25799b51fd54",
            "4941c7034f4d413abeff78b880ee7127",
            "fc93584276c4435d9ffd3a370aa3a0d5",
            "fa98e034a0e84e61b38aea0561ca3eed",
            "3717b19a152b4c91add9f617ea7d7cb8",
            "0096d760123648fca238342d0cad6b39",
            "05cd084c22484229b960fd987ebeb807",
            "b75d630f763c4b31a4dce46020930889",
            "5ff36610ffe448ec8f47396e4ffae0bf",
            "8504f7f325d24a1ba0ba3b2aab43ad1d",
            "126aef3b91fb473e8e9741a281b09334",
            "d5ede370eb4f46faa6ae460de498e5f5",
            "13f4b4847fec474da57be6cb53797616",
            "4acb6d7bb6254f788d4a03bb20316510",
            "76ac16a29db4479faeead34b1b1b420a",
            "fdb91bc21df24b44bd824f23213dc07a",
            "621123ecebf24d028d503a7e29e821a8",
            "ed1d169cd0fc42298bf48b29252badbd",
            "8ec7f0456a874bd9b95a28596ce9b0fa",
            "141192b6dbc9439192db595c1efcfcf1",
            "9a93c7880bbe42dfa18877ec0079fd6a",
            "a29a06ff7cd24b0ea5aad57a10fe29de",
            "8672bbd9b5f649b3aa128e72c6757d71",
            "0c030e4457404e2ba85a8601e7802a2e",
            "bd2dd6ad35a6412ba5e03d07e6c2bbc9",
            "f197c4d7ab5b42128f101a3910e8aca8",
            "ce6d941b03304304967b38653e8127d5",
            "23a6dda1164448928a8f131574b8b83e",
            "c779f0e03ba04672899d4d49eb493377",
            "e20d34e560344adc814e38b7c526a4f6",
            "2fb0694ef2174d1e92723e0632902d40",
            "953a0d127c9f4f359af7f628386248d5",
            "024522a87e5c48b7937a90fdbc1ff33f",
            "26158ed45d114ceeabfa5f1fc9f4e156",
            "6f8e341214c042729fe4a70447ba30e4",
            "4242eb01050145538c3d7de905a747b1",
            "7b0d20b67f864e398816166e334d96ad",
            "531e2c2ce73e41ecbaeb2d0685355b3e",
            "f91ac509d127405daf8f925291cabf13",
            "066e4ebaa9ee46e5b5a8cedbc0eb8379",
            "8bb4c6451f5a46f19f77255c5f3c287d",
            "9bb3a1207608405696199aeafc596cb7",
            "5c3cc39dcb914ce5880d27f993c18ea9",
            "b7e8b5d958bb4cb0a63fe1f64455cac7",
            "b09697ce3c824ff6a2b0a813d9145b6c",
            "6ea1b865ac3241dea3a6f3cd95c26798",
            "b762385982d346b7ab3d81edc6c81228",
            "fc3273e0b1a24bbc8a7dbea60d68a969",
            "7943b5f7c37b4ccfb4d1e160d577292f",
            "fb75021b12e74b03bc4bb0a9e13e447e",
            "90fb1b326a6f4f6498e44a222e2d43ab",
            "80fb8c57657e44a1bbe97d1a6548f496",
            "5d968ba4e45e4ed49f76ab8c5b1f0bd8",
            "2d660a5e5e764291875e7c9ca8f39142",
            "f1995cd7617f40acb2c57fd8a10eb339",
            "cca07734c5ed4d4989655964404a7932",
            "a6f19fbf36e1400880988ece9212c1be",
            "bb4ae9aca72b43d6bd3db74a3dab26b4",
            "188766d215e54912b93559ea3d6bfe83",
            "9b15127e564c40c3891a976bdc2276cf",
            "72f91384add64924ac8452ce9469298d",
            "497c57b6e20c4f7da6c9df417880770c",
            "e2fb165bd80f416ca5e9468f051a07e1",
            "3550098099544308bef7d80a5e0d20ac",
            "13e5570b17664c3383d9684d9789eba6",
            "3f94e7e03a25449bb95b6a7fac76d18d",
            "fc2921dd48774029914c4a9328667980",
            "7cc36393748a408ca11d2e8b0297348e",
            "8d058c4695ff4e5fa5723aa246dd998d",
            "432be824c495458c9ce6e345ff2c711b",
            "53e8920f12914e6a92924830f9f5caa8",
            "288ffa96d65841dababdbf88591cd845",
            "9972c864fdaf4e0aaa8084b2d41a6cbc",
            "741298e2c9e64c02b91ce10a8bc37758",
            "acc16ee509c74da59fe3b2d4acb1d468",
            "2f263909365141969fffa8ba8e1e9d39",
            "f1a82c5a315b43cfac41c510eb06e312",
            "353a5c6596d444789e6ee2530218ffca",
            "e758f970ee7b4705ab7f15f929797ec9",
            "e1be0b9f604c4008bc71de75f74f53b3",
            "f59a50d758674153b1823efb877d0b3e",
            "58564c18559c4e7595567f3d9be60fd0",
            "58986cb6fb4c4268becdfa610a1225e7",
            "5b6eea6a0d8f46e491029c5259d8d207",
            "7aba5ee918424672a9e795beb50a04cf",
            "610a24c4b63f40a997ff74d207c11e29",
            "896ad7a1fb624cd4846187b7b94dc08f",
            "47c3033100724169bb137ca27d4e39fe",
            "d7bb78dea42e46fba2525abbfe94cf9c",
            "d1de219d64734986993a11f0914d5714",
            "31c4f0b5c5c143c29db5aec81e17a9fd",
            "4f82ec2c802443339d726dc0ad67ef51",
            "0800da197e4649a7af246a6022909015",
            "596013b38518466088775df19aebade7",
            "394411a1764144ec92662a974ca81873",
            "eac75aa666204fd3a72f97420f2d8ee0",
            "36a7b0c992a84d5fb30028f6021a10cf",
            "3d468fb07122409f845d293edcedf885",
            "21ccc45ba8304a8287978940966b5f30",
            "1c02962484b04b9bb0fd5e1b31edac5b",
            "9925adfd6a854aa4b35ef47d6b5eb956",
            "3bd10dccd1454aff8bac2659054fc4e0",
            "4b7d3579e22a4be284fe9a0584e0f76e",
            "dd862dc4a2d54c31919e3e2b45495e0b",
            "a68fcf9e58f2459a8e637a50b9ed97d9",
            "6965c6ba7d8c4756a4d977d9e1fcadcb",
            "75346055f4604256a94266ddefc94c5a",
            "222618f7dede404b870de16b0be9a244",
            "9cac093cc6fb49329c015d8d2d1e5e16",
            "46b1f3c88c7b4832a942a38a87471a1a",
            "de2db5fd80954220bab58792ea43e986",
            "60b54ff971044d8c8fb7b57c6b19d331",
            "b8e864bf46724d7d91673711bfa82c14",
            "d6e641ba20f443a5892611a0b4ebd776",
            "ee634591a7cb416b9cb86d8c81d7e3b8",
            "cd632367e91c4106a92cbb9e52c66f28",
            "6b34dd3a63ee46c8862fa29e16a9e270",
            "6fafc81439094437bc8722b22ed848ae",
            "b6a177075c50487f90e966394c44d8f9",
            "e49f9b02e87c44e4ba0fe8fb98fd759d",
            "69045917ca4c42eab6ba84d99f918165",
            "8f10f8568a4d462a80e3a01afee036e2",
            "2457b55b3c354802a2fb1736ac1c49ec",
            "a9a0c1b3d352444196d9c2dd0c43cf8a",
            "f7070f8c94af43bdbf95bf9b10f850ff",
            "444512a4706146a98f355b1d1d40f5ae",
            "4cdbc87b59c741c091d2da6e558af494",
            "f093d69a007e43bd837bfc6f8376e53c",
            "f7922152e1aa41a0baee16c7e7772070",
            "0d47d50ea5aa49c2a5f23d0c4175997b",
            "e2db886aa949475eaa46338c970686f5",
            "b4da9fde8a6249e1bc1d09d7c11b0d1b",
            "921083cbec3744518312bfc0de9bcc4d",
            "15a429254192404ca70e138291aadc9b",
            "0830bb1adb4040d0b097b7143c844950",
            "7300dbaaf10d445d81e5fd61f1529c3f",
            "55c58c5def9545b6b71c89d55e4c74bf",
            "70f2d798de4a4a259e4e5d401aae58a2",
            "8b5d5736681b48c89c0ad800825c6ab5",
            "caaebfdf856b4cfd926932af96a2fc3b",
            "55b4609ee519496fb6f1cdb363e4b4e4",
            "48cb4c2efd1a491a95e8f6f80c4b6e14",
            "32c934d9ea0f4fc48ad1da3ed2e66808",
            "82192e686c6648af8d70767d626edbbf",
            "ed58bd83ede14c87a5bebef6b115b0dd",
            "bc180a88f10949be8d0bf96910af13e3",
            "6c1b15789b22497295c5a668ef8f6539",
            "e8ff510c9b474219a9f925f25aad5a38",
            "92fede0b028e45fcaac7cab9187d82b6",
            "99cc377209c64d9ea6c67c9ace674c4d",
            "3899045e58f1441d99e9bcabe973ed90",
            "de08f91b04604b2eb9bddb68fa9d264c",
            "f0135212f56149c6b49ff5be279812ff",
            "dec03a57fc9940fda3843068162c1c48",
            "ab96eca229f645b09310134a11affe2e",
            "9c24b67cf1144d52a07759f9f1fd5f63",
            "3501626ba98c4bb28e0d225c62f5cce2",
            "7ad6243ca59c43439f1f64dd0c28a425",
            "5c1d94993c4340368925fff1e4a3a7c2",
            "21de44173bd44f9786cb144b2b76cbb9",
            "40da08ab138a4fa79e1bc0f4195443b0",
            "c29ec68a8c9d46d6af26dca4cd397a3d",
            "ba51c837342245698619bc53056b1bbc",
            "8595df5366fb4854aaf3844a0b06da6b",
            "f6efae59635446419b5605ca6dce09e7",
            "56365802ee5b4cabb77553f0f8c3bad3",
            "27be018b278e44728a844d8922a45b2f",
            "f970d33c70884e158db6087b5f205aec",
            "e3c355f471894a6388e084b75345861d",
            "d8fb1407140b4774942c737edfeaf42d",
            "4849db32911d419e99720df587b84d17",
            "a6c73ab77319416bb7192794696219b6",
            "8c760b605433472da29ecf64358c24d1",
            "862b913abfc543248cf28a1898b091e0",
            "317d3ba3c82b40e4b9082f35e173e677",
            "d9ce56ac70e44acb84c1fb296595fc81",
            "7a2ef9444b59455e827d1c6fad75f4f4",
            "f636b475aea0404db9dfb41beb72b2fc",
            "d94ceac277574afc8be27f00c9cac984",
            "0c0ee53166754601ac4c1d05c20f910d",
            "243825911eec404eb5a6990ec799f966",
            "53ea80ded3ae4104b3065582a831af7a",
            "c7356009c29b47dfb87bb82b046ec27c",
            "b5ad7bf9a644472ba5152119edc74a28",
            "b628e77bfad541e0842bf2edbee59a93",
            "cc38aa2a5fc547feafd4c27e949138b2",
            "087b369130484ad99a6ce1618bb04299",
            "cefcdb5951c7408f8950210ceb5dc035",
            "a2b9bbf01d724c16b8fb7a97a7d06e56",
            "88c7aa2b086746f98b95aa5abe045932",
            "22769a3d21934665939f9491bf05f5f9",
            "902c5429e5db4e3e80aa4dfd39938fd7",
            "ca9046f8d4154e9cb20c9cebe5a5c6cb",
            "1b9b191f776f4735b01637ce24f3f873",
            "dbbdb7c031f841f8a6a042fba88ada2b",
            "ed4793bf1a814853a56d0721fa2e2b1c",
            "2aff4183e4204a6ab09d252825fa1689",
            "5bf3bcc9d663485194020005902f9e16",
            "bc8b2d5770dc447d8aa40f99c17452e6",
            "cf155e431c1343cfb3ce321b1659f161",
            "0f1a02ed49184bcf95a603368c2c6d20",
            "1d9370e96bf74d80bec5f73692568043",
            "c1ad31e1f14a44d5bf772723299e0393",
            "f0703b8a35ea4eeeb1b7f2eb09e5b6a4",
            "63bc06f585054f4bb41e9251357978a1",
            "844aece8f4ba48b8b99642cac57a5e90",
            "10d6922224ce4557812af8ca6af9f1ac",
            "eedeb65e118d454b90f59f9da22151fd",
            "5c0a0abe4ebb43f3817c8f95156e2420",
            "0df0d0efdc3342c689486ba6beded8cb",
            "ffe065eeda5b4315b72e347de1f84e93",
            "ab7bda527a8a4a43ae738eb488da76ac",
            "280c534d1135497fb08569193757340a",
            "464f7f6d85634e05a1425ed10e15cf1a",
            "23784f16f95e4838a1794db6908c4173",
            "05dbb3bfc7e84c24a04d9421bc372461",
            "ad3d27fb159c4bb0b327629730e3835b",
            "64377348f99944518af880e34233b9c1",
            "3b77c2b5bc584e3887c41ffd1cc3f58d",
            "43caf7b057834c4898fbd2045ec45357",
            "0d124142247d4ced8121d6348470d18a",
            "eef18c203fc94631b960913ce430b425",
            "b2f6a3182cca402183a4760b85a4165d",
            "b178b382673749868e06aac169a053bf",
            "970025c110844d1a82fdcffdf4c29975",
            "895a8e57328c47b9b07a3f927b053980",
            "778dd0342b964d6fba01bf4fe3ee8f63",
            "fc8e944a929e48c383c15744ab479b81",
            "51fd9370135d4c4a85cf3f28686f1619",
            "8620dd3fe70844b982be6ab9f9027050",
            "9900124b043a42e5a58818ba097162c1",
            "801269e3647544ef83ea30c6874b2690",
            "b92973dee8ad4686b4b7347e626370da",
            "27d48ea13ee94d39a9c0400ea5b4cf0f",
            "a1307e94504f495ea6b552dd5d12c058",
            "097543ee2ff5441fabf5f0529a08b9b5",
            "fef2404f002347d4a531e719aa77ba94",
            "a0322d2add42485fa92364926de0d502",
            "2ea9f491d97843eaa0e253532a999e1a",
            "f20a7d7a90474d9aa746a6af42ad7d45",
            "e19083c1ec1646de89ba79d96312cbee",
            "32daefe0826a4befa067896cd8a7c70c",
            "bbe2fe0ff04e497fa1361ec5d7c26b4b",
            "2d3eee80b6114bb191f09b8be105fb6a",
            "a8ea555255aa40daa819f256fe31f233",
            "83c23d79407d46adb76bb1dc784139eb",
            "360215ac8a7444b59416006d03fbed90",
            "a17c136dc0034e72926174b0c6b7d109",
            "33b8065d513b424980e4417117ab78ef",
            "1955c47773c64e549e4ed4777efa59c5",
            "ebbba05a750b402d8c561d128b7811ef",
            "32e4483ad47e4a2ebc8e19c8b4f769b1",
            "355a7e34803e4648a948d6caa9f750e9",
            "0705ec14abbb4cb3b6d1795cadd72ebf",
            "329df4c340224b9c847c17baee19d1a4",
            "4abae5873ed04cca987f7cdb2c66c4bd",
            "78f47bb8c9e54c9284224b7a58387051",
            "8dde0152689946a9a11eb6aea0bab6ba",
            "d0a0fc93de0d49f598eae98c65e9c256",
            "c2200d68dcd84b689cc146fd7e1dc931",
            "19ffb574ea834f2489b806492b50b787",
            "854a0a5746f1445f9070b324e1a82943",
            "27aec8a6666e4ee695ab99c781a13a8b",
            "fc6639bc299f41fc8b887124832ae777",
            "73980678b4c846698dbee481385149a6",
            "bde4d3ad083240a284c2942e0a51b66f",
            "4b330af982864ba997076fe889801107",
            "0cc2c5bae66f47aaa4e6ddca0423cbc3",
            "e9b7662d2f24496da2826b94e51f9196",
            "9595ecef7d4644308c8f2dc2a1d2a041",
            "86715874c86e4d219e4c459e75afe8ff",
            "4de0c6b6b48647cbb63c7b166cf83f99",
            "3c10951bd82a447ba5a0916a8acbf40e",
            "e49a7806b81340c1a3082b98e964ad16",
            "7233ea3a37b940d6aa5536543f03a9c6",
            "182e9366dd57442a97e6aeae3452148c",
            "c6bfa026c26444c697d1a7785a7de2e9",
            "027282cccc404437a8bd8e1687b0eb51",
            "ddc0afe38b96479ba9905de0bc0ca40e",
            "e6b2553b64fd49ad9c4b96c0ca95f810",
            "38117a46e1ce46c4a31d0dfe241e8eb7",
            "bb0896f266be41f985a7769793a85921",
            "892a39bde6384121b419b119b3324087",
            "79973f875e084c6db9b4b0d19a9c8002",
            "5257eacbb85b43418eb708bc9aa56578",
            "ba1d40ba5e7e47d19831c38c332ab355",
            "a3cb31d85b4b4484b1d280f3d37301c1",
            "26f5c4a07e93419a9fd7e04fe27c5a89",
            "ea64718fce7e49a190582423f8b7f7e9",
            "c3bdf64e2ae145658c761a3e0fdf1acd",
            "70654bdbc9d148abb888a7654e3abe18",
            "45638867f40046e0a00a731eda12aa11",
            "c06331e4c9c341869d7fa57374f673d9",
            "e261e8aa70254bcea0dcc4405caf52c8",
            "d2ab0cbfa83f4f339b17946080f62ab3",
            "8b8e07e429aa4712923856c41a448fdf",
            "a5c16dbb15514be8971bc58c43ee3d6f",
            "5eed486d4e704967992b0fc9d082c2cb",
            "78df5f13ef894630be1ecfec42e87a0b",
            "c1043341a4e442528b33ab14665e3c6c",
            "50644e4c273648a7a2353b4bfcd182a1",
            "bbdac5b38b3c4a21a58e4c895ee73f35",
            "8541567036a74df49302d23d5c693345",
            "d9df9c3b25b547c0bf90a3934d8185aa",
            "f69b0daa9c2e48b6b58ddbf592c764c1",
            "52b9c46041ea45d089d786a8b24d0b10",
            "2a26d7ea83444b7c88ccae3fa063a12e",
            "a97d69690ae0495584f26ed4630f2f31",
            "47f8337443a64c39833da2f48629d5d2",
            "1b36cab4dbab4505a14bf6c999f523ac",
            "6fed4da0ae574dad87e503bf19082f6d",
            "7f381766d7a74c2e83a7ebe98b9bcdec",
            "6ca3cd2d23f74b07ac9e8c4645e18e55",
            "93a77ee4c10e4ff4b0ac0bd9ea7f8120",
            "4f59ff811bd0406489fc8caa99f79422",
            "e95a601bffb5428a8f2ad996d3858830",
            "b9ea1f8c3abe42ecb46f36743630e872",
            "24a3b09077d04e518887604785b3ba9d",
            "6c9bf18404554a1ea442043215678f3f",
            "4e8dc18479fa457a9dd67d6853f4d5ed",
            "5d7e2a38b12c4640ba9fad9ca5287bd8",
            "d40fdd1d7e8b4c5e91b544bbc828df56",
            "bbffa7735e4747ed9e57629b32bfb45f",
            "a10f7d6478884a499ee491c769b836aa",
            "3167984c57a74ceabdc18b30af3d5588",
            "0d2c03feb0f44422a7b3d3bd22eb91bf",
            "5b6c0dcbd61b4406a68a94e68c46d271",
            "eb06ccd629c74d6cafb0b7840a179075",
            "14c70152b2c642b2845520dc405b8191",
            "97b2c567e04a4c999bbddf372a22519a",
            "ec5c44575bf3443c812456d0a4b41dc9",
            "cda654699c0244008cfd0f7fd57c4742",
            "040cacff31014ec6acba46a2f606b695",
            "fd1c0ce00c3f47ab81d92fc46d5d1fee",
            "96a3fed5456b471aa51350583096b9e3",
            "293547ad12c049c7a0babfa43309fc59",
            "4219d486bcaa4b63ae88189f5c8f507b",
            "9efc56e645d043308df3d86619c436ba",
            "49c07daf543642c2a1bc1081e0aeaaf0",
            "790d2c10a55a4f3f863c832b83e2db56",
            "0e4ae9612bf04c989b58ef80479af477",
            "41ee32c08323470fa4c14d058f2133b6",
            "f3834431f09e45bd9dcdc9d326482142",
            "ff12000f867d4647a70fd67be7977d51",
            "293abe146c3240e5ae9f4b07ee9a269d",
            "f328c6975e4648d286d0ea31e6996fea",
            "0a4a5f2330f74aa2890290495acea420",
            "822f898493284b9a8746d15226e2edac",
            "0b28cd51d6744023a1915fc341e251b6",
            "d92f671fb959436d891971658521e100",
            "d9f942f35ba4402baf77961fee19b02e",
            "9c4bf53d2816446c99d0152243277cd0",
            "752ba3a1c4a344b5920d29b82b576247",
            "dcf8bcab6def43ed9e8134fb5554f79e",
            "ec57dd636b634af685bdc3dc3687c573",
            "0126576ec16e430fb239b77b48138559",
            "9663f73421424044b8e8d39e58611905",
            "6945d115f25d423a888d4d388077599a",
            "2454b7679b25488cbe7815f6e0ad395a",
            "8c9e5e3a70f14ec0a4a7a4321ba3abb8",
            "4f16d064ca944593bd3de8d98de13504",
            "1663abcb604a44abbc7de82962c57e6f",
            "8ddddeac903641f6a9cc08f7d0fc2eff",
            "ae7e72a75af549cdb2745bd5adceaab1",
            "3ed09d922df54f23836dce234b8637c9",
            "d6e6335186e044f1b879ba6b522aab82",
            "816c0d171929490f8290339b2b2158ca",
            "3c0028a88ffd40e0bfe23e825ee61995",
            "295b5301e9474681887ed12290a8e8b1",
            "bbc3a06c4b99426ab6aaca2a0b16173d",
            "f32f4a57158447f0976c4302a2dde5fe",
            "929e4acb9dd44c8eb803924f8f77c2a5",
            "bc7b4f426eed4fb6ac3caaeb95bd0ff6",
            "4280993f8a3d4bf8a0962b6eb59df896",
            "24b8bedb78034aef93685a513bddc5fc",
            "30f61f083ec84e7a9036e2e02fa429d6",
            "496b03c7d3d14282a5aeddf5d42e1277",
            "c33d967d1db8413b8111c53d4901da9d",
            "839cc37ab6aa45d4b508e6e172d2716b",
            "9e3975c596704a19a587e9b6785583c9",
            "99edb4e023f6490bbd83b3e7471a2735",
            "aaee9e2e53cf40439a7e139508d3c912",
            "751dbbe3bcbe4b71ad1cc5b8cef85f3a",
            "eb9fc5ade92840b2a91c0697952b6f01",
            "49ccbdca09204c599041efba69a57ada",
            "d57969049d1c4a21859bb9f8b763dd46",
            "989eaa14a3eb4466bfa43f2700b95c15",
            "cd1f9b41970140349dd38b64a84e65d5",
            "34d70a9bc99d42959b944d2f9ad5d197",
            "4d99ba4af23543bb973e6afa8c8088fd",
            "5a555f3a4c1344fa9685cbe6f9056cd1"
          ]
        },
        "id": "mZ1g238hKwCJ",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1753776445437,
          "user_tz": -180,
          "elapsed": 117937,
          "user": {
            "displayName": "MD.RAFIUL BISWAS",
            "userId": "15801257354586832441"
          }
        },
        "outputId": "1270eb1a-b97a-464c-bc65-0e122bc326cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training/validation dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f82c92fb76a5444c9c021c2176da4b9e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/456M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e62a695c4e64a7e9fdd6aa595d6bb30"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/416M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f41ccc07ce50436e8f432e151ef0511a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/469M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b1023e8a4b04baa96027a8e1bc84f6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/459M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "05cd084c22484229b960fd987ebeb807"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "validation-00000-of-00004.parquet:   0%|          | 0.00/420M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed1d169cd0fc42298bf48b29252badbd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "validation-00001-of-00004.parquet:   0%|          | 0.00/402M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c779f0e03ba04672899d4d49eb493377"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "validation-00002-of-00004.parquet:   0%|          | 0.00/462M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "066e4ebaa9ee46e5b5a8cedbc0eb8379"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "validation-00003-of-00004.parquet:   0%|          | 0.00/448M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90fb1b326a6f4f6498e44a222e2d43ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/12900 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "497c57b6e20c4f7da6c9df417880770c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/12700 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9972c864fdaf4e0aaa8084b2d41a6cbc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset structure:\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['ID', 'audio', 'country'],\n",
            "        num_rows: 12900\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['ID', 'audio', 'country'],\n",
            "        num_rows: 12700\n",
            "    })\n",
            "})\n",
            "\n",
            "Setting up test dataset configuration...\n",
            "Checking test dataset sizes...\n",
            "Checking Algeria...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b6eea6a0d8f46e491029c5259d8d207"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00000-of-00002.parquet:   0%|          | 0.00/214M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "394411a1764144ec92662a974ca81873"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00001-of-00002.parquet:   0%|          | 0.00/243M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6965c6ba7d8c4756a4d977d9e1fcadcb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/727 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b34dd3a63ee46c8862fa29e16a9e270"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Algeria: 727 samples\n",
            "Checking Egypt...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00000-of-00003.parquet:   0%|          | 0.00/338M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f093d69a007e43bd837bfc6f8376e53c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00001-of-00003.parquet:   0%|          | 0.00/329M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b5d5736681b48c89c0ad800825c6ab5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00002-of-00003.parquet:   0%|          | 0.00/370M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99cc377209c64d9ea6c67c9ace674c4d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/1600 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "40da08ab138a4fa79e1bc0f4195443b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Egypt: 1,600 samples\n",
            "Checking Jordan...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00000-of-00002.parquet:   0%|          | 0.00/236M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a6c73ab77319416bb7192794696219b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00001-of-00002.parquet:   0%|          | 0.00/220M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7356009c29b47dfb87bb82b046ec27c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/1600 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b9b191f776f4735b01637ce24f3f873"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Jordan: 1,600 samples\n",
            "Checking Mauritania...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00000-of-00003.parquet:   0%|          | 0.00/317M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63bc06f585054f4bb41e9251357978a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00001-of-00003.parquet:   0%|          | 0.00/327M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "05dbb3bfc7e84c24a04d9421bc372461"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00002-of-00003.parquet:   0%|          | 0.00/326M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "778dd0342b964d6fba01bf4fe3ee8f63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/1600 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0322d2add42485fa92364926de0d502"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Mauritania: 1,600 samples\n",
            "Checking Morocco...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00000-of-00002.parquet:   0%|          | 0.00/442M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33b8065d513b424980e4417117ab78ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00001-of-00002.parquet:   0%|          | 0.00/472M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2200d68dcd84b689cc146fd7e1dc931"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/1600 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86715874c86e4d219e4c459e75afe8ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Morocco: 1,600 samples\n",
            "Checking Palestine...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00000-of-00002.parquet:   0%|          | 0.00/358M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb0896f266be41f985a7769793a85921"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00001-of-00002.parquet:   0%|          | 0.00/368M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c06331e4c9c341869d7fa57374f673d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/900 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9df9c3b25b547c0bf90a3934d8185aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Palestine: 900 samples\n",
            "Checking UAE...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00000-of-00003.parquet:   0%|          | 0.00/343M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f59ff811bd0406489fc8caa99f79422"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00001-of-00003.parquet:   0%|          | 0.00/358M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d2c03feb0f44422a7b3d3bd22eb91bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00002-of-00003.parquet:   0%|          | 0.00/349M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4219d486bcaa4b63ae88189f5c8f507b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/1600 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "822f898493284b9a8746d15226e2edac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  UAE: 1,600 samples\n",
            "Checking Yemen...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00000-of-00002.parquet:   0%|          | 0.00/364M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2454b7679b25488cbe7815f6e0ad395a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00001-of-00002.parquet:   0%|          | 0.00/417M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bbc3a06c4b99426ab6aaca2a0b16173d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/1183 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99edb4e023f6490bbd83b3e7471a2735"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Yemen: 1,183 samples\n",
            "\n",
            "Total test samples across all dialects: 10,810\n",
            "Memory efficient approach: Will process each dialect separately during transcription\n",
            "\u2705 Dataset configuration complete - using memory-efficient processing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== CELL 5: Define Country Labels and Helper Functions =====\n",
        "\n",
        "# Country labels for classification\n",
        "countries = ['Algeria', 'Egypt', 'Jordan', 'Mauritania', 'Morocco', 'Palestine', 'UAE', 'Yemen']\n",
        "labels2id = {key:id for id, key in enumerate(countries)}\n",
        "id2labels = {id:key for key,id in labels2id.items()}\n",
        "\n",
        "print(\"Country labels mapping:\")\n",
        "for i, country in enumerate(countries):\n",
        "    print(f\"{i}: {country}\")\n",
        "\n",
        "# Helper function to check trainable parameters\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Print trainable parameters with detailed breakdown\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "\n",
        "    print(\"\\n=== MODEL PARAMETER BREAKDOWN ===\")\n",
        "    for name, param in model.named_parameters():\n",
        "        param_count = param.numel()\n",
        "        all_param += param_count\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param_count\n",
        "            print(f\"\u2713 TRAINABLE: {name:40} - {param_count:>10,} params\")\n",
        "        else:\n",
        "            print(f\"\u2717 FROZEN:    {name:40} - {param_count:>10,} params\")\n",
        "\n",
        "    print(f\"\\n=== SUMMARY ===\")\n",
        "    print(f\"Total parameters:     {all_param:>15,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:>15,}\")\n",
        "    print(f\"Frozen parameters:    {all_param - trainable_params:>15,}\")\n",
        "    print(f\"Trainable percentage: {100 * trainable_params / all_param:>14.2f}%\")\n",
        "    print(f\"Model size:           {all_param/1e6:>14.1f}M parameters\")\n",
        "    print(f\"Training only:        {trainable_params/1e6:>14.1f}M parameters\")\n",
        "\n",
        "    # Memory efficiency info\n",
        "    trainable_memory_mb = trainable_params * 4 / (1024 * 1024)  # Assuming float32\n",
        "    print(f\"Trainable memory:     {trainable_memory_mb:>14.1f}MB\")\n",
        "    print(\"=\" * 50)"
      ],
      "metadata": {
        "id": "EBJlK7UeLUVu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1753776445468,
          "user_tz": -180,
          "elapsed": 20,
          "user": {
            "displayName": "MD.RAFIUL BISWAS",
            "userId": "15801257354586832441"
          }
        },
        "outputId": "05740393-4c65-42a6-9c78-f05f57b625e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Country labels mapping:\n",
            "0: Algeria\n",
            "1: Egypt\n",
            "2: Jordan\n",
            "3: Mauritania\n",
            "4: Morocco\n",
            "5: Palestine\n",
            "6: UAE\n",
            "7: Yemen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== CELL 6: Define Cost Calculation Functions =====\n",
        "\n",
        "def llr(logits):\n",
        "    \"\"\"Calculate log likelihood ratios\"\"\"\n",
        "    classes = logits.shape[1]\n",
        "    l2 = logits.unsqueeze(dim=1)\n",
        "\n",
        "    l = logits.unsqueeze(dim=1)\n",
        "    l = l.repeat(1,8,1)\n",
        "    l2 = l2.repeat(1,8,1)\n",
        "    l2 = l2.permute((0,2,1))\n",
        "    dif = l-l2\n",
        "    e = torch.exp(dif)\n",
        "    for i in range(len(e)):\n",
        "        e[i].fill_diagonal_(0)\n",
        "    return -torch.log(torch.sum(e,dim=-1)/ (classes-1))\n",
        "\n",
        "def compute_actual_cost(scores, labels, p_target, c_miss=1, c_fa=1):\n",
        "    \"\"\"Actual Cost Function from the NIST Speech Recognition Evaluation tool package\"\"\"\n",
        "    beta = c_fa * (1 - p_target) / (c_miss * p_target)\n",
        "    decisions = (scores >= np.log(beta)).astype('i')\n",
        "    num_targets = np.sum(labels)\n",
        "    fp = np.sum(decisions * (1 - labels))\n",
        "    num_nontargets = np.sum(1 - labels)\n",
        "    fn = np.sum((1 - decisions) * labels)\n",
        "    fpr = fp / num_nontargets if num_nontargets > 0 else np.nan\n",
        "    fnr = fn / num_targets if num_targets > 0 else np.nan\n",
        "    return fnr + beta * fpr, fpr, fnr\n",
        "\n",
        "def compute_ave_cost(logits, labels, num_l=8):\n",
        "    \"\"\"Compute average cost\"\"\"\n",
        "    llratio = llr(logits)\n",
        "    llratio = llratio.numpy()\n",
        "    labels = labels.numpy()\n",
        "    order = labels.argsort()\n",
        "    labels.sort()\n",
        "    llratio = llratio[order]\n",
        "    indices = np.where(labels[:-1] != labels[1:])[0]\n",
        "    indices = np.append(indices, [-1])\n",
        "    one_hot = np.eye(8)[labels]\n",
        "    fprs = []\n",
        "    fnrs = []\n",
        "    last = 0\n",
        "    for i in indices:\n",
        "        _, fpr, fnr = compute_actual_cost(llratio[last:i], one_hot[last:i], 0.5)\n",
        "        fprs.append(fpr)\n",
        "        fnrs.append(fnr)\n",
        "        last = i\n",
        "    fpr = sum(fprs)/num_l\n",
        "    fnr = sum(fnrs)/num_l\n",
        "    cost = fpr+fnr\n",
        "    return cost, fpr, fnr"
      ],
      "metadata": {
        "id": "jWkB4472PY3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== CELL 7: Define Enhanced Collate Function with Torchaudio =====\n",
        "\n",
        "def col_fun_whisper_enhanced(samples):\n",
        "    \"\"\"\n",
        "    Enhanced collate function using torchaudio for better audio processing\n",
        "    Works for both training (with labels) and test (without labels) data\n",
        "    \"\"\"\n",
        "    arrays = []\n",
        "    countries = []\n",
        "    ids = []\n",
        "\n",
        "    for sample in samples:\n",
        "        # Get ID\n",
        "        if 'ID' in sample:\n",
        "            ids.append(sample['ID'])\n",
        "\n",
        "        # Get audio array\n",
        "        audio_array = sample['audio']['array']\n",
        "        sample_rate = sample['audio'].get('sampling_rate', 16000)\n",
        "\n",
        "        # Convert to tensor if needed\n",
        "        if isinstance(audio_array, np.ndarray):\n",
        "            audio_tensor = torch.from_numpy(audio_array).float()\n",
        "        else:\n",
        "            audio_tensor = audio_array.float()\n",
        "\n",
        "        # Ensure mono audio\n",
        "        if audio_tensor.dim() > 1:\n",
        "            audio_tensor = torch.mean(audio_tensor, dim=0)\n",
        "\n",
        "        # Resample to 16kHz if needed (Whisper expects 16kHz)\n",
        "        if sample_rate != 16000:\n",
        "            resampler = T.Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "            audio_tensor = resampler(audio_tensor)\n",
        "\n",
        "        # Normalize audio\n",
        "        if audio_tensor.abs().max() > 0:\n",
        "            audio_tensor = audio_tensor / audio_tensor.abs().max()\n",
        "\n",
        "        # Pad or truncate to reasonable length (30 seconds max for Whisper)\n",
        "        max_length = 16000 * 30  # 30 seconds at 16kHz\n",
        "        if len(audio_tensor) > max_length:\n",
        "            audio_tensor = audio_tensor[:max_length]\n",
        "\n",
        "        arrays.append(audio_tensor)\n",
        "\n",
        "        # Get country label if available (for training data)\n",
        "        if 'country' in sample:\n",
        "            countries.append(labels2id[sample['country']])\n",
        "\n",
        "    # Pad sequences to same length\n",
        "    lengths = [len(arr) for arr in arrays]\n",
        "    max_len = max(lengths)\n",
        "\n",
        "    padded_arrays = []\n",
        "    for arr in arrays:\n",
        "        if len(arr) < max_len:\n",
        "            padded = torch.nn.functional.pad(arr, (0, max_len - len(arr)))\n",
        "        else:\n",
        "            padded = arr\n",
        "        padded_arrays.append(padded)\n",
        "\n",
        "    packed = torch.stack(padded_arrays)\n",
        "\n",
        "    if countries:  # Training data\n",
        "        countries_tensor = torch.tensor(countries)\n",
        "        return packed, lengths, countries_tensor, ids\n",
        "    else:  # Test data\n",
        "        return packed, lengths, None, ids\n",
        "\n",
        "# Also keep the original collate function as backup\n",
        "def col_fun_whisper(samples):\n",
        "    \"\"\"\n",
        "    Original collate function (backup)\n",
        "    \"\"\"\n",
        "    arrays = []\n",
        "    countries = []\n",
        "    ids = []\n",
        "\n",
        "    for sample in samples:\n",
        "        # Get ID\n",
        "        if 'ID' in sample:\n",
        "            ids.append(sample['ID'])\n",
        "\n",
        "        # Get audio array\n",
        "        audio_array = sample['audio']['array']\n",
        "\n",
        "        # Ensure audio is in correct format for Whisper (16kHz, mono)\n",
        "        if isinstance(audio_array, torch.Tensor):\n",
        "            audio_array = audio_array.numpy()\n",
        "\n",
        "        # Pad or truncate to reasonable length (30 seconds max for Whisper)\n",
        "        max_length = 16000 * 30  # 30 seconds at 16kHz\n",
        "        if len(audio_array) > max_length:\n",
        "            audio_array = audio_array[:max_length]\n",
        "\n",
        "        arrays.append(torch.from_numpy(audio_array).float())\n",
        "\n",
        "        # Get country label if available (for training data)\n",
        "        if 'country' in sample:\n",
        "            countries.append(labels2id[sample['country']])\n",
        "\n",
        "    # Pad sequences to same length\n",
        "    lengths = [len(arr) for arr in arrays]\n",
        "    max_len = max(lengths)\n",
        "\n",
        "    padded_arrays = []\n",
        "    for arr in arrays:\n",
        "        if len(arr) < max_len:\n",
        "            padded = torch.nn.functional.pad(arr, (0, max_len - len(arr)))\n",
        "        else:\n",
        "            padded = arr\n",
        "        padded_arrays.append(padded)\n",
        "\n",
        "    packed = torch.stack(padded_arrays)\n",
        "\n",
        "    if countries:  # Training data\n",
        "        countries_tensor = torch.tensor(countries)\n",
        "        return packed, lengths, countries_tensor, ids\n",
        "    else:  # Test data\n",
        "        return packed, lengths, None, ids\n",
        "\n",
        "print(\"\u2705 Enhanced collate function with torchaudio support defined!\")\n",
        "print(\"\u2139\ufe0f  Will use enhanced version with audio preprocessing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9UIdSpcPerP",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1753776445503,
          "user_tz": -180,
          "elapsed": 17,
          "user": {
            "displayName": "MD.RAFIUL BISWAS",
            "userId": "15801257354586832441"
          }
        },
        "outputId": "1731fcf7-cb72-4288-8b56-7ae81dcfd335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Enhanced collate function with torchaudio support defined!\n",
            "\u2139\ufe0f  Will use enhanced version with audio preprocessing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== CELL 8: Define the Main Model Class =====\n",
        "\n",
        "# Import required modules for the model class\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import whisper\n",
        "\n",
        "class WhisperARBERTDialectClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Combined Whisper ASR + ARBERT/MARBERT text classification pipeline\n",
        "    Following efficient transfer learning approach like SpeechBrain\n",
        "    \"\"\"\n",
        "    def __init__(self, whisper_model_name=\"large-v3\", text_model_name=\"UBC-NLP/MARBERT\", num_classes=8):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load Whisper model for ASR\n",
        "        print(f\"Loading Whisper model: {whisper_model_name}\")\n",
        "        self.whisper_model = whisper.load_model(whisper_model_name)\n",
        "\n",
        "        # Load ARBERT/MARBERT model for text classification\n",
        "        print(f\"Loading text model: {text_model_name}\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(text_model_name)\n",
        "\n",
        "        # Load the base model without classification head (like SpeechBrain)\n",
        "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
        "\n",
        "        # Freeze Whisper model\n",
        "        for param in self.whisper_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Freeze text encoder (like SpeechBrain approach)\n",
        "        for param in self.text_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Replace classifier layer (similar to SpeechBrain approach)\n",
        "        # Get hidden size from the text encoder\n",
        "        hidden_size = self.text_encoder.config.hidden_size\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "        print(f\"Created new classifier: {hidden_size} -> {num_classes}\")\n",
        "        print(f\"Frozen Whisper parameters: {sum(p.numel() for p in self.whisper_model.parameters()):,}\")\n",
        "        print(f\"Frozen text encoder parameters: {sum(p.numel() for p in self.text_encoder.parameters()):,}\")\n",
        "        print(f\"Trainable classifier parameters: {sum(p.numel() for p in self.classifier.parameters()):,}\")\n",
        "\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def transcribe_audio(self, audio_tensor):\n",
        "        \"\"\"\n",
        "        Transcribe audio using Whisper\n",
        "        \"\"\"\n",
        "        # Convert tensor to numpy and ensure proper format\n",
        "        if isinstance(audio_tensor, torch.Tensor):\n",
        "            audio_numpy = audio_tensor.cpu().numpy()\n",
        "        else:\n",
        "            audio_numpy = audio_tensor\n",
        "\n",
        "        # Whisper expects audio in specific format\n",
        "        if audio_numpy.ndim > 1:\n",
        "            audio_numpy = audio_numpy.squeeze()\n",
        "\n",
        "        # Transcribe with Whisper\n",
        "        result = self.whisper_model.transcribe(\n",
        "          audio_numpy,\n",
        "          language='ar',\n",
        "          task='transcribe',\n",
        "          beam_size=1,    # Higher quality (default for v3)\n",
        "          best_of=1,      # Better results\n",
        "          temperature=0   # Deterministic\n",
        "        )\n",
        "\n",
        "        return result['text']\n",
        "\n",
        "    def classify_text(self, text):\n",
        "        \"\"\"\n",
        "        Classify text using frozen ARBERT/MARBERT encoder + new classifier\n",
        "        \"\"\"\n",
        "        # Tokenize text\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            return_tensors='pt',\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=512\n",
        "        )\n",
        "\n",
        "        # Move to device\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "        # Get embeddings from frozen text encoder\n",
        "        with torch.no_grad():\n",
        "            encoder_outputs = self.text_encoder(**inputs)\n",
        "            # Use [CLS] token embedding (first token)\n",
        "            embeddings = encoder_outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Pass through trainable classifier\n",
        "        logits = self.classifier(embeddings)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def forward(self, audio_batch, training=False, return_transcriptions=False):\n",
        "        \"\"\"\n",
        "        Forward pass: Audio -> Whisper -> Text -> Frozen ARBERT -> New Classifier -> Logits\n",
        "        \"\"\"\n",
        "        batch_size = audio_batch.shape[0]\n",
        "        all_logits = []\n",
        "        transcriptions = []\n",
        "\n",
        "        # During training, we need to handle gradients properly\n",
        "        if training:\n",
        "            # Pre-transcribe all audio to avoid gradient issues\n",
        "            transcripts = []\n",
        "            for i in range(batch_size):\n",
        "                audio_sample = audio_batch[i].cpu().numpy()\n",
        "                try:\n",
        "                    transcript = self.transcribe_audio(audio_sample)\n",
        "                    if not transcript.strip():\n",
        "                        transcript = \"\u0644\u0627 \u064a\u0648\u062c\u062f \u0646\u0635\"  # \"No text\" in Arabic\n",
        "                    transcripts.append(transcript)\n",
        "                    if return_transcriptions:\n",
        "                        transcriptions.append(transcript)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error transcribing audio {i}: {e}\")\n",
        "                    transcripts.append(\"\u0644\u0627 \u064a\u0648\u062c\u062f \u0646\u0635\")\n",
        "                    if return_transcriptions:\n",
        "                        transcriptions.append(\"\u0644\u0627 \u064a\u0648\u062c\u062f \u0646\u0635\")\n",
        "\n",
        "            # Batch process all transcripts\n",
        "            if transcripts:\n",
        "                inputs = self.tokenizer(\n",
        "                    transcripts,\n",
        "                    return_tensors='pt',\n",
        "                    truncation=True,\n",
        "                    padding=True,\n",
        "                    max_length=512\n",
        "                )\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "                # Get embeddings from frozen encoder\n",
        "                with torch.no_grad():\n",
        "                    encoder_outputs = self.text_encoder(**inputs)\n",
        "                    embeddings = encoder_outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "                # Pass through trainable classifier\n",
        "                final_logits = self.classifier(embeddings)\n",
        "            else:\n",
        "                final_logits = torch.randn(batch_size, 8).to(self.device)\n",
        "\n",
        "        else:\n",
        "            # Inference mode - process individually\n",
        "            for i in range(batch_size):\n",
        "                audio_sample = audio_batch[i].cpu().numpy()\n",
        "\n",
        "                try:\n",
        "                    # Transcribe audio\n",
        "                    transcript = self.transcribe_audio(audio_sample)\n",
        "\n",
        "                    # Handle empty transcriptions\n",
        "                    if not transcript.strip():\n",
        "                        transcript = \"\u0644\u0627 \u064a\u0648\u062c\u062f \u0646\u0635\"  # \"No text\" in Arabic\n",
        "\n",
        "                    # Store transcription if requested\n",
        "                    if return_transcriptions:\n",
        "                        transcriptions.append(transcript)\n",
        "\n",
        "                    # Classify text\n",
        "                    logits = self.classify_text(transcript)\n",
        "                    all_logits.append(logits)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing audio sample {i}: {e}\")\n",
        "                    # Return random logits as fallback\n",
        "                    fallback_logits = torch.randn(1, 8).to(self.device)\n",
        "                    all_logits.append(fallback_logits)\n",
        "                    if return_transcriptions:\n",
        "                        transcriptions.append(\"\u0644\u0627 \u064a\u0648\u062c\u062f \u0646\u0635\")\n",
        "\n",
        "            # Concatenate all logits\n",
        "            final_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "        # Get predictions\n",
        "        predictions = torch.argmax(final_logits, dim=1)\n",
        "\n",
        "        # Get max scores (for compatibility with baseline)\n",
        "        max_scores = torch.max(final_logits, dim=1)[0]\n",
        "\n",
        "        # Return in same format as baseline: (logits, max_scores, predictions, country_names)\n",
        "        country_names = [id2labels[pred.item()] for pred in predictions]\n",
        "\n",
        "        if return_transcriptions:\n",
        "            return final_logits, max_scores, predictions, country_names, transcriptions\n",
        "        else:\n",
        "            return final_logits, max_scores, predictions, country_names\n",
        "\n",
        "print(\"Model class defined successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zoOXxJdPsID",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1753776445519,
          "user_tz": -180,
          "elapsed": 14,
          "user": {
            "displayName": "MD.RAFIUL BISWAS",
            "userId": "15801257354586832441"
          }
        },
        "outputId": "1fe51c7c-43e6-4145-ce17-868b9c378148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model class defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== CELL 9: Create and Initialize Model =====\n",
        "\n",
        "# Create the model\n",
        "model = WhisperARBERTDialectClassifier(\n",
        "    whisper_model_name=\"large-v3\",  # Using Whisper Large v3\n",
        "    text_model_name=\"UBC-NLP/MARBERT\",  # or \"UBC-NLP/ARBERT\"\n",
        "    num_classes=8\n",
        ")\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316,
          "referenced_widgets": [
            "20f23bf1aae34931aadff320c138b37a",
            "9517408ec0404ce787bdc8100cca5478",
            "c2305dc39bda4e7aaf318bccb713ccc4",
            "d012009600d2477ea686eb31f709d521",
            "247e399d658e4df0862a986350980858",
            "62329d3e047249d68bcd54507a141b6d",
            "7140ccd836164f6ca390bd83b0554388",
            "1a168584aa314e478bb9a1b7088650a6",
            "cebcecda5db6427a8d3e3b43fb9d86de",
            "2ec2ca3fa46245ecab616b85f85a14fb",
            "f8aa7b1349f248d0befab2736e6bb340",
            "44786f255182498393d12305de203c9a",
            "97bdcd2603204f9092b52788b14b9aed",
            "8f41fbddf6c04982bddf1931216132b3",
            "d347171f7b654dc1b54c61906cb038d7",
            "79a9c8cf3af348439c53cfe4b966df0c",
            "8de2c7e79d604219b55cf0c85a19240f",
            "7138321c17e744bf9154cdf30549a01b",
            "f1bc0a87dd364b678eb4cb62b5b17845",
            "14834c900e1d4333b54c4cf4e027142f",
            "e3ad1012cbfc43c9bc718c2cd38caa68",
            "236154074e904e3a8f7f99dce433176e",
            "b7d5f7332ea8410aab3677186f292282",
            "8c1ab095d5e144a5a227c9d218b4c800",
            "fd76f5cbac0b4a2d9c98cfa45274831e",
            "617a347dee7b4f1fa9fa7cedc17f7708",
            "429a332bea2d4ca68e3a3705e32c3ae9",
            "9a2d02e44c404edeb971fb080d2b04ce",
            "2c6f2235ff1346f5ac2f220f189fe78d",
            "31bb26b66eae4551b7c292e8cd719c23",
            "232f1b2108c7426780640914cae4dc9d",
            "4732df093e9440d59fd274793a84050a",
            "ee4b8e86f7e84776b58b2602bd5d9cd5",
            "c3f9886c01d54842ba8785157396d65d",
            "d012f1f170a0402195c8c3bd6a28f4ea",
            "f3229344cecf4e319c4634cab5dd1a50",
            "19d61718c1d144aea2607f8e4ccdb254",
            "e8a9064667004b73b9c5a8aae3835cb8",
            "f09d59438ceb4e3c88167eb93bfecf48",
            "df51fd3cfcd745ac8a61bf85f6a28b01",
            "59c166808945474b997ac502203f5bf5",
            "90296c33809a4dbfa490d7774a1a1b84",
            "1d3a73e36e2443e9b6967b601ad6e110",
            "ab10619486f64964a6f818b490e8e37d",
            "bf6f48363b004a0a8ec0470d491748a4",
            "38d69c9c47564f05850ddf3d73f67cb3",
            "f0120ea8a9884a66aae6fdd53ac4d377",
            "36f88c2186c54ba4b6f71a96eb6d4d26",
            "60b3d1567d114acfbfc680553d77befa",
            "9175bf32bbd845d2b6e96331f6fe3d71",
            "6ea27eb5a3184b3fa1e23c980fddba4a",
            "ad9ba800d9e04dc396724e0bd7f18632",
            "e78fa42ee8ca4f119a6dccb745af37e7",
            "e2281ca126d3498a991eaf1df3d5f65c",
            "cd76a50797dc47aa825c7c3c769a62f2"
          ]
        },
        "id": "31E_6ASOPzZb",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1753776618103,
          "user_tz": -180,
          "elapsed": 172583,
          "user": {
            "displayName": "MD.RAFIUL BISWAS",
            "userId": "15801257354586832441"
          }
        },
        "outputId": "9d28d457-87b9-477e-d910-980905e02aaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Whisper model: large-v3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.88G/2.88G [02:08<00:00, 24.1MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading text model: UBC-NLP/MARBERT\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/376 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20f23bf1aae34931aadff320c138b37a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/701 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44786f255182498393d12305de203c9a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7d5f7332ea8410aab3677186f292282"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3f9886c01d54842ba8785157396d65d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/654M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf6f48363b004a0a8ec0470d491748a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created new classifier: 768 -> 8\n",
            "Frozen Whisper parameters: 1,541,570,560\n",
            "Frozen text encoder parameters: 162,841,344\n",
            "Trainable classifier parameters: 6,152\n",
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== CELL 10: Create Data Loaders (Enhanced with Validation Training) =====\n",
        "\n",
        "# Check if datasets were loaded successfully\n",
        "if 'ds' not in locals() and 'ds' not in globals():\n",
        "    print(\"\u274c Training/validation dataset not found. Loading now...\")\n",
        "    try:\n",
        "        ds = load_dataset(\"UBC-NLP/NADI2025_subtask2_SLID\")\n",
        "        ds.set_format(\"torch\")\n",
        "        print(\"\u2705 Training/validation dataset loaded successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Failed to load training/validation dataset: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Try enhanced collate function first, fall back to original if needed\n",
        "try:\n",
        "    print(\"\ud83d\udd04 Testing enhanced collate function with torchaudio...\")\n",
        "    test_sample = [ds['train'][0]]  # Test with one sample\n",
        "    col_fun_whisper_enhanced(test_sample)\n",
        "    collate_fn_to_use = col_fun_whisper_enhanced\n",
        "    print(\"\u2705 Using enhanced collate function with torchaudio preprocessing\")\n",
        "except Exception as e:\n",
        "    print(f\"\u26a0\ufe0f  Enhanced collate function failed: {e}\")\n",
        "    print(\"\ud83d\udd04 Falling back to original collate function...\")\n",
        "    collate_fn_to_use = col_fun_whisper\n",
        "    print(\"\u2705 Using original collate function\")\n",
        "\n",
        "# OPTION 1: Standard training (train only on training set)\n",
        "print(\"\\n\ud83e\udd14 Training Data Configuration Options:\")\n",
        "print(\"1\ufe0f\u20e3  Standard: Use only training data (12,900 samples)\")\n",
        "print(\"2\ufe0f\u20e3  Enhanced: Use training + validation data (25,600 samples)\")\n",
        "\n",
        "# Let user choose or default to enhanced\n",
        "USE_VALIDATION_FOR_TRAINING = True  # Set to False if you want standard training only\n",
        "\n",
        "if USE_VALIDATION_FOR_TRAINING:\n",
        "    print(\"\u2705 Selected: Enhanced training (using validation data for training)\")\n",
        "\n",
        "    # Combine training and validation datasets\n",
        "    from datasets import concatenate_datasets\n",
        "    combined_train_ds = concatenate_datasets([ds['train'], ds['validation']])\n",
        "\n",
        "    # Create enhanced training loader with combined data\n",
        "    trainloader = DataLoader(\n",
        "        combined_train_ds,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn_to_use,\n",
        "        batch_size=8\n",
        "    )\n",
        "\n",
        "    # For evaluation, we'll use a subset of the validation data (or skip evaluation)\n",
        "    # Option A: Use small subset of validation for evaluation\n",
        "    val_subset_size = 10  # Use only 1000 samples for quick evaluation\n",
        "    val_indices = list(range(0, min(val_subset_size, len(ds['validation']))))\n",
        "    val_subset = ds['validation'].select(val_indices)\n",
        "\n",
        "    valloader = DataLoader(\n",
        "        val_subset,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn_to_use,\n",
        "        batch_size=4\n",
        "    )\n",
        "\n",
        "    print(f\"\ud83d\udcca Enhanced training configuration:\")\n",
        "    print(f\"   \ud83c\udfaf Training samples: {len(combined_train_ds):,} (train + validation)\")\n",
        "    print(f\"   \ud83d\udcca Evaluation samples: {len(val_subset):,} (subset of validation)\")\n",
        "    print(f\"   \ud83d\ude80 More data = potentially better performance!\")\n",
        "\n",
        "else:\n",
        "    print(\"\u2705 Selected: Standard training (using only training data)\")\n",
        "\n",
        "    # Standard configuration\n",
        "    trainloader = DataLoader(ds['train'], shuffle=True, collate_fn=collate_fn_to_use, batch_size=8)\n",
        "    valloader = DataLoader(ds['validation'], shuffle=False, collate_fn=collate_fn_to_use, batch_size=4)\n",
        "\n",
        "    print(f\"\ud83d\udcca Standard training configuration:\")\n",
        "    print(f\"   \ud83c\udfaf Training samples: {len(ds['train']):,}\")\n",
        "    print(f\"   \ud83d\udcca Evaluation samples: {len(ds['validation']):,}\")\n",
        "\n",
        "# No test loader creation here - we'll process dialects individually\n",
        "print(\"\u2139\ufe0f  Test data will be processed dialect-by-dialect to save memory\")\n",
        "\n",
        "print(\"\\nData loaders summary:\")\n",
        "print(f\"\u2705 Training batches: {len(trainloader):,}\")\n",
        "print(f\"\u2705 Evaluation batches: {len(valloader):,}\")\n",
        "print(f\"\ud83c\udfb5 Audio processing: {'Enhanced (torchaudio)' if collate_fn_to_use == col_fun_whisper_enhanced else 'Standard'}\")\n",
        "print(f\"\ud83d\udcc8 Training approach: {'Enhanced (train+val)' if USE_VALIDATION_FOR_TRAINING else 'Standard (train only)'}\")\n",
        "print(f\"\u2139\ufe0f  Test processing: Memory-efficient dialect-by-dialect approach\")\n",
        "\n",
        "# Store configuration for later use\n",
        "if 'TEST_CONFIG' in locals() or 'TEST_CONFIG' in globals():\n",
        "    DIALECTS = TEST_CONFIG['dialects']\n",
        "    print(f\"\u2139\ufe0f  Will process {TEST_CONFIG['total_samples']:,} test samples across {len(DIALECTS)} dialects\")\n",
        "else:\n",
        "    DIALECTS = ['Algeria', 'Egypt', 'Jordan', 'Mauritania', 'Morocco', 'Palestine', 'UAE', 'Yemen']\n",
        "    print(f\"\u2139\ufe0f  Using default dialects list: {DIALECTS}\")\n",
        "\n",
        "# Store the collate function for later use\n",
        "COLLATE_FN = collate_fn_to_use\n",
        "\n",
        "# Force garbage collection to free up memory\n",
        "import gc\n",
        "gc.collect()\n",
        "print(\"\ud83e\uddf9 Memory cleanup completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413,
          "referenced_widgets": [
            "87dd3fbcd7764a65a5f78653c67e3069",
            "e251a169752545f9a3edad85895165e8",
            "67d3c5e6ba0d47939c78f7af1d79d0fa",
            "2be3c6736e1b422abb0049897d3ead68",
            "4a6b1278c3614721bd8823f50ac476ac",
            "52c048b5c9364b3c84cdd62f54732807",
            "ee0732228ef44ae59d31926c07870312",
            "658f80c13cb746a19391559f6b938ff6",
            "06c08b8c1c60437784bd259dc2030d94",
            "6c60a476456a49dc9ec6d0a9ec5d612b",
            "6ced4a5108014127ab9eb2a480ddd550"
          ]
        },
        "id": "4rGCY6-fP2xv",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1753776633076,
          "user_tz": -180,
          "elapsed": 14967,
          "user": {
            "displayName": "MD.RAFIUL BISWAS",
            "userId": "15801257354586832441"
          }
        },
        "outputId": "086f2e32-16ca-4dd3-f6ee-551906d8b49d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83d\udd04 Testing enhanced collate function with torchaudio...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/654M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87dd3fbcd7764a65a5f78653c67e3069"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Using enhanced collate function with torchaudio preprocessing\n",
            "\n",
            "\ud83e\udd14 Training Data Configuration Options:\n",
            "1\ufe0f\u20e3  Standard: Use only training data (12,900 samples)\n",
            "2\ufe0f\u20e3  Enhanced: Use training + validation data (25,600 samples)\n",
            "\u2705 Selected: Enhanced training (using validation data for training)\n",
            "\ud83d\udcca Enhanced training configuration:\n",
            "   \ud83c\udfaf Training samples: 25,600 (train + validation)\n",
            "   \ud83d\udcca Evaluation samples: 10 (subset of validation)\n",
            "   \ud83d\ude80 More data = potentially better performance!\n",
            "\u2139\ufe0f  Test data will be processed dialect-by-dialect to save memory\n",
            "\n",
            "Data loaders summary:\n",
            "\u2705 Training batches: 3,200\n",
            "\u2705 Evaluation batches: 3\n",
            "\ud83c\udfb5 Audio processing: Enhanced (torchaudio)\n",
            "\ud83d\udcc8 Training approach: Enhanced (train+val)\n",
            "\u2139\ufe0f  Test processing: Memory-efficient dialect-by-dialect approach\n",
            "\u2139\ufe0f  Will process 10,810 test samples across 8 dialects\n",
            "\ud83e\uddf9 Memory cleanup completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== CELL 11: Define Training Function (Fast Version with Enhanced Data) =====\n",
        "\n",
        "def train_whisper_arbert_model_fast(model, trainloader, valloader, device, skip_eval=False):\n",
        "    \"\"\"\n",
        "    Fast training loop for Whisper + ARBERT model\n",
        "    Can optionally skip evaluation entirely for maximum speed\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    model.device = device\n",
        "\n",
        "    # Only train the new classifier layer (like SpeechBrain approach)\n",
        "    optimizer = AdamW(model.classifier.parameters(), lr=2e-5)\n",
        "\n",
        "    max_steps = 5000 if len(trainloader) > 2000 else 3000  # More steps if more data\n",
        "    logging_steps = 250  # Adjusted for potentially longer training\n",
        "\n",
        "    print(f\"\ud83d\udcca Training configuration:\")\n",
        "    print(f\"   \ud83c\udfaf Total steps: {max_steps:,}\")\n",
        "    print(f\"   \ud83d\udcc8 Training samples per epoch: ~{len(trainloader) * 8:,}\")\n",
        "    print(f\"   \u26a1 Final evaluation only: {'No evaluation' if skip_eval else 'Yes'}\")\n",
        "\n",
        "    # Simple learning rate schedule\n",
        "    scheduler = LinearLR(optimizer, start_factor=0.1, end_factor=1.0, total_iters=300)\n",
        "\n",
        "    loss_fn = CrossEntropyLoss()\n",
        "\n",
        "    avg_loss = 0\n",
        "    step = 1\n",
        "\n",
        "    print(\"\ud83d\ude80 Starting FAST training with enhanced data...\")\n",
        "    print_trainable_parameters(model)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    while step <= max_steps:\n",
        "\n",
        "        for batch in tqdm(trainloader, desc=f\"Training Step {step}\", leave=False):\n",
        "            if step > max_steps:\n",
        "                break\n",
        "\n",
        "            wavs, _, labels, _ = batch  # Updated to handle IDs\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            try:\n",
        "                # Forward pass with training flag\n",
        "                o = model(wavs, training=True)\n",
        "                logits = o[0]\n",
        "\n",
        "                # Calculate loss\n",
        "                loss = loss_fn(logits, labels)\n",
        "                loss.backward()\n",
        "\n",
        "                optimizer.step()\n",
        "                if step <= 300:\n",
        "                    scheduler.step()\n",
        "\n",
        "                avg_loss += loss.item()\n",
        "\n",
        "                # Logging (reduced frequency)\n",
        "                if step % logging_steps == 0:\n",
        "                    elapsed_time = time.time() - start_time\n",
        "                    steps_per_sec = step / elapsed_time\n",
        "                    eta_seconds = (max_steps - step) / steps_per_sec\n",
        "                    eta_minutes = eta_seconds / 60\n",
        "\n",
        "                    print(f\"Step {step}/{max_steps}: Loss {avg_loss/logging_steps:.4f}, \"\n",
        "                          f\"LR {optimizer.param_groups[0]['lr']:.2e}, \"\n",
        "                          f\"Speed: {steps_per_sec:.1f} steps/s, \"\n",
        "                          f\"ETA: {eta_minutes:.1f}min\")\n",
        "                    avg_loss = 0\n",
        "\n",
        "                step += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in training step {step}: {e}\")\n",
        "                # Skip this batch and continue\n",
        "                step += 1\n",
        "                continue\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\u23f1\ufe0f  Total training time: {total_time/3600:.1f} hours ({total_time/60:.1f} minutes)\")\n",
        "\n",
        "    # Evaluate only if not skipping\n",
        "    if not skip_eval and valloader is not None:\n",
        "        print(f\"\\n\ud83d\udcca Training completed! Running final evaluation...\")\n",
        "        final_acc, final_cost, final_fpr, final_fnr = eval_loop_whisper(model, valloader)\n",
        "        print(f\"\ud83c\udfaf Final Results: ACC {final_acc:.2f}% AvgCost {final_cost:.4f}, FPR {final_fpr:.4f}, FNR {final_fnr:.4f}\")\n",
        "    else:\n",
        "        print(f\"\u2705 Training completed! (Evaluation skipped for maximum speed)\")\n",
        "\n",
        "    return model\n",
        "\n",
        "print(\"\u26a1 Enhanced fast training function defined successfully!\")\n",
        "print(\"\ud83d\udca1 Features: Optional evaluation skip, adaptive training steps, enhanced data support\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14PI77UhQqnd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1753727836080,
          "user_tz": -180,
          "elapsed": 14,
          "user": {
            "displayName": "MD.RAFIUL BISWAS",
            "userId": "15801257354586832441"
          }
        },
        "outputId": "3c37ebf2-aede-4652-b155-2d1025cb1eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u26a1 Enhanced fast training function defined successfully!\n",
            "\ud83d\udca1 Features: Optional evaluation skip, adaptive training steps, enhanced data support\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== CELL 12: Define Evaluation Function =====\n",
        "\n",
        "def eval_loop_whisper(model, loader):\n",
        "    \"\"\"\n",
        "    Evaluation loop for Whisper + ARBERT model\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    tp = 0\n",
        "    n = 0\n",
        "    logits_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
        "            wavs, _, labels, _ = batch  # Updated to handle IDs\n",
        "\n",
        "            try:\n",
        "                # Forward pass in evaluation mode\n",
        "                o = model(wavs, training=False)\n",
        "\n",
        "                # Collect results\n",
        "                logits_list.append(o[0].cpu())\n",
        "                labels_list.append(labels.cpu())\n",
        "\n",
        "                # Calculate accuracy\n",
        "                preds = o[2].cpu()\n",
        "                tp += accuracy_score(labels, preds, normalize=False)\n",
        "                n += len(labels)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in evaluation: {e}\")\n",
        "                # Skip this batch\n",
        "                continue\n",
        "\n",
        "    if len(logits_list) == 0:\n",
        "        print(\"No successful predictions during evaluation!\")\n",
        "        return 0.0, 1.0, 1.0, 1.0\n",
        "\n",
        "    # Concatenate results\n",
        "    logits = torch.concat(logits_list, dim=0)\n",
        "    labels = torch.concat(labels_list, dim=0)\n",
        "\n",
        "    # Calculate cost metrics\n",
        "    cost, fpr, fnr = compute_ave_cost(logits, labels)\n",
        "\n",
        "    return tp/n * 100, cost, fpr, fnr\n",
        "\n",
        "print(\"Evaluation function defined successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGVYBdHbQ3yT",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1753727852818,
          "user_tz": -180,
          "elapsed": 45,
          "user": {
            "displayName": "MD.RAFIUL BISWAS",
            "userId": "15801257354586832441"
          }
        },
        "outputId": "cbabffde-ebf3-4dbe-e1be-543b463af267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation function defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== CELL 13: Define Transcription Function for Test Data =====\n",
        "\n",
        "def transcribe_test_data(model, test_loader, output_file='test_transcriptions.csv'):\n",
        "    \"\"\"\n",
        "    Transcribe test data and save results to CSV\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_ids = []\n",
        "    all_transcriptions = []\n",
        "    all_predictions = []\n",
        "    all_logits = []\n",
        "    all_dialects = []  # Track original dialect labels\n",
        "\n",
        "    print(\"Transcribing test data...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Processing test data\"):\n",
        "            wavs, _, _, ids = batch\n",
        "\n",
        "            # Get dialect info from the original samples if available\n",
        "            batch_dialects = []\n",
        "            for sample_id in ids:\n",
        "                # Try to find the dialect from the loaded data\n",
        "                dialect_found = \"Unknown\"\n",
        "                for sample in all_test_data:\n",
        "                    if sample.get('ID') == sample_id:\n",
        "                        dialect_found = sample.get('dialect', 'Unknown')\n",
        "                        break\n",
        "                batch_dialects.append(dialect_found)\n",
        "\n",
        "            try:\n",
        "                # Forward pass with transcription return enabled\n",
        "                o = model(wavs, training=False, return_transcriptions=True)\n",
        "                logits, max_scores, predictions, country_names, transcriptions = o\n",
        "\n",
        "                # Store results\n",
        "                all_ids.extend(ids)\n",
        "                all_transcriptions.extend(transcriptions)\n",
        "                all_predictions.extend(country_names)\n",
        "                all_logits.extend(logits.cpu().numpy())\n",
        "                all_dialects.extend(batch_dialects)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing batch: {e}\")\n",
        "                # Handle batch failure\n",
        "                batch_size = len(ids)\n",
        "                all_ids.extend(ids)\n",
        "                all_transcriptions.extend([\"\u0644\u0627 \u064a\u0648\u062c\u062f \u0646\u0635\"] * batch_size)\n",
        "                all_predictions.extend([\"Egypt\"] * batch_size)  # Default prediction\n",
        "                all_logits.extend([np.random.randn(8)] * batch_size)\n",
        "                all_dialects.extend(batch_dialects)\n",
        "\n",
        "    # Create DataFrame with results\n",
        "    results_df = pd.DataFrame({\n",
        "        'ID': all_ids,\n",
        "        'original_dialect': all_dialects,\n",
        "        'transcription': all_transcriptions,\n",
        "        'predicted_country': all_predictions\n",
        "    })\n",
        "\n",
        "    # Save transcriptions\n",
        "    results_df.to_csv(output_file, index=False)\n",
        "    print(f\"Transcriptions saved to {output_file}\")\n",
        "\n",
        "    return results_df, all_logits\n",
        "\n",
        "print(\"Transcription function defined successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O298LXnqRL9c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1753727856209,
          "user_tz": -180,
          "elapsed": 8,
          "user": {
            "displayName": "MD.RAFIUL BISWAS",
            "userId": "15801257354586832441"
          }
        },
        "outputId": "e46e6d12-7335-4b0d-89d2-5c35fcef0546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription function defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== CELL 14: Define Submission Writing Functions =====\n",
        "\n",
        "def write_logits(o, filename):\n",
        "    with open(filename, \"a\") as fp:\n",
        "        for i in o:\n",
        "            l = \"\"\n",
        "            for j in i:\n",
        "                l += str(j.item()) + \"\\t\"\n",
        "            fp.write(str.strip(l)+\"\\n\")\n",
        "\n",
        "def write_preds(o, filename):\n",
        "    with open(filename, \"a\") as fp:\n",
        "        for i in o:\n",
        "            fp.write(str.strip(str(i.item()))+\"\\n\")\n",
        "\n",
        "def submission_writer_whisper(model, loader, logits_file='logits.tsv', preds_file='predictions.tsv'):\n",
        "    \"\"\"\n",
        "    Generate submission files for Whisper + ARBERT model\n",
        "    Works with validation loader (has labels)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Clear existing files\n",
        "    open(logits_file, 'w').close()\n",
        "    open(preds_file, 'w').close()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=\"Generating submission\"):\n",
        "            wavs, _, labels, _ = batch\n",
        "\n",
        "            # Forward pass\n",
        "            o = model(wavs, training=False)\n",
        "\n",
        "            # Write results\n",
        "            write_logits(o[0].cpu(), logits_file)\n",
        "            write_preds(o[2].cpu(), preds_file)\n",
        "\n",
        "    print(f\"\u2705 Submission files created:\")\n",
        "    print(f\"   \ud83d\udcc4 {logits_file}\")\n",
        "    print(f\"   \ud83d\udcc4 {preds_file}\")\n",
        "\n",
        "print(\"Submission functions defined successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLaKEpHERS2y",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1753727861838,
          "user_tz": -180,
          "elapsed": 8,
          "user": {
            "displayName": "MD.RAFIUL BISWAS",
            "userId": "15801257354586832441"
          }
        },
        "outputId": "fecf356b-ed4d-419c-f630-dd684502ed09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission functions defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Techinque: Only test data transcribe"
      ],
      "metadata": {
        "id": "fTv93kOlJHMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== COMPLETE BATCH TRANSCRIPTION SYSTEM (NO TRAINING) =====\n",
        "# Standalone system with all required components\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import whisper\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"\ud83d\ude80 BATCH TRANSCRIPTION MODE - NO TRAINING\")\n",
        "print(\"=\"*60)\n",
        "print(\"\u26a1 Optimized for speed with 10,000 test samples\")\n",
        "print(\"\ud83c\udfaf Using pretrained Whisper + simple classification\")\n",
        "\n",
        "# ===== STEP 1: Define Required Components =====\n",
        "\n",
        "# Country labels mapping\n",
        "countries = ['Algeria', 'Egypt', 'Jordan', 'Mauritania', 'Morocco', 'Palestine', 'UAE', 'Yemen']\n",
        "labels2id = {key: id for id, key in enumerate(countries)}\n",
        "id2labels = {id: key for key, id in labels2id.items()}\n",
        "\n",
        "print(\"Country labels mapping:\")\n",
        "for i, country in enumerate(countries):\n",
        "    print(f\"  {i}: {country}\")\n",
        "\n",
        "# Enhanced collate function with error handling\n",
        "def col_fun_whisper_batch(samples):\n",
        "    \"\"\"\n",
        "    Robust collate function for batch processing with error handling\n",
        "    \"\"\"\n",
        "    arrays = []\n",
        "    ids = []\n",
        "\n",
        "    print(f\"    \ud83d\udd0d Processing batch with {len(samples)} samples\")\n",
        "\n",
        "    for i, sample in enumerate(samples):\n",
        "        try:\n",
        "            # Get ID\n",
        "            sample_id = sample.get('ID', f'unknown_{i}')\n",
        "            ids.append(sample_id)\n",
        "\n",
        "            # Get audio array\n",
        "            if 'audio' not in sample:\n",
        "                print(f\"    \u26a0\ufe0f  No audio in sample {sample_id}\")\n",
        "                # Create dummy audio\n",
        "                arrays.append(torch.zeros(16000))  # 1 second of silence\n",
        "                continue\n",
        "\n",
        "            audio_data = sample['audio']\n",
        "            if isinstance(audio_data, dict):\n",
        "                audio_array = audio_data.get('array', [])\n",
        "                sample_rate = audio_data.get('sampling_rate', 16000)\n",
        "            else:\n",
        "                audio_array = audio_data\n",
        "                sample_rate = 16000\n",
        "\n",
        "            # Handle empty or invalid audio\n",
        "            if audio_array is None or len(audio_array) == 0:\n",
        "                print(f\"    \u26a0\ufe0f  Empty audio for sample {sample_id}\")\n",
        "                arrays.append(torch.zeros(16000))  # 1 second of silence\n",
        "                continue\n",
        "\n",
        "            # Convert to tensor if needed\n",
        "            if isinstance(audio_array, np.ndarray):\n",
        "                audio_tensor = torch.from_numpy(audio_array).float()\n",
        "            else:\n",
        "                audio_tensor = torch.tensor(audio_array).float()\n",
        "\n",
        "            # Handle edge cases\n",
        "            if audio_tensor.numel() == 0:\n",
        "                print(f\"    \u26a0\ufe0f  Zero-size tensor for sample {sample_id}\")\n",
        "                arrays.append(torch.zeros(16000))\n",
        "                continue\n",
        "\n",
        "            # Ensure mono audio\n",
        "            if audio_tensor.dim() > 1:\n",
        "                audio_tensor = torch.mean(audio_tensor, dim=0)\n",
        "\n",
        "            # Ensure 1D tensor\n",
        "            if audio_tensor.dim() == 0:\n",
        "                audio_tensor = audio_tensor.unsqueeze(0)\n",
        "\n",
        "            # Normalize audio safely\n",
        "            max_val = audio_tensor.abs().max()\n",
        "            if max_val > 0:\n",
        "                audio_tensor = audio_tensor / max_val\n",
        "\n",
        "            # Ensure minimum length\n",
        "            min_length = 1600  # 0.1 seconds minimum\n",
        "            if len(audio_tensor) < min_length:\n",
        "                audio_tensor = torch.nn.functional.pad(audio_tensor, (0, min_length - len(audio_tensor)))\n",
        "\n",
        "            # Truncate to reasonable length (30 seconds max for Whisper)\n",
        "            max_length = 16000 * 30  # 30 seconds at 16kHz\n",
        "            if len(audio_tensor) > max_length:\n",
        "                audio_tensor = audio_tensor[:max_length]\n",
        "\n",
        "            arrays.append(audio_tensor)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    \u274c Error processing sample {i}: {e}\")\n",
        "            # Add dummy data for failed sample\n",
        "            ids.append(f'failed_{i}')\n",
        "            arrays.append(torch.zeros(16000))\n",
        "\n",
        "    # Handle empty batch\n",
        "    if len(arrays) == 0:\n",
        "        print(f\"    \u26a0\ufe0f  Empty batch, creating dummy data\")\n",
        "        arrays = [torch.zeros(16000)]\n",
        "        ids = ['dummy_0']\n",
        "\n",
        "    # Pad sequences to same length\n",
        "    lengths = [len(arr) for arr in arrays]\n",
        "    if len(lengths) == 0:\n",
        "        max_len = 16000\n",
        "    else:\n",
        "        max_len = max(lengths)\n",
        "\n",
        "    padded_arrays = []\n",
        "    for arr in arrays:\n",
        "        if len(arr) < max_len:\n",
        "            padded = torch.nn.functional.pad(arr, (0, max_len - len(arr)))\n",
        "        else:\n",
        "            padded = arr\n",
        "        padded_arrays.append(padded)\n",
        "\n",
        "    try:\n",
        "        packed = torch.stack(padded_arrays)\n",
        "    except Exception as e:\n",
        "        print(f\"    \u274c Error stacking tensors: {e}\")\n",
        "        # Create fallback tensor\n",
        "        packed = torch.zeros(len(padded_arrays), max_len)\n",
        "        for i, arr in enumerate(padded_arrays):\n",
        "            packed[i, :len(arr)] = arr[:max_len]\n",
        "\n",
        "    print(f\"    \u2705 Batch processed: {packed.shape}\")\n",
        "\n",
        "    # Return format: audio, lengths, labels (None for test), ids\n",
        "    return packed, lengths, None, ids\n",
        "\n",
        "# ===== STEP 2: Define Model Class =====\n",
        "\n",
        "class WhisperARBERTDialectClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Batch-optimized Whisper + ARBERT model for transcription\n",
        "    \"\"\"\n",
        "    def __init__(self, whisper_model_name=\"large-v3\", text_model_name=\"UBC-NLP/MARBERT\", num_classes=8):\n",
        "        super().__init__()\n",
        "\n",
        "        print(f\"Loading Whisper model: {whisper_model_name}\")\n",
        "        self.whisper_model = whisper.load_model(whisper_model_name)\n",
        "\n",
        "        print(f\"Loading text model: {text_model_name}\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(text_model_name)\n",
        "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
        "\n",
        "        # Freeze all pretrained models\n",
        "        for param in self.whisper_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.text_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Simple classifier\n",
        "        hidden_size = self.text_encoder.config.hidden_size\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        print(f\"\u2705 Model initialized - Whisper + MARBERT + Classifier\")\n",
        "\n",
        "    def transcribe_batch(self, audio_batch):\n",
        "        \"\"\"\n",
        "        Efficiently transcribe a batch of audio samples\n",
        "        \"\"\"\n",
        "        transcriptions = []\n",
        "\n",
        "        for i in range(audio_batch.shape[0]):\n",
        "            audio_numpy = audio_batch[i].cpu().numpy()\n",
        "\n",
        "            try:\n",
        "                # Whisper transcription\n",
        "                result = self.whisper_model.transcribe(\n",
        "                    audio_numpy,\n",
        "                    language='ar',\n",
        "                    task='transcribe',\n",
        "                    beam_size=1,\n",
        "                    best_of=1,\n",
        "                    temperature=0\n",
        "                )\n",
        "\n",
        "                text = result['text'].strip()\n",
        "                if not text:\n",
        "                    text = \"\u0644\u0627 \u064a\u0648\u062c\u062f \u0646\u0635\"\n",
        "\n",
        "                transcriptions.append(text)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  \u26a0\ufe0f  Transcription error for sample {i}: {e}\")\n",
        "                transcriptions.append(\"\u0644\u0627 \u064a\u0648\u062c\u062f \u0646\u0635\")\n",
        "\n",
        "        return transcriptions\n",
        "\n",
        "    def classify_batch(self, texts):\n",
        "        \"\"\"\n",
        "        Classify a batch of texts efficiently\n",
        "        \"\"\"\n",
        "        if not texts:\n",
        "            return torch.randn(1, 8).to(self.device)\n",
        "\n",
        "        try:\n",
        "            # Tokenize all texts at once\n",
        "            inputs = self.tokenizer(\n",
        "                texts,\n",
        "                return_tensors='pt',\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=512\n",
        "            )\n",
        "\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "            # Get embeddings\n",
        "            with torch.no_grad():\n",
        "                encoder_outputs = self.text_encoder(**inputs)\n",
        "                embeddings = encoder_outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "            # Classify\n",
        "            logits = self.classifier(embeddings)\n",
        "            return logits\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  \u26a0\ufe0f  Classification error: {e}\")\n",
        "            batch_size = len(texts)\n",
        "            return torch.randn(batch_size, 8).to(self.device)\n",
        "\n",
        "    def forward(self, audio_batch, return_transcriptions=False):\n",
        "        \"\"\"\n",
        "        Complete forward pass: Audio -> Transcription -> Classification\n",
        "        \"\"\"\n",
        "        # Transcribe all audio\n",
        "        transcriptions = self.transcribe_batch(audio_batch)\n",
        "\n",
        "        # Classify all transcriptions\n",
        "        logits = self.classify_batch(transcriptions)\n",
        "\n",
        "        # Get predictions and scores\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "        max_scores = torch.max(logits, dim=1)[0]\n",
        "        country_names = [id2labels[pred.item()] for pred in predictions]\n",
        "\n",
        "        if return_transcriptions:\n",
        "            return logits, max_scores, predictions, country_names, transcriptions\n",
        "        else:\n",
        "            return logits, max_scores, predictions, country_names\n",
        "\n",
        "# ===== STEP 3: Initialize Model =====\n",
        "\n",
        "print(\"\\n\ud83d\udd27 Initializing model...\")\n",
        "model = WhisperARBERTDialectClassifier(\n",
        "    whisper_model_name=\"large-v3\",\n",
        "    text_model_name=\"UBC-NLP/MARBERT\",\n",
        "    num_classes=8\n",
        ")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "model.device = device\n",
        "\n",
        "# Initialize classifier to predict Egypt (most common)\n",
        "with torch.no_grad():\n",
        "    model.classifier.weight.fill_(0.0)\n",
        "    model.classifier.bias.fill_(0.0)\n",
        "    model.classifier.bias[1] = 2.0  # Bias toward Egypt (index 1)\n",
        "\n",
        "print(\"\u2705 Model ready for batch transcription!\")\n",
        "\n",
        "# ===== STEP 4: Batch Processing Function =====\n",
        "\n",
        "def process_test_dialect_batch(dialect_name, batch_size=1):\n",
        "    \"\"\"\n",
        "    Process one dialect with robust error handling\n",
        "    \"\"\"\n",
        "    print(f\"\\n\ud83d\udcc2 Processing {dialect_name}...\")\n",
        "\n",
        "    try:\n",
        "        # Load dialect dataset\n",
        "        print(f\"   \ud83d\udd04 Loading dataset for {dialect_name}...\")\n",
        "        dialect_ds = load_dataset(\"UBC-NLP/NADI2025_subtask2_ASR_Test\", dialect_name)\n",
        "        test_data = dialect_ds['test']\n",
        "\n",
        "        print(f\"   \ud83d\udcca {len(test_data):,} samples loaded\")\n",
        "\n",
        "        # Inspect first sample\n",
        "        if len(test_data) > 0:\n",
        "            sample = test_data[0]\n",
        "            print(f\"   \ud83d\udd0d Sample keys: {list(sample.keys())}\")\n",
        "            if 'audio' in sample:\n",
        "                audio_info = sample['audio']\n",
        "                if isinstance(audio_info, dict):\n",
        "                    print(f\"   \ud83c\udfb5 Audio keys: {list(audio_info.keys())}\")\n",
        "                    if 'array' in audio_info:\n",
        "                        array_shape = np.array(audio_info['array']).shape if audio_info['array'] is not None else \"None\"\n",
        "                        print(f\"   \ud83d\udcd0 Audio shape: {array_shape}\")\n",
        "\n",
        "        # Create data loader with error handling\n",
        "        test_loader = DataLoader(\n",
        "            test_data,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            collate_fn=col_fun_whisper_batch,\n",
        "            num_workers=0,\n",
        "            pin_memory=False,\n",
        "            drop_last=False  # Don't drop incomplete batches\n",
        "        )\n",
        "\n",
        "        print(f\"   \ud83d\udce6 Created data loader with {len(test_loader)} batches\")\n",
        "\n",
        "        # Process all batches\n",
        "        all_results = []\n",
        "        successful_batches = 0\n",
        "        failed_batches = 0\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(tqdm(test_loader, desc=f\"{dialect_name}\")):\n",
        "                try:\n",
        "                    wavs, lengths, _, ids = batch\n",
        "\n",
        "                    # Check batch validity\n",
        "                    if wavs.numel() == 0:\n",
        "                        print(f\"   \u26a0\ufe0f  Empty batch {batch_idx}, skipping\")\n",
        "                        failed_batches += 1\n",
        "                        continue\n",
        "\n",
        "                    print(f\"   \ud83d\udd04 Processing batch {batch_idx}: {wavs.shape} with {len(ids)} IDs\")\n",
        "\n",
        "                    # Process batch\n",
        "                    results = model(wavs, return_transcriptions=True)\n",
        "                    logits, max_scores, predictions, country_names, transcriptions = results\n",
        "\n",
        "                    # Validate results\n",
        "                    if len(transcriptions) != len(ids):\n",
        "                        print(f\"   \u26a0\ufe0f  Mismatch: {len(transcriptions)} transcriptions vs {len(ids)} IDs\")\n",
        "\n",
        "                    # Store results\n",
        "                    for i in range(min(len(ids), len(transcriptions), len(country_names))):\n",
        "                        all_results.append({\n",
        "                            'ID': ids[i],\n",
        "                            'dialect': dialect_name,\n",
        "                            'transcription': transcriptions[i] if i < len(transcriptions) else '\u0644\u0627 \u064a\u0648\u062c\u062f \u0646\u0635',\n",
        "                            'predicted_country': country_names[i] if i < len(country_names) else 'Egypt',\n",
        "                            'prediction_id': predictions[i].cpu().item() if i < len(predictions) else 1,\n",
        "                            'logits': logits[i].cpu().numpy() if i < len(logits) else np.random.randn(8),\n",
        "                            'confidence': max_scores[i].cpu().item() if i < len(max_scores) else 0.5\n",
        "                        })\n",
        "\n",
        "                    successful_batches += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"   \u274c Batch {batch_idx} error: {e}\")\n",
        "                    failed_batches += 1\n",
        "\n",
        "                    # Handle failed batch - create dummy results for all IDs in batch\n",
        "                    try:\n",
        "                        wavs, lengths, _, ids = batch\n",
        "                        for i, sample_id in enumerate(ids):\n",
        "                            all_results.append({\n",
        "                                'ID': sample_id,\n",
        "                                'dialect': dialect_name,\n",
        "                                'transcription': '\u0644\u0627 \u064a\u0648\u062c\u062f \u0646\u0635',\n",
        "                                'predicted_country': 'Egypt',\n",
        "                                'prediction_id': 1,\n",
        "                                'logits': np.random.randn(8),\n",
        "                                'confidence': 0.5\n",
        "                            })\n",
        "                    except Exception as e2:\n",
        "                        print(f\"   \u274c Could not extract IDs from failed batch: {e2}\")\n",
        "\n",
        "                # Memory cleanup every 3 batches\n",
        "                if batch_idx % 3 == 0 and torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "        print(f\"   \u2705 Completed: {len(all_results)} transcriptions\")\n",
        "        print(f\"   \ud83d\udcca Successful batches: {successful_batches}, Failed batches: {failed_batches}\")\n",
        "\n",
        "        # Cleanup\n",
        "        del dialect_ds, test_data, test_loader\n",
        "        gc.collect()\n",
        "\n",
        "        return all_results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   \u274c Critical error processing {dialect_name}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return []\n",
        "\n",
        "# ===== STEP 5: Process All Dialects =====\n",
        "\n",
        "dialects = ['Algeria', 'Egypt', 'Jordan', 'Mauritania', 'Morocco', 'Palestine', 'UAE', 'Yemen']\n",
        "all_transcription_results = []\n",
        "\n",
        "print(f\"\\n\ud83d\udd04 Processing {len(dialects)} dialects...\")\n",
        "start_time = time.time()\n",
        "\n",
        "for i, dialect in enumerate(dialects):\n",
        "    print(f\"\\n[{i+1}/{len(dialects)}] Starting {dialect}...\")\n",
        "\n",
        "    try:\n",
        "        dialect_results = process_test_dialect_batch(dialect, batch_size=1)  # Reduced batch size\n",
        "\n",
        "        if len(dialect_results) > 0:\n",
        "            all_transcription_results.extend(dialect_results)\n",
        "            print(f\"   \u2705 Added {len(dialect_results)} results from {dialect}\")\n",
        "        else:\n",
        "            print(f\"   \u26a0\ufe0f  No results from {dialect}\")\n",
        "\n",
        "        # Progress update\n",
        "        elapsed = time.time() - start_time\n",
        "        remaining_dialects = len(dialects) - (i + 1)\n",
        "\n",
        "        if i > 0:  # Avoid division by zero\n",
        "            avg_time_per_dialect = elapsed / (i + 1)\n",
        "            eta = remaining_dialects * avg_time_per_dialect\n",
        "            print(f\"   \u23f1\ufe0f  Elapsed: {elapsed/60:.1f}min, ETA: {eta/60:.1f}min\")\n",
        "\n",
        "        print(f\"   \ud83d\udcca Total results so far: {len(all_transcription_results):,}\")\n",
        "\n",
        "        # Force memory cleanup after each dialect\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   \u274c Critical error with {dialect}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        continue  # Try next dialect\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n\ud83c\udf89 ALL DIALECTS COMPLETED!\")\n",
        "print(f\"\u23f1\ufe0f  Total time: {total_time/60:.1f} minutes\")\n",
        "print(f\"\ud83d\udcca Total transcriptions: {len(all_transcription_results):,}\")\n",
        "\n",
        "# ===== STEP 6: Create Submission Files =====\n",
        "\n",
        "print(\"\\n\ud83d\udcdd Creating submission files...\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "results_df = pd.DataFrame(all_transcription_results)\n",
        "\n",
        "# Save detailed results\n",
        "results_df.to_csv('batch_transcriptions.csv', index=False, encoding='utf-8')\n",
        "print(\"\u2705 batch_transcriptions.csv saved\")\n",
        "\n",
        "# Create submission files\n",
        "logits_array = np.stack(results_df['logits'].values)\n",
        "predictions = results_df['prediction_id'].values\n",
        "\n",
        "# Write logits file\n",
        "with open('test_logits.tsv', 'w') as f:\n",
        "    for logit_row in logits_array:\n",
        "        f.write('\\t'.join([f\"{x:.6f}\" for x in logit_row]) + '\\n')\n",
        "\n",
        "# Write predictions file\n",
        "with open('test_predictions.tsv', 'w') as f:\n",
        "    for pred in predictions:\n",
        "        f.write(str(pred) + '\\n')\n",
        "\n",
        "print(\"\u2705 test_logits.tsv saved\")\n",
        "print(\"\u2705 test_predictions.tsv saved\")\n",
        "\n",
        "# Create submission zip\n",
        "import zipfile\n",
        "with zipfile.ZipFile('test_submission.zip', 'w') as zipf:\n",
        "    zipf.write('test_logits.tsv')\n",
        "    zipf.write('test_predictions.tsv')\n",
        "\n",
        "print(\"\u2705 test_submission.zip created\")\n",
        "\n",
        "# ===== STEP 7: Final Summary =====\n",
        "\n",
        "print(\"\\n\ud83d\udcca BATCH PROCESSING SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\u23f1\ufe0f  Total processing time: {total_time/60:.1f} minutes\")\n",
        "print(f\"\ud83d\udcca Total samples processed: {len(all_transcription_results):,}\")\n",
        "print(f\"\u26a1 Average speed: {len(all_transcription_results)/(total_time/60):.1f} samples/minute\")\n",
        "\n",
        "# Show distribution\n",
        "if len(results_df) > 0:\n",
        "    country_dist = results_df['predicted_country'].value_counts()\n",
        "    print(f\"\\n\ud83d\uddfa\ufe0f  Predicted Country Distribution:\")\n",
        "    for country, count in country_dist.items():\n",
        "        percentage = (count / len(results_df)) * 100\n",
        "        print(f\"   {country}: {count:,} ({percentage:.1f}%)\")\n",
        "\n",
        "    # Show sample transcriptions\n",
        "    print(f\"\\n\ud83d\udcdd Sample Transcriptions:\")\n",
        "    for i in range(min(3, len(results_df))):\n",
        "        row = results_df.iloc[i]\n",
        "        print(f\"   ID: {row['ID']}\")\n",
        "        print(f\"   Dialect: {row['dialect']}\")\n",
        "        print(f\"   Text: {row['transcription'][:80]}...\")\n",
        "        print(f\"   Country: {row['predicted_country']}\")\n",
        "        print()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"\ud83c\udf89 BATCH TRANSCRIPTION COMPLETED!\")\n",
        "print(\"\ud83d\udce6 Submit test_submission.zip to the competition\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Memory usage info\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\n\ud83d\udcbe Final GPU Memory Usage:\")\n",
        "    print(f\"   Allocated: {torch.cuda.memory_allocated()/1024**3:.1f}GB\")\n",
        "    print(f\"   Cached: {torch.cuda.memory_reserved()/1024**3:.1f}GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jxer8WT8PcRx",
        "outputId": "d8771a2b-e8b4-4f35-d805-6208d3c323c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83d\ude80 BATCH TRANSCRIPTION MODE - NO TRAINING\n",
            "============================================================\n",
            "\u26a1 Optimized for speed with 10,000 test samples\n",
            "\ud83c\udfaf Using pretrained Whisper + simple classification\n",
            "Country labels mapping:\n",
            "  0: Algeria\n",
            "  1: Egypt\n",
            "  2: Jordan\n",
            "  3: Mauritania\n",
            "  4: Morocco\n",
            "  5: Palestine\n",
            "  6: UAE\n",
            "  7: Yemen\n",
            "\n",
            "\ud83d\udd27 Initializing model...\n",
            "Loading Whisper model: large-v3\n",
            "Loading text model: UBC-NLP/MARBERT\n",
            "\u2705 Model initialized - Whisper + MARBERT + Classifier\n",
            "\u2705 Model ready for batch transcription!\n",
            "\n",
            "\ud83d\udd04 Processing 8 dialects...\n",
            "\n",
            "[1/8] Starting Algeria...\n",
            "\n",
            "\ud83d\udcc2 Processing Algeria...\n",
            "   \ud83d\udd04 Loading dataset for Algeria...\n",
            "   \ud83d\udcca 727 samples loaded\n",
            "   \ud83d\udd0d Sample keys: ['audio', 'ID', 'duration']\n",
            "   \ud83c\udfb5 Audio keys: ['path', 'array', 'sampling_rate']\n",
            "   \ud83d\udcd0 Audio shape: (31929,)\n",
            "   \ud83d\udce6 Created data loader with 727 batches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   0%|          | 0/727 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 31929])\n",
            "   \ud83d\udd04 Processing batch 0: torch.Size([1, 31929]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   0%|          | 1/727 [00:01<16:17,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 24937])\n",
            "   \ud83d\udd04 Processing batch 1: torch.Size([1, 24937]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   0%|          | 2/727 [00:01<10:50,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 33983])\n",
            "   \ud83d\udd04 Processing batch 2: torch.Size([1, 33983]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   0%|          | 3/727 [00:02<09:20,  1.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 19119])\n",
            "   \ud83d\udd04 Processing batch 3: torch.Size([1, 19119]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   1%|          | 4/727 [00:02<07:24,  1.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 34736])\n",
            "   \ud83d\udd04 Processing batch 4: torch.Size([1, 34736]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   1%|          | 5/727 [00:03<07:25,  1.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 38976])\n",
            "   \ud83d\udd04 Processing batch 5: torch.Size([1, 38976]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   1%|          | 6/727 [00:04<07:26,  1.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 38290])\n",
            "   \ud83d\udd04 Processing batch 6: torch.Size([1, 38290]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   1%|          | 7/727 [00:04<06:47,  1.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 24217])\n",
            "   \ud83d\udd04 Processing batch 7: torch.Size([1, 24217]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   1%|          | 8/727 [00:05<06:20,  1.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 48348])\n",
            "   \ud83d\udd04 Processing batch 8: torch.Size([1, 48348]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   1%|          | 9/727 [00:05<06:32,  1.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 27652])\n",
            "   \ud83d\udd04 Processing batch 9: torch.Size([1, 27652]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   1%|\u258f         | 10/727 [00:06<06:22,  1.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 43014])\n",
            "   \ud83d\udd04 Processing batch 10: torch.Size([1, 43014]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   2%|\u258f         | 11/727 [00:06<06:14,  1.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 32002])\n",
            "   \ud83d\udd04 Processing batch 11: torch.Size([1, 32002]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   2%|\u258f         | 12/727 [00:07<05:50,  2.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 26677])\n",
            "   \ud83d\udd04 Processing batch 12: torch.Size([1, 26677]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   2%|\u258f         | 13/727 [00:07<06:26,  1.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 34108])\n",
            "   \ud83d\udd04 Processing batch 13: torch.Size([1, 34108]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   2%|\u258f         | 14/727 [00:08<06:53,  1.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 38350])\n",
            "   \ud83d\udd04 Processing batch 14: torch.Size([1, 38350]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   2%|\u258f         | 15/727 [00:08<06:33,  1.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 163005])\n",
            "   \ud83d\udd04 Processing batch 15: torch.Size([1, 163005]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   2%|\u258f         | 16/727 [00:10<10:54,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 30769])\n",
            "   \ud83d\udd04 Processing batch 16: torch.Size([1, 30769]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   2%|\u258f         | 17/727 [00:11<09:50,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 32875])\n",
            "   \ud83d\udd04 Processing batch 17: torch.Size([1, 32875]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   2%|\u258f         | 18/727 [00:11<08:09,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 27236])\n",
            "   \ud83d\udd04 Processing batch 18: torch.Size([1, 27236]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   3%|\u258e         | 19/727 [00:11<06:50,  1.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 49682])\n",
            "   \ud83d\udd04 Processing batch 19: torch.Size([1, 49682]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   3%|\u258e         | 20/727 [00:12<05:54,  1.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 41127])\n",
            "   \ud83d\udd04 Processing batch 20: torch.Size([1, 41127]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   3%|\u258e         | 21/727 [00:12<05:51,  2.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 37366])\n",
            "   \ud83d\udd04 Processing batch 21: torch.Size([1, 37366]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   3%|\u258e         | 22/727 [00:13<05:24,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 32102])\n",
            "   \ud83d\udd04 Processing batch 22: torch.Size([1, 32102]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   3%|\u258e         | 23/727 [00:13<05:31,  2.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 39883])\n",
            "   \ud83d\udd04 Processing batch 23: torch.Size([1, 39883]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   3%|\u258e         | 24/727 [00:14<05:36,  2.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 41175])\n",
            "   \ud83d\udd04 Processing batch 24: torch.Size([1, 41175]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   3%|\u258e         | 25/727 [00:14<06:10,  1.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 43650])\n",
            "   \ud83d\udd04 Processing batch 25: torch.Size([1, 43650]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   4%|\u258e         | 26/727 [00:15<05:35,  2.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 43351])\n",
            "   \ud83d\udd04 Processing batch 26: torch.Size([1, 43351]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   4%|\u258e         | 27/727 [00:15<05:18,  2.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 42099])\n",
            "   \ud83d\udd04 Processing batch 27: torch.Size([1, 42099]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   4%|\u258d         | 28/727 [00:16<06:04,  1.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 28: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   4%|\u258d         | 29/727 [00:16<06:27,  1.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 42486])\n",
            "   \ud83d\udd04 Processing batch 29: torch.Size([1, 42486]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   4%|\u258d         | 30/727 [00:17<06:39,  1.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 34082])\n",
            "   \ud83d\udd04 Processing batch 30: torch.Size([1, 34082]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   4%|\u258d         | 31/727 [00:17<06:03,  1.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 39457])\n",
            "   \ud83d\udd04 Processing batch 31: torch.Size([1, 39457]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   4%|\u258d         | 32/727 [00:18<05:19,  2.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 53962])\n",
            "   \ud83d\udd04 Processing batch 32: torch.Size([1, 53962]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   5%|\u258d         | 33/727 [00:18<05:32,  2.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 37801])\n",
            "   \ud83d\udd04 Processing batch 33: torch.Size([1, 37801]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   5%|\u258d         | 34/727 [00:19<05:44,  2.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 37215])\n",
            "   \ud83d\udd04 Processing batch 34: torch.Size([1, 37215]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   5%|\u258d         | 35/727 [00:19<05:32,  2.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 35: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   5%|\u258d         | 36/727 [00:30<40:27,  3.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 37130])\n",
            "   \ud83d\udd04 Processing batch 36: torch.Size([1, 37130]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   5%|\u258c         | 37/727 [00:30<29:32,  2.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 36321])\n",
            "   \ud83d\udd04 Processing batch 37: torch.Size([1, 36321]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   5%|\u258c         | 38/727 [00:31<21:54,  1.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 39451])\n",
            "   \ud83d\udd04 Processing batch 38: torch.Size([1, 39451]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   5%|\u258c         | 39/727 [00:31<17:23,  1.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 237833])\n",
            "   \ud83d\udd04 Processing batch 39: torch.Size([1, 237833]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   6%|\u258c         | 40/727 [00:33<18:03,  1.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 123788])\n",
            "   \ud83d\udd04 Processing batch 40: torch.Size([1, 123788]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   6%|\u258c         | 41/727 [00:34<15:01,  1.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 32661])\n",
            "   \ud83d\udd04 Processing batch 41: torch.Size([1, 32661]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   6%|\u258c         | 42/727 [00:34<12:36,  1.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 41494])\n",
            "   \ud83d\udd04 Processing batch 42: torch.Size([1, 41494]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   6%|\u258c         | 43/727 [00:35<10:56,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 123034])\n",
            "   \ud83d\udd04 Processing batch 43: torch.Size([1, 123034]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   6%|\u258c         | 44/727 [00:36<11:05,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 26094])\n",
            "   \ud83d\udd04 Processing batch 44: torch.Size([1, 26094]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   6%|\u258c         | 45/727 [00:36<08:56,  1.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 40727])\n",
            "   \ud83d\udd04 Processing batch 45: torch.Size([1, 40727]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   6%|\u258b         | 46/727 [00:37<07:46,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u26a0\ufe0f  Empty audio for sample 12213_1\n",
            "    \u2705 Batch processed: torch.Size([1, 16000])\n",
            "   \ud83d\udd04 Processing batch 46: torch.Size([1, 16000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   6%|\u258b         | 47/727 [00:37<07:48,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 37141])\n",
            "   \ud83d\udd04 Processing batch 47: torch.Size([1, 37141]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   7%|\u258b         | 48/727 [00:38<07:48,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 36082])\n",
            "   \ud83d\udd04 Processing batch 48: torch.Size([1, 36082]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   7%|\u258b         | 49/727 [00:38<06:49,  1.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 34739])\n",
            "   \ud83d\udd04 Processing batch 49: torch.Size([1, 34739]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   7%|\u258b         | 50/727 [00:39<06:25,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 26985])\n",
            "   \ud83d\udd04 Processing batch 50: torch.Size([1, 26985]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   7%|\u258b         | 51/727 [00:39<05:40,  1.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 35330])\n",
            "   \ud83d\udd04 Processing batch 51: torch.Size([1, 35330]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   7%|\u258b         | 52/727 [00:40<06:03,  1.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 41519])\n",
            "   \ud83d\udd04 Processing batch 52: torch.Size([1, 41519]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   7%|\u258b         | 53/727 [00:40<05:44,  1.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 106305])\n",
            "   \ud83d\udd04 Processing batch 53: torch.Size([1, 106305]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   7%|\u258b         | 54/727 [00:42<08:44,  1.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 43165])\n",
            "   \ud83d\udd04 Processing batch 54: torch.Size([1, 43165]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   8%|\u258a         | 55/727 [00:42<08:27,  1.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 43374])\n",
            "   \ud83d\udd04 Processing batch 55: torch.Size([1, 43374]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   8%|\u258a         | 56/727 [00:43<08:16,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 29538])\n",
            "   \ud83d\udd04 Processing batch 56: torch.Size([1, 29538]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   8%|\u258a         | 57/727 [00:44<07:51,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 43364])\n",
            "   \ud83d\udd04 Processing batch 57: torch.Size([1, 43364]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   8%|\u258a         | 58/727 [00:44<07:34,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 31882])\n",
            "   \ud83d\udd04 Processing batch 58: torch.Size([1, 31882]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   8%|\u258a         | 59/727 [00:45<06:56,  1.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 25326])\n",
            "   \ud83d\udd04 Processing batch 59: torch.Size([1, 25326]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   8%|\u258a         | 60/727 [00:45<06:10,  1.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 42453])\n",
            "   \ud83d\udd04 Processing batch 60: torch.Size([1, 42453]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   8%|\u258a         | 61/727 [00:46<06:39,  1.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 384504])\n",
            "   \ud83d\udd04 Processing batch 61: torch.Size([1, 384504]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   9%|\u258a         | 62/727 [00:49<15:09,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 36920])\n",
            "   \ud83d\udd04 Processing batch 62: torch.Size([1, 36920]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   9%|\u258a         | 63/727 [00:49<11:45,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 43739])\n",
            "   \ud83d\udd04 Processing batch 63: torch.Size([1, 43739]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   9%|\u2589         | 64/727 [00:50<09:43,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 39585])\n",
            "   \ud83d\udd04 Processing batch 64: torch.Size([1, 39585]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   9%|\u2589         | 65/727 [00:50<07:57,  1.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 37037])\n",
            "   \ud83d\udd04 Processing batch 65: torch.Size([1, 37037]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   9%|\u2589         | 66/727 [00:51<07:26,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 33736])\n",
            "   \ud83d\udd04 Processing batch 66: torch.Size([1, 33736]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   9%|\u2589         | 67/727 [00:51<06:31,  1.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 32536])\n",
            "   \ud83d\udd04 Processing batch 67: torch.Size([1, 32536]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   9%|\u2589         | 68/727 [00:52<05:52,  1.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 40556])\n",
            "   \ud83d\udd04 Processing batch 68: torch.Size([1, 40556]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:   9%|\u2589         | 69/727 [00:52<05:16,  2.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 31468])\n",
            "   \ud83d\udd04 Processing batch 69: torch.Size([1, 31468]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  10%|\u2589         | 70/727 [00:52<04:51,  2.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 43799])\n",
            "   \ud83d\udd04 Processing batch 70: torch.Size([1, 43799]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  10%|\u2589         | 71/727 [00:53<04:33,  2.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 39141])\n",
            "   \ud83d\udd04 Processing batch 71: torch.Size([1, 39141]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  10%|\u2589         | 72/727 [00:53<05:15,  2.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 112584])\n",
            "   \ud83d\udd04 Processing batch 72: torch.Size([1, 112584]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  10%|\u2588         | 73/727 [00:54<05:43,  1.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 22663])\n",
            "   \ud83d\udd04 Processing batch 73: torch.Size([1, 22663]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  10%|\u2588         | 74/727 [00:55<05:59,  1.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 114775])\n",
            "   \ud83d\udd04 Processing batch 74: torch.Size([1, 114775]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  10%|\u2588         | 75/727 [00:56<08:25,  1.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 139680])\n",
            "   \ud83d\udd04 Processing batch 75: torch.Size([1, 139680]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  10%|\u2588         | 76/727 [00:56<07:54,  1.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 235791])\n",
            "   \ud83d\udd04 Processing batch 76: torch.Size([1, 235791]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  11%|\u2588         | 77/727 [00:57<06:44,  1.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 154041])\n",
            "   \ud83d\udd04 Processing batch 77: torch.Size([1, 154041]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  11%|\u2588         | 78/727 [00:58<07:07,  1.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 225332])\n",
            "   \ud83d\udd04 Processing batch 78: torch.Size([1, 225332]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  11%|\u2588         | 79/727 [00:58<07:00,  1.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 113839])\n",
            "   \ud83d\udd04 Processing batch 79: torch.Size([1, 113839]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  11%|\u2588         | 80/727 [00:59<08:23,  1.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 328730])\n",
            "   \ud83d\udd04 Processing batch 80: torch.Size([1, 328730]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  11%|\u2588         | 81/727 [01:02<15:12,  1.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 193488])\n",
            "   \ud83d\udd04 Processing batch 81: torch.Size([1, 193488]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  11%|\u2588\u258f        | 82/727 [01:04<15:34,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 479257])\n",
            "   \ud83d\udd04 Processing batch 82: torch.Size([1, 479257]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  11%|\u2588\u258f        | 83/727 [01:08<24:03,  2.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 95960])\n",
            "   \ud83d\udd04 Processing batch 83: torch.Size([1, 95960]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  12%|\u2588\u258f        | 84/727 [01:08<18:46,  1.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 348884])\n",
            "   \ud83d\udd04 Processing batch 84: torch.Size([1, 348884]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  12%|\u2588\u258f        | 85/727 [01:10<18:05,  1.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 132286])\n",
            "   \ud83d\udd04 Processing batch 85: torch.Size([1, 132286]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  12%|\u2588\u258f        | 86/727 [01:11<14:38,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 212544])\n",
            "   \ud83d\udd04 Processing batch 86: torch.Size([1, 212544]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  12%|\u2588\u258f        | 87/727 [01:11<11:39,  1.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 110569])\n",
            "   \ud83d\udd04 Processing batch 87: torch.Size([1, 110569]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  12%|\u2588\u258f        | 88/727 [01:12<11:14,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 88: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  12%|\u2588\u258f        | 89/727 [01:22<39:06,  3.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 139163])\n",
            "   \ud83d\udd04 Processing batch 89: torch.Size([1, 139163]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  12%|\u2588\u258f        | 90/727 [01:23<30:54,  2.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 376816])\n",
            "   \ud83d\udd04 Processing batch 90: torch.Size([1, 376816]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  13%|\u2588\u258e        | 91/727 [01:26<31:50,  3.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 57093])\n",
            "   \ud83d\udd04 Processing batch 91: torch.Size([1, 57093]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  13%|\u2588\u258e        | 92/727 [01:27<24:11,  2.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 83357])\n",
            "   \ud83d\udd04 Processing batch 92: torch.Size([1, 83357]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  13%|\u2588\u258e        | 93/727 [01:27<18:51,  1.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 98906])\n",
            "   \ud83d\udd04 Processing batch 93: torch.Size([1, 98906]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  13%|\u2588\u258e        | 94/727 [01:28<15:10,  1.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 162905])\n",
            "   \ud83d\udd04 Processing batch 94: torch.Size([1, 162905]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  13%|\u2588\u258e        | 95/727 [01:29<12:34,  1.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 233601])\n",
            "   \ud83d\udd04 Processing batch 95: torch.Size([1, 233601]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  13%|\u2588\u258e        | 96/727 [01:29<10:13,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 147220])\n",
            "   \ud83d\udd04 Processing batch 96: torch.Size([1, 147220]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  13%|\u2588\u258e        | 97/727 [01:30<09:07,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 187369])\n",
            "   \ud83d\udd04 Processing batch 97: torch.Size([1, 187369]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  13%|\u2588\u258e        | 98/727 [01:30<08:36,  1.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 267982])\n",
            "   \ud83d\udd04 Processing batch 98: torch.Size([1, 267982]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  14%|\u2588\u258e        | 99/727 [01:31<07:58,  1.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 54761])\n",
            "   \ud83d\udd04 Processing batch 99: torch.Size([1, 54761]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  14%|\u2588\u258d        | 100/727 [01:31<06:58,  1.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 245845])\n",
            "   \ud83d\udd04 Processing batch 100: torch.Size([1, 245845]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  14%|\u2588\u258d        | 101/727 [01:32<06:48,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 130496])\n",
            "   \ud83d\udd04 Processing batch 101: torch.Size([1, 130496]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  14%|\u2588\u258d        | 102/727 [01:33<06:40,  1.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 124670])\n",
            "   \ud83d\udd04 Processing batch 102: torch.Size([1, 124670]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  14%|\u2588\u258d        | 103/727 [01:33<06:37,  1.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 125582])\n",
            "   \ud83d\udd04 Processing batch 103: torch.Size([1, 125582]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  14%|\u2588\u258d        | 104/727 [01:34<06:32,  1.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 104: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  14%|\u2588\u258d        | 105/727 [01:37<14:17,  1.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 257975])\n",
            "   \ud83d\udd04 Processing batch 105: torch.Size([1, 257975]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  15%|\u2588\u258d        | 106/727 [01:38<13:17,  1.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 321485])\n",
            "   \ud83d\udd04 Processing batch 106: torch.Size([1, 321485]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  15%|\u2588\u258d        | 107/727 [01:39<11:30,  1.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 104653])\n",
            "   \ud83d\udd04 Processing batch 107: torch.Size([1, 104653]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  15%|\u2588\u258d        | 108/727 [01:39<09:56,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 93315])\n",
            "   \ud83d\udd04 Processing batch 108: torch.Size([1, 93315]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  15%|\u2588\u258d        | 109/727 [01:40<08:44,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 251000])\n",
            "   \ud83d\udd04 Processing batch 109: torch.Size([1, 251000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  15%|\u2588\u258c        | 110/727 [01:41<08:03,  1.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 98264])\n",
            "   \ud83d\udd04 Processing batch 110: torch.Size([1, 98264]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  15%|\u2588\u258c        | 111/727 [01:41<07:32,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 56151])\n",
            "   \ud83d\udd04 Processing batch 111: torch.Size([1, 56151]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  15%|\u2588\u258c        | 112/727 [01:42<07:11,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 129135])\n",
            "   \ud83d\udd04 Processing batch 112: torch.Size([1, 129135]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  16%|\u2588\u258c        | 113/727 [01:43<08:07,  1.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 195263])\n",
            "   \ud83d\udd04 Processing batch 113: torch.Size([1, 195263]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  16%|\u2588\u258c        | 114/727 [01:44<07:35,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 22484])\n",
            "   \ud83d\udd04 Processing batch 114: torch.Size([1, 22484]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  16%|\u2588\u258c        | 115/727 [01:44<06:26,  1.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 62735])\n",
            "   \ud83d\udd04 Processing batch 115: torch.Size([1, 62735]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  16%|\u2588\u258c        | 116/727 [01:45<06:23,  1.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 129991])\n",
            "   \ud83d\udd04 Processing batch 116: torch.Size([1, 129991]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  16%|\u2588\u258c        | 117/727 [01:46<07:55,  1.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 94508])\n",
            "   \ud83d\udd04 Processing batch 117: torch.Size([1, 94508]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  16%|\u2588\u258c        | 118/727 [01:46<06:39,  1.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 271844])\n",
            "   \ud83d\udd04 Processing batch 118: torch.Size([1, 271844]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  16%|\u2588\u258b        | 119/727 [01:48<11:06,  1.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 88137])\n",
            "   \ud83d\udd04 Processing batch 119: torch.Size([1, 88137]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  17%|\u2588\u258b        | 120/727 [01:49<08:50,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 100947])\n",
            "   \ud83d\udd04 Processing batch 120: torch.Size([1, 100947]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  17%|\u2588\u258b        | 121/727 [01:49<08:04,  1.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 23133])\n",
            "   \ud83d\udd04 Processing batch 121: torch.Size([1, 23133]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  17%|\u2588\u258b        | 122/727 [01:50<07:31,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 46267])\n",
            "   \ud83d\udd04 Processing batch 122: torch.Size([1, 46267]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  17%|\u2588\u258b        | 123/727 [01:51<07:25,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 71504])\n",
            "   \ud83d\udd04 Processing batch 123: torch.Size([1, 71504]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  17%|\u2588\u258b        | 124/727 [01:51<06:19,  1.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 111088])\n",
            "   \ud83d\udd04 Processing batch 124: torch.Size([1, 111088]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  17%|\u2588\u258b        | 125/727 [01:52<08:22,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 310413])\n",
            "   \ud83d\udd04 Processing batch 125: torch.Size([1, 310413]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  17%|\u2588\u258b        | 126/727 [01:55<13:23,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 55224])\n",
            "   \ud83d\udd04 Processing batch 126: torch.Size([1, 55224]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  17%|\u2588\u258b        | 127/727 [01:55<10:26,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 147509])\n",
            "   \ud83d\udd04 Processing batch 127: torch.Size([1, 147509]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  18%|\u2588\u258a        | 128/727 [02:03<30:46,  3.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 113208])\n",
            "   \ud83d\udd04 Processing batch 128: torch.Size([1, 113208]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  18%|\u2588\u258a        | 129/727 [02:03<22:34,  2.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 251470])\n",
            "   \ud83d\udd04 Processing batch 129: torch.Size([1, 251470]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  18%|\u2588\u258a        | 130/727 [02:06<23:04,  2.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 122622])\n",
            "   \ud83d\udd04 Processing batch 130: torch.Size([1, 122622]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  18%|\u2588\u258a        | 131/727 [02:07<20:17,  2.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 148555])\n",
            "   \ud83d\udd04 Processing batch 131: torch.Size([1, 148555]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  18%|\u2588\u258a        | 132/727 [02:08<17:09,  1.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 179873])\n",
            "   \ud83d\udd04 Processing batch 132: torch.Size([1, 179873]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  18%|\u2588\u258a        | 133/727 [02:09<16:01,  1.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 130533])\n",
            "   \ud83d\udd04 Processing batch 133: torch.Size([1, 130533]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  18%|\u2588\u258a        | 134/727 [02:11<15:03,  1.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 46183])\n",
            "   \ud83d\udd04 Processing batch 134: torch.Size([1, 46183]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  19%|\u2588\u258a        | 135/727 [02:11<11:56,  1.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 87958])\n",
            "   \ud83d\udd04 Processing batch 135: torch.Size([1, 87958]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  19%|\u2588\u258a        | 136/727 [02:12<09:48,  1.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 110492])\n",
            "   \ud83d\udd04 Processing batch 136: torch.Size([1, 110492]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  19%|\u2588\u2589        | 137/727 [02:12<08:48,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 166081])\n",
            "   \ud83d\udd04 Processing batch 137: torch.Size([1, 166081]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  19%|\u2588\u2589        | 138/727 [02:14<09:55,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 218876])\n",
            "   \ud83d\udd04 Processing batch 138: torch.Size([1, 218876]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  19%|\u2588\u2589        | 139/727 [02:15<12:04,  1.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 91272])\n",
            "   \ud83d\udd04 Processing batch 139: torch.Size([1, 91272]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  19%|\u2588\u2589        | 140/727 [02:16<10:15,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 459505])\n",
            "   \ud83d\udd04 Processing batch 140: torch.Size([1, 459505]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  19%|\u2588\u2589        | 141/727 [02:17<09:55,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 167614])\n",
            "   \ud83d\udd04 Processing batch 141: torch.Size([1, 167614]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  20%|\u2588\u2589        | 142/727 [02:19<11:32,  1.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 142: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  20%|\u2588\u2589        | 143/727 [02:24<23:01,  2.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 66385])\n",
            "   \ud83d\udd04 Processing batch 143: torch.Size([1, 66385]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  20%|\u2588\u2589        | 144/727 [02:24<17:08,  1.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 121557])\n",
            "   \ud83d\udd04 Processing batch 144: torch.Size([1, 121557]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  20%|\u2588\u2589        | 145/727 [02:25<13:48,  1.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 151959])\n",
            "   \ud83d\udd04 Processing batch 145: torch.Size([1, 151959]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  20%|\u2588\u2588        | 146/727 [02:26<13:35,  1.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 127966])\n",
            "   \ud83d\udd04 Processing batch 146: torch.Size([1, 127966]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  20%|\u2588\u2588        | 147/727 [02:27<12:16,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 70735])\n",
            "   \ud83d\udd04 Processing batch 147: torch.Size([1, 70735]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  20%|\u2588\u2588        | 148/727 [02:28<10:43,  1.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 120968])\n",
            "   \ud83d\udd04 Processing batch 148: torch.Size([1, 120968]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  20%|\u2588\u2588        | 149/727 [02:28<08:46,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 79169])\n",
            "   \ud83d\udd04 Processing batch 149: torch.Size([1, 79169]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  21%|\u2588\u2588        | 150/727 [02:29<07:09,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 181340])\n",
            "   \ud83d\udd04 Processing batch 150: torch.Size([1, 181340]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  21%|\u2588\u2588        | 151/727 [02:30<07:47,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 106030])\n",
            "   \ud83d\udd04 Processing batch 151: torch.Size([1, 106030]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  21%|\u2588\u2588        | 152/727 [02:30<07:13,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 187902])\n",
            "   \ud83d\udd04 Processing batch 152: torch.Size([1, 187902]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  21%|\u2588\u2588        | 153/727 [02:31<06:48,  1.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 160087])\n",
            "   \ud83d\udd04 Processing batch 153: torch.Size([1, 160087]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  21%|\u2588\u2588        | 154/727 [02:31<06:33,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 216910])\n",
            "   \ud83d\udd04 Processing batch 154: torch.Size([1, 216910]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  21%|\u2588\u2588\u258f       | 155/727 [02:32<06:37,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 43666])\n",
            "   \ud83d\udd04 Processing batch 155: torch.Size([1, 43666]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  21%|\u2588\u2588\u258f       | 156/727 [02:33<06:22,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 148087])\n",
            "   \ud83d\udd04 Processing batch 156: torch.Size([1, 148087]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  22%|\u2588\u2588\u258f       | 157/727 [02:34<08:28,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 70213])\n",
            "   \ud83d\udd04 Processing batch 157: torch.Size([1, 70213]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  22%|\u2588\u2588\u258f       | 158/727 [02:35<07:10,  1.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 185044])\n",
            "   \ud83d\udd04 Processing batch 158: torch.Size([1, 185044]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  22%|\u2588\u2588\u258f       | 159/727 [02:44<32:39,  3.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 295919])\n",
            "   \ud83d\udd04 Processing batch 159: torch.Size([1, 295919]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  22%|\u2588\u2588\u258f       | 160/727 [02:46<28:29,  3.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 96386])\n",
            "   \ud83d\udd04 Processing batch 160: torch.Size([1, 96386]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  22%|\u2588\u2588\u258f       | 161/727 [02:47<23:05,  2.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 79826])\n",
            "   \ud83d\udd04 Processing batch 161: torch.Size([1, 79826]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  22%|\u2588\u2588\u258f       | 162/727 [02:48<18:07,  1.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 143060])\n",
            "   \ud83d\udd04 Processing batch 162: torch.Size([1, 143060]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  22%|\u2588\u2588\u258f       | 163/727 [02:49<15:40,  1.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 194633])\n",
            "   \ud83d\udd04 Processing batch 163: torch.Size([1, 194633]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  23%|\u2588\u2588\u258e       | 164/727 [02:51<16:23,  1.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 170865])\n",
            "   \ud83d\udd04 Processing batch 164: torch.Size([1, 170865]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  23%|\u2588\u2588\u258e       | 165/727 [02:53<16:02,  1.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 183154])\n",
            "   \ud83d\udd04 Processing batch 165: torch.Size([1, 183154]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  23%|\u2588\u2588\u258e       | 166/727 [02:54<14:40,  1.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 263741])\n",
            "   \ud83d\udd04 Processing batch 166: torch.Size([1, 263741]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  23%|\u2588\u2588\u258e       | 167/727 [02:56<16:00,  1.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 225483])\n",
            "   \ud83d\udd04 Processing batch 167: torch.Size([1, 225483]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  23%|\u2588\u2588\u258e       | 168/727 [02:58<16:28,  1.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 139677])\n",
            "   \ud83d\udd04 Processing batch 168: torch.Size([1, 139677]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  23%|\u2588\u2588\u258e       | 169/727 [03:00<16:14,  1.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 113237])\n",
            "   \ud83d\udd04 Processing batch 169: torch.Size([1, 113237]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  23%|\u2588\u2588\u258e       | 170/727 [03:01<14:43,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 137673])\n",
            "   \ud83d\udd04 Processing batch 170: torch.Size([1, 137673]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  24%|\u2588\u2588\u258e       | 171/727 [03:02<13:59,  1.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 72920])\n",
            "   \ud83d\udd04 Processing batch 171: torch.Size([1, 72920]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  24%|\u2588\u2588\u258e       | 172/727 [03:03<11:08,  1.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 104132])\n",
            "   \ud83d\udd04 Processing batch 172: torch.Size([1, 104132]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  24%|\u2588\u2588\u258d       | 173/727 [03:04<10:34,  1.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 54733])\n",
            "   \ud83d\udd04 Processing batch 173: torch.Size([1, 54733]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  24%|\u2588\u2588\u258d       | 174/727 [03:04<09:34,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 139317])\n",
            "   \ud83d\udd04 Processing batch 174: torch.Size([1, 139317]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  24%|\u2588\u2588\u258d       | 175/727 [03:05<09:00,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 155174])\n",
            "   \ud83d\udd04 Processing batch 175: torch.Size([1, 155174]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  24%|\u2588\u2588\u258d       | 176/727 [03:06<08:02,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 89426])\n",
            "   \ud83d\udd04 Processing batch 176: torch.Size([1, 89426]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  24%|\u2588\u2588\u258d       | 177/727 [03:07<08:09,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 64646])\n",
            "   \ud83d\udd04 Processing batch 177: torch.Size([1, 64646]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  24%|\u2588\u2588\u258d       | 178/727 [03:07<07:24,  1.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 163343])\n",
            "   \ud83d\udd04 Processing batch 178: torch.Size([1, 163343]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  25%|\u2588\u2588\u258d       | 179/727 [03:09<10:18,  1.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 196419])\n",
            "   \ud83d\udd04 Processing batch 179: torch.Size([1, 196419]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  25%|\u2588\u2588\u258d       | 180/727 [03:12<13:35,  1.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 221409])\n",
            "   \ud83d\udd04 Processing batch 180: torch.Size([1, 221409]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  25%|\u2588\u2588\u258d       | 181/727 [03:13<14:25,  1.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 189510])\n",
            "   \ud83d\udd04 Processing batch 181: torch.Size([1, 189510]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  25%|\u2588\u2588\u258c       | 182/727 [03:15<14:46,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 125767])\n",
            "   \ud83d\udd04 Processing batch 182: torch.Size([1, 125767]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  25%|\u2588\u2588\u258c       | 183/727 [03:17<13:50,  1.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 123871])\n",
            "   \ud83d\udd04 Processing batch 183: torch.Size([1, 123871]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  25%|\u2588\u2588\u258c       | 184/727 [03:17<11:21,  1.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 173373])\n",
            "   \ud83d\udd04 Processing batch 184: torch.Size([1, 173373]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  25%|\u2588\u2588\u258c       | 185/727 [03:19<12:00,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 80245])\n",
            "   \ud83d\udd04 Processing batch 185: torch.Size([1, 80245]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  26%|\u2588\u2588\u258c       | 186/727 [03:19<10:22,  1.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 110443])\n",
            "   \ud83d\udd04 Processing batch 186: torch.Size([1, 110443]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  26%|\u2588\u2588\u258c       | 187/727 [03:20<09:22,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 30548])\n",
            "   \ud83d\udd04 Processing batch 187: torch.Size([1, 30548]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  26%|\u2588\u2588\u258c       | 188/727 [03:21<08:12,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 149803])\n",
            "   \ud83d\udd04 Processing batch 188: torch.Size([1, 149803]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  26%|\u2588\u2588\u258c       | 189/727 [03:22<09:43,  1.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 133942])\n",
            "   \ud83d\udd04 Processing batch 189: torch.Size([1, 133942]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  26%|\u2588\u2588\u258c       | 190/727 [03:23<10:04,  1.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 134515])\n",
            "   \ud83d\udd04 Processing batch 190: torch.Size([1, 134515]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  26%|\u2588\u2588\u258b       | 191/727 [03:24<08:41,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 142523])\n",
            "   \ud83d\udd04 Processing batch 191: torch.Size([1, 142523]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  26%|\u2588\u2588\u258b       | 192/727 [03:26<10:35,  1.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 141384])\n",
            "   \ud83d\udd04 Processing batch 192: torch.Size([1, 141384]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  27%|\u2588\u2588\u258b       | 193/727 [03:28<12:29,  1.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 35290])\n",
            "   \ud83d\udd04 Processing batch 193: torch.Size([1, 35290]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  27%|\u2588\u2588\u258b       | 194/727 [03:28<10:28,  1.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 111819])\n",
            "   \ud83d\udd04 Processing batch 194: torch.Size([1, 111819]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  27%|\u2588\u2588\u258b       | 195/727 [03:29<09:45,  1.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 336350])\n",
            "   \ud83d\udd04 Processing batch 195: torch.Size([1, 336350]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  27%|\u2588\u2588\u258b       | 196/727 [03:31<10:07,  1.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 134681])\n",
            "   \ud83d\udd04 Processing batch 196: torch.Size([1, 134681]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  27%|\u2588\u2588\u258b       | 197/727 [03:34<15:13,  1.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 69714])\n",
            "   \ud83d\udd04 Processing batch 197: torch.Size([1, 69714]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  27%|\u2588\u2588\u258b       | 198/727 [03:34<12:17,  1.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 215426])\n",
            "   \ud83d\udd04 Processing batch 198: torch.Size([1, 215426]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  27%|\u2588\u2588\u258b       | 199/727 [03:35<10:16,  1.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 65829])\n",
            "   \ud83d\udd04 Processing batch 199: torch.Size([1, 65829]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  28%|\u2588\u2588\u258a       | 200/727 [03:35<08:07,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 230867])\n",
            "   \ud83d\udd04 Processing batch 200: torch.Size([1, 230867]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  28%|\u2588\u2588\u258a       | 201/727 [03:38<12:00,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 69764])\n",
            "   \ud83d\udd04 Processing batch 201: torch.Size([1, 69764]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  28%|\u2588\u2588\u258a       | 202/727 [03:38<09:20,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 91140])\n",
            "   \ud83d\udd04 Processing batch 202: torch.Size([1, 91140]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  28%|\u2588\u2588\u258a       | 203/727 [03:39<09:09,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 51536])\n",
            "   \ud83d\udd04 Processing batch 203: torch.Size([1, 51536]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  28%|\u2588\u2588\u258a       | 204/727 [03:40<08:00,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 300889])\n",
            "   \ud83d\udd04 Processing batch 204: torch.Size([1, 300889]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  28%|\u2588\u2588\u258a       | 205/727 [03:42<13:03,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 300248])\n",
            "   \ud83d\udd04 Processing batch 205: torch.Size([1, 300248]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  28%|\u2588\u2588\u258a       | 206/727 [03:45<14:57,  1.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 55690])\n",
            "   \ud83d\udd04 Processing batch 206: torch.Size([1, 55690]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  28%|\u2588\u2588\u258a       | 207/727 [03:45<12:17,  1.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 53829])\n",
            "   \ud83d\udd04 Processing batch 207: torch.Size([1, 53829]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  29%|\u2588\u2588\u258a       | 208/727 [03:46<09:31,  1.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 205862])\n",
            "   \ud83d\udd04 Processing batch 208: torch.Size([1, 205862]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  29%|\u2588\u2588\u258a       | 209/727 [03:48<11:23,  1.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 168222])\n",
            "   \ud83d\udd04 Processing batch 209: torch.Size([1, 168222]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  29%|\u2588\u2588\u2589       | 210/727 [03:49<12:13,  1.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 467865])\n",
            "   \ud83d\udd04 Processing batch 210: torch.Size([1, 467865]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  29%|\u2588\u2588\u2589       | 211/727 [03:53<17:08,  1.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 68636])\n",
            "   \ud83d\udd04 Processing batch 211: torch.Size([1, 68636]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  29%|\u2588\u2588\u2589       | 212/727 [03:53<14:07,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 283780])\n",
            "   \ud83d\udd04 Processing batch 212: torch.Size([1, 283780]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  29%|\u2588\u2588\u2589       | 213/727 [03:54<11:28,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 113513])\n",
            "   \ud83d\udd04 Processing batch 213: torch.Size([1, 113513]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  29%|\u2588\u2588\u2589       | 214/727 [03:54<08:59,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 131991])\n",
            "   \ud83d\udd04 Processing batch 214: torch.Size([1, 131991]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  30%|\u2588\u2588\u2589       | 215/727 [03:55<07:53,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 387914])\n",
            "   \ud83d\udd04 Processing batch 215: torch.Size([1, 387914]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  30%|\u2588\u2588\u2589       | 216/727 [03:59<14:47,  1.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 229301])\n",
            "   \ud83d\udd04 Processing batch 216: torch.Size([1, 229301]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  30%|\u2588\u2588\u2589       | 217/727 [04:01<15:03,  1.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 70723])\n",
            "   \ud83d\udd04 Processing batch 217: torch.Size([1, 70723]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  30%|\u2588\u2588\u2589       | 218/727 [04:01<11:45,  1.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 237888])\n",
            "   \ud83d\udd04 Processing batch 218: torch.Size([1, 237888]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  30%|\u2588\u2588\u2588       | 219/727 [04:03<14:11,  1.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 127390])\n",
            "   \ud83d\udd04 Processing batch 219: torch.Size([1, 127390]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  30%|\u2588\u2588\u2588       | 220/727 [04:04<11:50,  1.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 225135])\n",
            "   \ud83d\udd04 Processing batch 220: torch.Size([1, 225135]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  30%|\u2588\u2588\u2588       | 221/727 [04:05<11:17,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 305105])\n",
            "   \ud83d\udd04 Processing batch 221: torch.Size([1, 305105]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  31%|\u2588\u2588\u2588       | 222/727 [04:08<13:40,  1.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 142221])\n",
            "   \ud83d\udd04 Processing batch 222: torch.Size([1, 142221]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  31%|\u2588\u2588\u2588       | 223/727 [04:17<34:13,  4.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 335629])\n",
            "   \ud83d\udd04 Processing batch 223: torch.Size([1, 335629]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  31%|\u2588\u2588\u2588       | 224/727 [04:27<48:31,  5.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 89849])\n",
            "   \ud83d\udd04 Processing batch 224: torch.Size([1, 89849]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  31%|\u2588\u2588\u2588       | 225/727 [04:28<36:25,  4.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 272051])\n",
            "   \ud83d\udd04 Processing batch 225: torch.Size([1, 272051]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  31%|\u2588\u2588\u2588       | 226/727 [04:30<30:11,  3.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 287246])\n",
            "   \ud83d\udd04 Processing batch 226: torch.Size([1, 287246]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  31%|\u2588\u2588\u2588       | 227/727 [04:30<22:01,  2.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 172929])\n",
            "   \ud83d\udd04 Processing batch 227: torch.Size([1, 172929]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  31%|\u2588\u2588\u2588\u258f      | 228/727 [04:32<19:13,  2.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 280837])\n",
            "   \ud83d\udd04 Processing batch 228: torch.Size([1, 280837]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  31%|\u2588\u2588\u2588\u258f      | 229/727 [04:33<15:00,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 56387])\n",
            "   \ud83d\udd04 Processing batch 229: torch.Size([1, 56387]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  32%|\u2588\u2588\u2588\u258f      | 230/727 [04:33<12:01,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 77290])\n",
            "   \ud83d\udd04 Processing batch 230: torch.Size([1, 77290]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  32%|\u2588\u2588\u2588\u258f      | 231/727 [04:34<09:55,  1.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 46179])\n",
            "   \ud83d\udd04 Processing batch 231: torch.Size([1, 46179]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  32%|\u2588\u2588\u2588\u258f      | 232/727 [04:34<08:28,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 140803])\n",
            "   \ud83d\udd04 Processing batch 232: torch.Size([1, 140803]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  32%|\u2588\u2588\u2588\u258f      | 233/727 [04:36<09:15,  1.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 181092])\n",
            "   \ud83d\udd04 Processing batch 233: torch.Size([1, 181092]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  32%|\u2588\u2588\u2588\u258f      | 234/727 [04:38<10:36,  1.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 212447])\n",
            "   \ud83d\udd04 Processing batch 234: torch.Size([1, 212447]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  32%|\u2588\u2588\u2588\u258f      | 235/727 [04:39<12:07,  1.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 80903])\n",
            "   \ud83d\udd04 Processing batch 235: torch.Size([1, 80903]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  32%|\u2588\u2588\u2588\u258f      | 236/727 [04:40<09:58,  1.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 193226])\n",
            "   \ud83d\udd04 Processing batch 236: torch.Size([1, 193226]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  33%|\u2588\u2588\u2588\u258e      | 237/727 [04:42<10:54,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 149075])\n",
            "   \ud83d\udd04 Processing batch 237: torch.Size([1, 149075]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  33%|\u2588\u2588\u2588\u258e      | 238/727 [04:43<10:50,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 121726])\n",
            "   \ud83d\udd04 Processing batch 238: torch.Size([1, 121726]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  33%|\u2588\u2588\u2588\u258e      | 239/727 [04:45<11:22,  1.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 222114])\n",
            "   \ud83d\udd04 Processing batch 239: torch.Size([1, 222114]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  33%|\u2588\u2588\u2588\u258e      | 240/727 [04:46<11:16,  1.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 134762])\n",
            "   \ud83d\udd04 Processing batch 240: torch.Size([1, 134762]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  33%|\u2588\u2588\u2588\u258e      | 241/727 [04:47<10:26,  1.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 172820])\n",
            "   \ud83d\udd04 Processing batch 241: torch.Size([1, 172820]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  33%|\u2588\u2588\u2588\u258e      | 242/727 [04:49<11:29,  1.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 211993])\n",
            "   \ud83d\udd04 Processing batch 242: torch.Size([1, 211993]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  33%|\u2588\u2588\u2588\u258e      | 243/727 [04:49<09:31,  1.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 171349])\n",
            "   \ud83d\udd04 Processing batch 243: torch.Size([1, 171349]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  34%|\u2588\u2588\u2588\u258e      | 244/727 [04:50<08:21,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 286822])\n",
            "   \ud83d\udd04 Processing batch 244: torch.Size([1, 286822]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  34%|\u2588\u2588\u2588\u258e      | 245/727 [04:51<07:21,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 155869])\n",
            "   \ud83d\udd04 Processing batch 245: torch.Size([1, 155869]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  34%|\u2588\u2588\u2588\u258d      | 246/727 [04:52<08:54,  1.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 450688])\n",
            "   \ud83d\udd04 Processing batch 246: torch.Size([1, 450688]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  34%|\u2588\u2588\u2588\u258d      | 247/727 [04:56<14:41,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 180530])\n",
            "   \ud83d\udd04 Processing batch 247: torch.Size([1, 180530]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  34%|\u2588\u2588\u2588\u258d      | 248/727 [04:57<13:04,  1.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 408210])\n",
            "   \ud83d\udd04 Processing batch 248: torch.Size([1, 408210]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  34%|\u2588\u2588\u2588\u258d      | 249/727 [04:58<10:38,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 133436])\n",
            "   \ud83d\udd04 Processing batch 249: torch.Size([1, 133436]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  34%|\u2588\u2588\u2588\u258d      | 250/727 [04:59<10:40,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 323484])\n",
            "   \ud83d\udd04 Processing batch 250: torch.Size([1, 323484]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  35%|\u2588\u2588\u2588\u258d      | 251/727 [05:02<14:57,  1.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 87465])\n",
            "   \ud83d\udd04 Processing batch 251: torch.Size([1, 87465]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  35%|\u2588\u2588\u2588\u258d      | 252/727 [05:03<12:43,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 422065])\n",
            "   \ud83d\udd04 Processing batch 252: torch.Size([1, 422065]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  35%|\u2588\u2588\u2588\u258d      | 253/727 [05:10<24:28,  3.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 127037])\n",
            "   \ud83d\udd04 Processing batch 253: torch.Size([1, 127037]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  35%|\u2588\u2588\u2588\u258d      | 254/727 [05:11<19:35,  2.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 75468])\n",
            "   \ud83d\udd04 Processing batch 254: torch.Size([1, 75468]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  35%|\u2588\u2588\u2588\u258c      | 255/727 [05:12<15:44,  2.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 134819])\n",
            "   \ud83d\udd04 Processing batch 255: torch.Size([1, 134819]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  35%|\u2588\u2588\u2588\u258c      | 256/727 [05:13<13:34,  1.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 131459])\n",
            "   \ud83d\udd04 Processing batch 256: torch.Size([1, 131459]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  35%|\u2588\u2588\u2588\u258c      | 257/727 [05:14<11:43,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 116236])\n",
            "   \ud83d\udd04 Processing batch 257: torch.Size([1, 116236]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  35%|\u2588\u2588\u2588\u258c      | 258/727 [05:15<10:27,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 186650])\n",
            "   \ud83d\udd04 Processing batch 258: torch.Size([1, 186650]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  36%|\u2588\u2588\u2588\u258c      | 259/727 [05:16<11:34,  1.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 219438])\n",
            "   \ud83d\udd04 Processing batch 259: torch.Size([1, 219438]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  36%|\u2588\u2588\u2588\u258c      | 260/727 [05:19<13:25,  1.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 44088])\n",
            "   \ud83d\udd04 Processing batch 260: torch.Size([1, 44088]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  36%|\u2588\u2588\u2588\u258c      | 261/727 [05:19<10:49,  1.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 75210])\n",
            "   \ud83d\udd04 Processing batch 261: torch.Size([1, 75210]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  36%|\u2588\u2588\u2588\u258c      | 262/727 [05:20<09:26,  1.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 68726])\n",
            "   \ud83d\udd04 Processing batch 262: torch.Size([1, 68726]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  36%|\u2588\u2588\u2588\u258c      | 263/727 [05:21<08:32,  1.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 228222])\n",
            "   \ud83d\udd04 Processing batch 263: torch.Size([1, 228222]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  36%|\u2588\u2588\u2588\u258b      | 264/727 [05:22<09:17,  1.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 184413])\n",
            "   \ud83d\udd04 Processing batch 264: torch.Size([1, 184413]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  36%|\u2588\u2588\u2588\u258b      | 265/727 [05:33<30:50,  4.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 73450])\n",
            "   \ud83d\udd04 Processing batch 265: torch.Size([1, 73450]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  37%|\u2588\u2588\u2588\u258b      | 266/727 [05:34<23:20,  3.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 193221])\n",
            "   \ud83d\udd04 Processing batch 266: torch.Size([1, 193221]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  37%|\u2588\u2588\u2588\u258b      | 267/727 [05:35<20:06,  2.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 264483])\n",
            "   \ud83d\udd04 Processing batch 267: torch.Size([1, 264483]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  37%|\u2588\u2588\u2588\u258b      | 268/727 [05:36<15:31,  2.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 196037])\n",
            "   \ud83d\udd04 Processing batch 268: torch.Size([1, 196037]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  37%|\u2588\u2588\u2588\u258b      | 269/727 [05:38<15:17,  2.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 71430])\n",
            "   \ud83d\udd04 Processing batch 269: torch.Size([1, 71430]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  37%|\u2588\u2588\u2588\u258b      | 270/727 [05:39<12:22,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 196037])\n",
            "   \ud83d\udd04 Processing batch 270: torch.Size([1, 196037]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  37%|\u2588\u2588\u2588\u258b      | 271/727 [05:40<12:00,  1.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 271: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  37%|\u2588\u2588\u2588\u258b      | 272/727 [05:44<16:08,  2.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 136682])\n",
            "   \ud83d\udd04 Processing batch 272: torch.Size([1, 136682]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  38%|\u2588\u2588\u2588\u258a      | 273/727 [05:45<13:38,  1.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 383441])\n",
            "   \ud83d\udd04 Processing batch 273: torch.Size([1, 383441]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  38%|\u2588\u2588\u2588\u258a      | 274/727 [05:46<11:39,  1.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 244426])\n",
            "   \ud83d\udd04 Processing batch 274: torch.Size([1, 244426]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  38%|\u2588\u2588\u2588\u258a      | 275/727 [05:47<11:47,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 167674])\n",
            "   \ud83d\udd04 Processing batch 275: torch.Size([1, 167674]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  38%|\u2588\u2588\u2588\u258a      | 276/727 [05:58<32:07,  4.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 119261])\n",
            "   \ud83d\udd04 Processing batch 276: torch.Size([1, 119261]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  38%|\u2588\u2588\u2588\u258a      | 277/727 [05:59<24:26,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 129888])\n",
            "   \ud83d\udd04 Processing batch 277: torch.Size([1, 129888]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  38%|\u2588\u2588\u2588\u258a      | 278/727 [05:59<18:05,  2.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 119261])\n",
            "   \ud83d\udd04 Processing batch 278: torch.Size([1, 119261]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  38%|\u2588\u2588\u2588\u258a      | 279/727 [06:00<14:13,  1.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 160589])\n",
            "   \ud83d\udd04 Processing batch 279: torch.Size([1, 160589]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  39%|\u2588\u2588\u2588\u258a      | 280/727 [06:00<11:31,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 64944])\n",
            "   \ud83d\udd04 Processing batch 280: torch.Size([1, 64944]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  39%|\u2588\u2588\u2588\u258a      | 281/727 [06:01<08:51,  1.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 166493])\n",
            "   \ud83d\udd04 Processing batch 281: torch.Size([1, 166493]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  39%|\u2588\u2588\u2588\u2589      | 282/727 [06:01<07:33,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 121080])\n",
            "   \ud83d\udd04 Processing batch 282: torch.Size([1, 121080]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  39%|\u2588\u2588\u2588\u2589      | 283/727 [06:02<07:09,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 251880])\n",
            "   \ud83d\udd04 Processing batch 283: torch.Size([1, 251880]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  39%|\u2588\u2588\u2588\u2589      | 284/727 [06:04<09:14,  1.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 178415])\n",
            "   \ud83d\udd04 Processing batch 284: torch.Size([1, 178415]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  39%|\u2588\u2588\u2588\u2589      | 285/727 [06:06<10:44,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 153103])\n",
            "   \ud83d\udd04 Processing batch 285: torch.Size([1, 153103]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  39%|\u2588\u2588\u2588\u2589      | 286/727 [06:07<09:48,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 329396])\n",
            "   \ud83d\udd04 Processing batch 286: torch.Size([1, 329396]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  39%|\u2588\u2588\u2588\u2589      | 287/727 [06:10<12:32,  1.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 145861])\n",
            "   \ud83d\udd04 Processing batch 287: torch.Size([1, 145861]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  40%|\u2588\u2588\u2588\u2589      | 288/727 [06:11<12:22,  1.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 192304])\n",
            "   \ud83d\udd04 Processing batch 288: torch.Size([1, 192304]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  40%|\u2588\u2588\u2588\u2589      | 289/727 [06:13<12:44,  1.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 145623])\n",
            "   \ud83d\udd04 Processing batch 289: torch.Size([1, 145623]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  40%|\u2588\u2588\u2588\u2589      | 290/727 [06:15<12:36,  1.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 41607])\n",
            "   \ud83d\udd04 Processing batch 290: torch.Size([1, 41607]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  40%|\u2588\u2588\u2588\u2588      | 291/727 [06:16<09:56,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 115417])\n",
            "   \ud83d\udd04 Processing batch 291: torch.Size([1, 115417]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  40%|\u2588\u2588\u2588\u2588      | 292/727 [06:16<08:39,  1.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 98709])\n",
            "   \ud83d\udd04 Processing batch 292: torch.Size([1, 98709]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  40%|\u2588\u2588\u2588\u2588      | 293/727 [06:17<08:07,  1.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 76774])\n",
            "   \ud83d\udd04 Processing batch 293: torch.Size([1, 76774]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  40%|\u2588\u2588\u2588\u2588      | 294/727 [06:18<07:17,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 109718])\n",
            "   \ud83d\udd04 Processing batch 294: torch.Size([1, 109718]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  41%|\u2588\u2588\u2588\u2588      | 295/727 [06:19<07:42,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 191030])\n",
            "   \ud83d\udd04 Processing batch 295: torch.Size([1, 191030]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  41%|\u2588\u2588\u2588\u2588      | 296/727 [06:22<10:26,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 113096])\n",
            "   \ud83d\udd04 Processing batch 296: torch.Size([1, 113096]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  41%|\u2588\u2588\u2588\u2588      | 297/727 [06:22<08:37,  1.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 146976])\n",
            "   \ud83d\udd04 Processing batch 297: torch.Size([1, 146976]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  41%|\u2588\u2588\u2588\u2588      | 298/727 [06:24<10:01,  1.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 335799])\n",
            "   \ud83d\udd04 Processing batch 298: torch.Size([1, 335799]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  41%|\u2588\u2588\u2588\u2588      | 299/727 [06:34<28:41,  4.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 106845])\n",
            "   \ud83d\udd04 Processing batch 299: torch.Size([1, 106845]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  41%|\u2588\u2588\u2588\u2588\u258f     | 300/727 [06:36<22:53,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 301457])\n",
            "   \ud83d\udd04 Processing batch 300: torch.Size([1, 301457]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  41%|\u2588\u2588\u2588\u2588\u258f     | 301/727 [06:38<21:46,  3.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 163698])\n",
            "   \ud83d\udd04 Processing batch 301: torch.Size([1, 163698]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  42%|\u2588\u2588\u2588\u2588\u258f     | 302/727 [06:39<17:03,  2.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 154364])\n",
            "   \ud83d\udd04 Processing batch 302: torch.Size([1, 154364]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  42%|\u2588\u2588\u2588\u2588\u258f     | 303/727 [06:40<14:37,  2.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 96347])\n",
            "   \ud83d\udd04 Processing batch 303: torch.Size([1, 96347]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  42%|\u2588\u2588\u2588\u2588\u258f     | 304/727 [06:41<11:37,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 74508])\n",
            "   \ud83d\udd04 Processing batch 304: torch.Size([1, 74508]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  42%|\u2588\u2588\u2588\u2588\u258f     | 305/727 [06:42<09:24,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 175994])\n",
            "   \ud83d\udd04 Processing batch 305: torch.Size([1, 175994]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  42%|\u2588\u2588\u2588\u2588\u258f     | 306/727 [06:43<09:23,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 155746])\n",
            "   \ud83d\udd04 Processing batch 306: torch.Size([1, 155746]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  42%|\u2588\u2588\u2588\u2588\u258f     | 307/727 [06:44<07:52,  1.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 95164])\n",
            "   \ud83d\udd04 Processing batch 307: torch.Size([1, 95164]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  42%|\u2588\u2588\u2588\u2588\u258f     | 308/727 [06:45<07:25,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 151389])\n",
            "   \ud83d\udd04 Processing batch 308: torch.Size([1, 151389]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  43%|\u2588\u2588\u2588\u2588\u258e     | 309/727 [06:46<08:56,  1.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 139998])\n",
            "   \ud83d\udd04 Processing batch 309: torch.Size([1, 139998]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  43%|\u2588\u2588\u2588\u2588\u258e     | 310/727 [06:47<07:32,  1.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 254608])\n",
            "   \ud83d\udd04 Processing batch 310: torch.Size([1, 254608]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  43%|\u2588\u2588\u2588\u2588\u258e     | 311/727 [06:49<09:26,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 340842])\n",
            "   \ud83d\udd04 Processing batch 311: torch.Size([1, 340842]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  43%|\u2588\u2588\u2588\u2588\u258e     | 312/727 [06:51<11:18,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 217303])\n",
            "   \ud83d\udd04 Processing batch 312: torch.Size([1, 217303]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  43%|\u2588\u2588\u2588\u2588\u258e     | 313/727 [06:52<09:06,  1.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 138418])\n",
            "   \ud83d\udd04 Processing batch 313: torch.Size([1, 138418]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  43%|\u2588\u2588\u2588\u2588\u258e     | 314/727 [06:53<08:58,  1.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 81178])\n",
            "   \ud83d\udd04 Processing batch 314: torch.Size([1, 81178]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  43%|\u2588\u2588\u2588\u2588\u258e     | 315/727 [06:54<08:29,  1.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 181901])\n",
            "   \ud83d\udd04 Processing batch 315: torch.Size([1, 181901]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  43%|\u2588\u2588\u2588\u2588\u258e     | 316/727 [06:56<09:45,  1.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 257751])\n",
            "   \ud83d\udd04 Processing batch 316: torch.Size([1, 257751]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  44%|\u2588\u2588\u2588\u2588\u258e     | 317/727 [06:58<10:40,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 87158])\n",
            "   \ud83d\udd04 Processing batch 317: torch.Size([1, 87158]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  44%|\u2588\u2588\u2588\u2588\u258e     | 318/727 [06:59<09:14,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 71396])\n",
            "   \ud83d\udd04 Processing batch 318: torch.Size([1, 71396]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  44%|\u2588\u2588\u2588\u2588\u258d     | 319/727 [07:00<08:00,  1.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 77886])\n",
            "   \ud83d\udd04 Processing batch 319: torch.Size([1, 77886]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  44%|\u2588\u2588\u2588\u2588\u258d     | 320/727 [07:00<07:24,  1.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 59837])\n",
            "   \ud83d\udd04 Processing batch 320: torch.Size([1, 59837]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  44%|\u2588\u2588\u2588\u2588\u258d     | 321/727 [07:01<06:30,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 191891])\n",
            "   \ud83d\udd04 Processing batch 321: torch.Size([1, 191891]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  44%|\u2588\u2588\u2588\u2588\u258d     | 322/727 [07:03<07:59,  1.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 86661])\n",
            "   \ud83d\udd04 Processing batch 322: torch.Size([1, 86661]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  44%|\u2588\u2588\u2588\u2588\u258d     | 323/727 [07:04<06:55,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 88724])\n",
            "   \ud83d\udd04 Processing batch 323: torch.Size([1, 88724]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  45%|\u2588\u2588\u2588\u2588\u258d     | 324/727 [07:04<06:20,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 161281])\n",
            "   \ud83d\udd04 Processing batch 324: torch.Size([1, 161281]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  45%|\u2588\u2588\u2588\u2588\u258d     | 325/727 [07:06<06:56,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 284423])\n",
            "   \ud83d\udd04 Processing batch 325: torch.Size([1, 284423]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  45%|\u2588\u2588\u2588\u2588\u258d     | 326/727 [07:06<05:45,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 179829])\n",
            "   \ud83d\udd04 Processing batch 326: torch.Size([1, 179829]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  45%|\u2588\u2588\u2588\u2588\u258d     | 327/727 [07:07<06:27,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 214536])\n",
            "   \ud83d\udd04 Processing batch 327: torch.Size([1, 214536]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  45%|\u2588\u2588\u2588\u2588\u258c     | 328/727 [07:09<08:19,  1.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 107107])\n",
            "   \ud83d\udd04 Processing batch 328: torch.Size([1, 107107]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  45%|\u2588\u2588\u2588\u2588\u258c     | 329/727 [07:10<07:03,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 69334])\n",
            "   \ud83d\udd04 Processing batch 329: torch.Size([1, 69334]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  45%|\u2588\u2588\u2588\u2588\u258c     | 330/727 [07:10<06:09,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 83038])\n",
            "   \ud83d\udd04 Processing batch 330: torch.Size([1, 83038]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  46%|\u2588\u2588\u2588\u2588\u258c     | 331/727 [07:11<05:38,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 209628])\n",
            "   \ud83d\udd04 Processing batch 331: torch.Size([1, 209628]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  46%|\u2588\u2588\u2588\u2588\u258c     | 332/727 [07:13<07:52,  1.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 194966])\n",
            "   \ud83d\udd04 Processing batch 332: torch.Size([1, 194966]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  46%|\u2588\u2588\u2588\u2588\u258c     | 333/727 [07:14<06:42,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 157369])\n",
            "   \ud83d\udd04 Processing batch 333: torch.Size([1, 157369]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  46%|\u2588\u2588\u2588\u2588\u258c     | 334/727 [07:14<05:55,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 101010])\n",
            "   \ud83d\udd04 Processing batch 334: torch.Size([1, 101010]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  46%|\u2588\u2588\u2588\u2588\u258c     | 335/727 [07:15<06:06,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 76824])\n",
            "   \ud83d\udd04 Processing batch 335: torch.Size([1, 76824]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  46%|\u2588\u2588\u2588\u2588\u258c     | 336/727 [07:16<05:17,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 167591])\n",
            "   \ud83d\udd04 Processing batch 336: torch.Size([1, 167591]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  46%|\u2588\u2588\u2588\u2588\u258b     | 337/727 [07:16<04:54,  1.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 119220])\n",
            "   \ud83d\udd04 Processing batch 337: torch.Size([1, 119220]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  46%|\u2588\u2588\u2588\u2588\u258b     | 338/727 [07:17<05:27,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 167809])\n",
            "   \ud83d\udd04 Processing batch 338: torch.Size([1, 167809]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  47%|\u2588\u2588\u2588\u2588\u258b     | 339/727 [07:19<07:40,  1.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 156147])\n",
            "   \ud83d\udd04 Processing batch 339: torch.Size([1, 156147]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  47%|\u2588\u2588\u2588\u2588\u258b     | 340/727 [07:21<07:53,  1.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 342007])\n",
            "   \ud83d\udd04 Processing batch 340: torch.Size([1, 342007]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  47%|\u2588\u2588\u2588\u2588\u258b     | 341/727 [07:23<10:22,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 65897])\n",
            "   \ud83d\udd04 Processing batch 341: torch.Size([1, 65897]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  47%|\u2588\u2588\u2588\u2588\u258b     | 342/727 [07:24<08:30,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 183533])\n",
            "   \ud83d\udd04 Processing batch 342: torch.Size([1, 183533]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  47%|\u2588\u2588\u2588\u2588\u258b     | 343/727 [07:26<09:41,  1.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 344904])\n",
            "   \ud83d\udd04 Processing batch 343: torch.Size([1, 344904]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  47%|\u2588\u2588\u2588\u2588\u258b     | 344/727 [07:28<10:33,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 132769])\n",
            "   \ud83d\udd04 Processing batch 344: torch.Size([1, 132769]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  47%|\u2588\u2588\u2588\u2588\u258b     | 345/727 [07:29<10:19,  1.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 149977])\n",
            "   \ud83d\udd04 Processing batch 345: torch.Size([1, 149977]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  48%|\u2588\u2588\u2588\u2588\u258a     | 346/727 [07:31<09:32,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 346: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  48%|\u2588\u2588\u2588\u2588\u258a     | 347/727 [07:34<13:16,  2.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 156061])\n",
            "   \ud83d\udd04 Processing batch 347: torch.Size([1, 156061]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  48%|\u2588\u2588\u2588\u2588\u258a     | 348/727 [07:35<10:27,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 76741])\n",
            "   \ud83d\udd04 Processing batch 348: torch.Size([1, 76741]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  48%|\u2588\u2588\u2588\u2588\u258a     | 349/727 [07:36<08:57,  1.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 49170])\n",
            "   \ud83d\udd04 Processing batch 349: torch.Size([1, 49170]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  48%|\u2588\u2588\u2588\u2588\u258a     | 350/727 [07:36<06:56,  1.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 192443])\n",
            "   \ud83d\udd04 Processing batch 350: torch.Size([1, 192443]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  48%|\u2588\u2588\u2588\u2588\u258a     | 351/727 [07:37<06:56,  1.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 351: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  48%|\u2588\u2588\u2588\u2588\u258a     | 352/727 [07:38<05:54,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 148084])\n",
            "   \ud83d\udd04 Processing batch 352: torch.Size([1, 148084]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  49%|\u2588\u2588\u2588\u2588\u258a     | 353/727 [07:38<05:16,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 81175])\n",
            "   \ud83d\udd04 Processing batch 353: torch.Size([1, 81175]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  49%|\u2588\u2588\u2588\u2588\u258a     | 354/727 [07:39<05:51,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 152332])\n",
            "   \ud83d\udd04 Processing batch 354: torch.Size([1, 152332]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  49%|\u2588\u2588\u2588\u2588\u2589     | 355/727 [07:40<05:15,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 59653])\n",
            "   \ud83d\udd04 Processing batch 355: torch.Size([1, 59653]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  49%|\u2588\u2588\u2588\u2588\u2589     | 356/727 [07:40<04:20,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 191284])\n",
            "   \ud83d\udd04 Processing batch 356: torch.Size([1, 191284]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  49%|\u2588\u2588\u2588\u2588\u2589     | 357/727 [07:41<04:11,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 78021])\n",
            "   \ud83d\udd04 Processing batch 357: torch.Size([1, 78021]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  49%|\u2588\u2588\u2588\u2588\u2589     | 358/727 [07:42<03:59,  1.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 271189])\n",
            "   \ud83d\udd04 Processing batch 358: torch.Size([1, 271189]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  49%|\u2588\u2588\u2588\u2588\u2589     | 359/727 [07:44<06:59,  1.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 131187])\n",
            "   \ud83d\udd04 Processing batch 359: torch.Size([1, 131187]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  50%|\u2588\u2588\u2588\u2588\u2589     | 360/727 [07:45<07:07,  1.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 76363])\n",
            "   \ud83d\udd04 Processing batch 360: torch.Size([1, 76363]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  50%|\u2588\u2588\u2588\u2588\u2589     | 361/727 [07:46<05:38,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 125314])\n",
            "   \ud83d\udd04 Processing batch 361: torch.Size([1, 125314]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  50%|\u2588\u2588\u2588\u2588\u2589     | 362/727 [07:46<05:09,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 165570])\n",
            "   \ud83d\udd04 Processing batch 362: torch.Size([1, 165570]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  50%|\u2588\u2588\u2588\u2588\u2589     | 363/727 [07:47<05:16,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 39601])\n",
            "   \ud83d\udd04 Processing batch 363: torch.Size([1, 39601]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  50%|\u2588\u2588\u2588\u2588\u2588     | 364/727 [07:48<04:39,  1.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 219572])\n",
            "   \ud83d\udd04 Processing batch 364: torch.Size([1, 219572]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  50%|\u2588\u2588\u2588\u2588\u2588     | 365/727 [07:50<07:00,  1.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 161654])\n",
            "   \ud83d\udd04 Processing batch 365: torch.Size([1, 161654]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  50%|\u2588\u2588\u2588\u2588\u2588     | 366/727 [07:50<06:00,  1.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 58273])\n",
            "   \ud83d\udd04 Processing batch 366: torch.Size([1, 58273]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  50%|\u2588\u2588\u2588\u2588\u2588     | 367/727 [07:51<05:32,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 189614])\n",
            "   \ud83d\udd04 Processing batch 367: torch.Size([1, 189614]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  51%|\u2588\u2588\u2588\u2588\u2588     | 368/727 [07:54<08:16,  1.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 381024])\n",
            "   \ud83d\udd04 Processing batch 368: torch.Size([1, 381024]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  51%|\u2588\u2588\u2588\u2588\u2588     | 369/727 [08:05<26:58,  4.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 172650])\n",
            "   \ud83d\udd04 Processing batch 369: torch.Size([1, 172650]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  51%|\u2588\u2588\u2588\u2588\u2588     | 370/727 [08:07<21:23,  3.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 90789])\n",
            "   \ud83d\udd04 Processing batch 370: torch.Size([1, 90789]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  51%|\u2588\u2588\u2588\u2588\u2588     | 371/727 [08:08<16:20,  2.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 348172])\n",
            "   \ud83d\udd04 Processing batch 371: torch.Size([1, 348172]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  51%|\u2588\u2588\u2588\u2588\u2588     | 372/727 [08:11<16:33,  2.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 194067])\n",
            "   \ud83d\udd04 Processing batch 372: torch.Size([1, 194067]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 373/727 [08:11<12:40,  2.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 86559])\n",
            "   \ud83d\udd04 Processing batch 373: torch.Size([1, 86559]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 374/727 [08:12<10:09,  1.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 244837])\n",
            "   \ud83d\udd04 Processing batch 374: torch.Size([1, 244837]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 375/727 [08:12<07:43,  1.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 265658])\n",
            "   \ud83d\udd04 Processing batch 375: torch.Size([1, 265658]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 376/727 [08:13<07:01,  1.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 75249])\n",
            "   \ud83d\udd04 Processing batch 376: torch.Size([1, 75249]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 377/727 [08:14<06:12,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 90549])\n",
            "   \ud83d\udd04 Processing batch 377: torch.Size([1, 90549]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 378/727 [08:15<05:24,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 109026])\n",
            "   \ud83d\udd04 Processing batch 378: torch.Size([1, 109026]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 379/727 [08:15<05:00,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 34767])\n",
            "   \ud83d\udd04 Processing batch 379: torch.Size([1, 34767]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 380/727 [08:16<04:33,  1.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 295336])\n",
            "   \ud83d\udd04 Processing batch 380: torch.Size([1, 295336]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 381/727 [08:17<05:57,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 112261])\n",
            "   \ud83d\udd04 Processing batch 381: torch.Size([1, 112261]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 382/727 [08:18<05:14,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 229954])\n",
            "   \ud83d\udd04 Processing batch 382: torch.Size([1, 229954]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 383/727 [08:20<06:08,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 265126])\n",
            "   \ud83d\udd04 Processing batch 383: torch.Size([1, 265126]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 384/727 [08:20<05:21,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 124879])\n",
            "   \ud83d\udd04 Processing batch 384: torch.Size([1, 124879]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 385/727 [08:21<04:58,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 57767])\n",
            "   \ud83d\udd04 Processing batch 385: torch.Size([1, 57767]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 386/727 [08:21<04:05,  1.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 213576])\n",
            "   \ud83d\udd04 Processing batch 386: torch.Size([1, 213576]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 387/727 [08:22<03:54,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 152992])\n",
            "   \ud83d\udd04 Processing batch 387: torch.Size([1, 152992]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 388/727 [08:22<03:30,  1.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 151850])\n",
            "   \ud83d\udd04 Processing batch 388: torch.Size([1, 151850]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  54%|\u2588\u2588\u2588\u2588\u2588\u258e    | 389/727 [08:24<05:27,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 377055])\n",
            "   \ud83d\udd04 Processing batch 389: torch.Size([1, 377055]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  54%|\u2588\u2588\u2588\u2588\u2588\u258e    | 390/727 [08:27<09:00,  1.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 464025])\n",
            "   \ud83d\udd04 Processing batch 390: torch.Size([1, 464025]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 391/727 [08:35<19:38,  3.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 100471])\n",
            "   \ud83d\udd04 Processing batch 391: torch.Size([1, 100471]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 392/727 [08:36<15:10,  2.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 106085])\n",
            "   \ud83d\udd04 Processing batch 392: torch.Size([1, 106085]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 393/727 [08:46<26:45,  4.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 68794])\n",
            "   \ud83d\udd04 Processing batch 393: torch.Size([1, 68794]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 394/727 [08:46<19:29,  3.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 177414])\n",
            "   \ud83d\udd04 Processing batch 394: torch.Size([1, 177414]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 395/727 [08:47<14:38,  2.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 106804])\n",
            "   \ud83d\udd04 Processing batch 395: torch.Size([1, 106804]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 396/727 [08:47<10:48,  1.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 396: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 397/727 [08:52<16:09,  2.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 92030])\n",
            "   \ud83d\udd04 Processing batch 397: torch.Size([1, 92030]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 398/727 [08:53<12:18,  2.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 162977])\n",
            "   \ud83d\udd04 Processing batch 398: torch.Size([1, 162977]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 399/727 [08:54<10:07,  1.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 399: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 400/727 [08:57<11:31,  2.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 249297])\n",
            "   \ud83d\udd04 Processing batch 400: torch.Size([1, 249297]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 401/727 [08:59<11:54,  2.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 244012])\n",
            "   \ud83d\udd04 Processing batch 401: torch.Size([1, 244012]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 402/727 [09:01<11:15,  2.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 173407])\n",
            "   \ud83d\udd04 Processing batch 402: torch.Size([1, 173407]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 403/727 [09:02<08:53,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 160486])\n",
            "   \ud83d\udd04 Processing batch 403: torch.Size([1, 160486]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 404/727 [09:03<07:49,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 44859])\n",
            "   \ud83d\udd04 Processing batch 404: torch.Size([1, 44859]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 405/727 [09:03<06:26,  1.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 146435])\n",
            "   \ud83d\udd04 Processing batch 405: torch.Size([1, 146435]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 406/727 [09:05<06:43,  1.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 311176])\n",
            "   \ud83d\udd04 Processing batch 406: torch.Size([1, 311176]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 407/727 [09:06<06:28,  1.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 139327])\n",
            "   \ud83d\udd04 Processing batch 407: torch.Size([1, 139327]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 408/727 [09:07<06:32,  1.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 175606])\n",
            "   \ud83d\udd04 Processing batch 408: torch.Size([1, 175606]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  56%|\u2588\u2588\u2588\u2588\u2588\u258b    | 409/727 [09:08<05:34,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 99629])\n",
            "   \ud83d\udd04 Processing batch 409: torch.Size([1, 99629]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  56%|\u2588\u2588\u2588\u2588\u2588\u258b    | 410/727 [09:08<05:23,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 285682])\n",
            "   \ud83d\udd04 Processing batch 410: torch.Size([1, 285682]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 411/727 [09:18<19:04,  3.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 95372])\n",
            "   \ud83d\udd04 Processing batch 411: torch.Size([1, 95372]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 412/727 [09:20<15:42,  2.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 154041])\n",
            "   \ud83d\udd04 Processing batch 412: torch.Size([1, 154041]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 413/727 [09:22<13:48,  2.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 187826])\n",
            "   \ud83d\udd04 Processing batch 413: torch.Size([1, 187826]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 414/727 [09:22<10:37,  2.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 235177])\n",
            "   \ud83d\udd04 Processing batch 414: torch.Size([1, 235177]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 415/727 [09:24<09:58,  1.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 189036])\n",
            "   \ud83d\udd04 Processing batch 415: torch.Size([1, 189036]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 416/727 [09:24<07:56,  1.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 52018])\n",
            "   \ud83d\udd04 Processing batch 416: torch.Size([1, 52018]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 417/727 [09:25<06:13,  1.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 134535])\n",
            "   \ud83d\udd04 Processing batch 417: torch.Size([1, 134535]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 418/727 [09:26<06:26,  1.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 33625])\n",
            "   \ud83d\udd04 Processing batch 418: torch.Size([1, 33625]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 419/727 [09:27<05:22,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 77309])\n",
            "   \ud83d\udd04 Processing batch 419: torch.Size([1, 77309]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 420/727 [09:28<05:11,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 162824])\n",
            "   \ud83d\udd04 Processing batch 420: torch.Size([1, 162824]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 421/727 [09:29<05:37,  1.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 176660])\n",
            "   \ud83d\udd04 Processing batch 421: torch.Size([1, 176660]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 422/727 [09:29<04:29,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 83395])\n",
            "   \ud83d\udd04 Processing batch 422: torch.Size([1, 83395]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 423/727 [09:30<04:03,  1.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 164350])\n",
            "   \ud83d\udd04 Processing batch 423: torch.Size([1, 164350]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 424/727 [09:32<06:14,  1.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 63337])\n",
            "   \ud83d\udd04 Processing batch 424: torch.Size([1, 63337]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 425/727 [09:33<04:57,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 174388])\n",
            "   \ud83d\udd04 Processing batch 425: torch.Size([1, 174388]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  59%|\u2588\u2588\u2588\u2588\u2588\u258a    | 426/727 [09:34<05:17,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 162801])\n",
            "   \ud83d\udd04 Processing batch 426: torch.Size([1, 162801]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  59%|\u2588\u2588\u2588\u2588\u2588\u258a    | 427/727 [09:34<04:15,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 194570])\n",
            "   \ud83d\udd04 Processing batch 427: torch.Size([1, 194570]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 428/727 [09:36<05:33,  1.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 210451])\n",
            "   \ud83d\udd04 Processing batch 428: torch.Size([1, 210451]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 429/727 [09:37<04:48,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 272185])\n",
            "   \ud83d\udd04 Processing batch 429: torch.Size([1, 272185]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 430/727 [09:37<04:02,  1.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 194965])\n",
            "   \ud83d\udd04 Processing batch 430: torch.Size([1, 194965]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 431/727 [09:39<05:11,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 380272])\n",
            "   \ud83d\udd04 Processing batch 431: torch.Size([1, 380272]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 432/727 [09:42<08:12,  1.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 184114])\n",
            "   \ud83d\udd04 Processing batch 432: torch.Size([1, 184114]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 433/727 [09:43<07:38,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 352287])\n",
            "   \ud83d\udd04 Processing batch 433: torch.Size([1, 352287]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 434/727 [09:46<09:04,  1.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 163382])\n",
            "   \ud83d\udd04 Processing batch 434: torch.Size([1, 163382]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 435/727 [09:46<07:06,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 140104])\n",
            "   \ud83d\udd04 Processing batch 435: torch.Size([1, 140104]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 436/727 [09:47<06:03,  1.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 56387])\n",
            "   \ud83d\udd04 Processing batch 436: torch.Size([1, 56387]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 437/727 [09:47<04:44,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 275427])\n",
            "   \ud83d\udd04 Processing batch 437: torch.Size([1, 275427]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 438/727 [09:50<07:10,  1.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 133395])\n",
            "   \ud83d\udd04 Processing batch 438: torch.Size([1, 133395]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 439/727 [09:51<06:27,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 108433])\n",
            "   \ud83d\udd04 Processing batch 439: torch.Size([1, 108433]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 440/727 [09:52<05:53,  1.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 135543])\n",
            "   \ud83d\udd04 Processing batch 440: torch.Size([1, 135543]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 441/727 [09:53<04:59,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 51585])\n",
            "   \ud83d\udd04 Processing batch 441: torch.Size([1, 51585]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 442/727 [09:53<04:11,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 149581])\n",
            "   \ud83d\udd04 Processing batch 442: torch.Size([1, 149581]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 443/727 [09:55<05:32,  1.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 277565])\n",
            "   \ud83d\udd04 Processing batch 443: torch.Size([1, 277565]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 444/727 [09:56<04:44,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 72683])\n",
            "   \ud83d\udd04 Processing batch 444: torch.Size([1, 72683]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 445/727 [09:56<04:11,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 94572])\n",
            "   \ud83d\udd04 Processing batch 445: torch.Size([1, 94572]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 446/727 [09:57<04:13,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 123477])\n",
            "   \ud83d\udd04 Processing batch 446: torch.Size([1, 123477]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 447/727 [09:58<03:48,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 142884])\n",
            "   \ud83d\udd04 Processing batch 447: torch.Size([1, 142884]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 448/727 [10:07<16:10,  3.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 244008])\n",
            "   \ud83d\udd04 Processing batch 448: torch.Size([1, 244008]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 449/727 [10:09<13:13,  2.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 33835])\n",
            "   \ud83d\udd04 Processing batch 449: torch.Size([1, 33835]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 450/727 [10:09<09:53,  2.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 131449])\n",
            "   \ud83d\udd04 Processing batch 450: torch.Size([1, 131449]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 451/727 [10:10<07:46,  1.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 250047])\n",
            "   \ud83d\udd04 Processing batch 451: torch.Size([1, 250047]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 452/727 [10:12<08:36,  1.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 133210])\n",
            "   \ud83d\udd04 Processing batch 452: torch.Size([1, 133210]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 453/727 [10:13<07:21,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 140634])\n",
            "   \ud83d\udd04 Processing batch 453: torch.Size([1, 140634]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 454/727 [10:15<07:03,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 141384])\n",
            "   \ud83d\udd04 Processing batch 454: torch.Size([1, 141384]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 455/727 [10:15<06:06,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 273111])\n",
            "   \ud83d\udd04 Processing batch 455: torch.Size([1, 273111]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 456/727 [10:18<07:56,  1.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 101579])\n",
            "   \ud83d\udd04 Processing batch 456: torch.Size([1, 101579]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 457/727 [10:19<06:37,  1.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 191674])\n",
            "   \ud83d\udd04 Processing batch 457: torch.Size([1, 191674]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 458/727 [10:19<05:06,  1.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 305172])\n",
            "   \ud83d\udd04 Processing batch 458: torch.Size([1, 305172]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 459/727 [10:22<07:12,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 141901])\n",
            "   \ud83d\udd04 Processing batch 459: torch.Size([1, 141901]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 460/727 [10:24<06:58,  1.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 52988])\n",
            "   \ud83d\udd04 Processing batch 460: torch.Size([1, 52988]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 461/727 [10:24<05:41,  1.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 69581])\n",
            "   \ud83d\udd04 Processing batch 461: torch.Size([1, 69581]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 462/727 [10:25<05:27,  1.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 253024])\n",
            "   \ud83d\udd04 Processing batch 462: torch.Size([1, 253024]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 463/727 [10:27<05:53,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 49997])\n",
            "   \ud83d\udd04 Processing batch 463: torch.Size([1, 49997]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 464/727 [10:27<04:54,  1.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 265769])\n",
            "   \ud83d\udd04 Processing batch 464: torch.Size([1, 265769]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 465/727 [10:29<05:14,  1.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 146808])\n",
            "   \ud83d\udd04 Processing batch 465: torch.Size([1, 146808]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 466/727 [10:30<05:05,  1.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 104862])\n",
            "   \ud83d\udd04 Processing batch 466: torch.Size([1, 104862]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 467/727 [10:31<04:37,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 77254])\n",
            "   \ud83d\udd04 Processing batch 467: torch.Size([1, 77254]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 468/727 [10:31<03:47,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 325968])\n",
            "   \ud83d\udd04 Processing batch 468: torch.Size([1, 325968]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 469/727 [10:34<06:07,  1.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 75535])\n",
            "   \ud83d\udd04 Processing batch 469: torch.Size([1, 75535]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 470/727 [10:35<05:30,  1.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 138335])\n",
            "   \ud83d\udd04 Processing batch 470: torch.Size([1, 138335]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 471/727 [10:36<05:46,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 268038])\n",
            "   \ud83d\udd04 Processing batch 471: torch.Size([1, 268038]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 472/727 [10:39<07:19,  1.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 169091])\n",
            "   \ud83d\udd04 Processing batch 472: torch.Size([1, 169091]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 473/727 [10:41<07:13,  1.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 166968])\n",
            "   \ud83d\udd04 Processing batch 473: torch.Size([1, 166968]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 474/727 [10:42<06:50,  1.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 356140])\n",
            "   \ud83d\udd04 Processing batch 474: torch.Size([1, 356140]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 475/727 [10:45<07:54,  1.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 82878])\n",
            "   \ud83d\udd04 Processing batch 475: torch.Size([1, 82878]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 476/727 [10:46<06:42,  1.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 298964])\n",
            "   \ud83d\udd04 Processing batch 476: torch.Size([1, 298964]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 477/727 [10:47<06:51,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 145072])\n",
            "   \ud83d\udd04 Processing batch 477: torch.Size([1, 145072]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 478/727 [10:49<06:51,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 248547])\n",
            "   \ud83d\udd04 Processing batch 478: torch.Size([1, 248547]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 479/727 [10:50<05:35,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 164931])\n",
            "   \ud83d\udd04 Processing batch 479: torch.Size([1, 164931]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 480/727 [10:51<05:33,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 276540])\n",
            "   \ud83d\udd04 Processing batch 480: torch.Size([1, 276540]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 481/727 [10:53<05:51,  1.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 179937])\n",
            "   \ud83d\udd04 Processing batch 481: torch.Size([1, 179937]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 482/727 [10:54<06:07,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 269769])\n",
            "   \ud83d\udd04 Processing batch 482: torch.Size([1, 269769]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 483/727 [10:57<08:12,  2.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 423360])\n",
            "   \ud83d\udd04 Processing batch 483: torch.Size([1, 423360]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 484/727 [10:58<06:17,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 250488])\n",
            "   \ud83d\udd04 Processing batch 484: torch.Size([1, 250488]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 485/727 [11:00<06:30,  1.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 400372])\n",
            "   \ud83d\udd04 Processing batch 485: torch.Size([1, 400372]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 486/727 [11:00<05:20,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 180707])\n",
            "   \ud83d\udd04 Processing batch 486: torch.Size([1, 180707]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 487/727 [11:02<05:45,  1.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 84162])\n",
            "   \ud83d\udd04 Processing batch 487: torch.Size([1, 84162]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 488/727 [11:03<05:13,  1.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 195057])\n",
            "   \ud83d\udd04 Processing batch 488: torch.Size([1, 195057]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 489/727 [11:05<06:06,  1.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 137922])\n",
            "   \ud83d\udd04 Processing batch 489: torch.Size([1, 137922]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 490/727 [11:07<06:05,  1.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 90494])\n",
            "   \ud83d\udd04 Processing batch 490: torch.Size([1, 90494]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 491/727 [11:07<05:10,  1.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 383109])\n",
            "   \ud83d\udd04 Processing batch 491: torch.Size([1, 383109]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 492/727 [11:10<06:29,  1.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 149440])\n",
            "   \ud83d\udd04 Processing batch 492: torch.Size([1, 149440]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 493/727 [11:29<27:16,  7.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 172651])\n",
            "   \ud83d\udd04 Processing batch 493: torch.Size([1, 172651]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 494/727 [11:30<19:45,  5.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 223983])\n",
            "   \ud83d\udd04 Processing batch 494: torch.Size([1, 223983]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 495/727 [11:31<14:29,  3.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 349509])\n",
            "   \ud83d\udd04 Processing batch 495: torch.Size([1, 349509]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 496/727 [11:33<12:35,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 82120])\n",
            "   \ud83d\udd04 Processing batch 496: torch.Size([1, 82120]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 497/727 [11:33<09:29,  2.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 97411])\n",
            "   \ud83d\udd04 Processing batch 497: torch.Size([1, 97411]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 498/727 [11:34<07:39,  2.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 258541])\n",
            "   \ud83d\udd04 Processing batch 498: torch.Size([1, 258541]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 499/727 [11:36<07:34,  1.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 211627])\n",
            "   \ud83d\udd04 Processing batch 499: torch.Size([1, 211627]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 500/727 [11:38<07:46,  2.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 187874])\n",
            "   \ud83d\udd04 Processing batch 500: torch.Size([1, 187874]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 501/727 [11:40<06:44,  1.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 141301])\n",
            "   \ud83d\udd04 Processing batch 501: torch.Size([1, 141301]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 502/727 [11:41<06:04,  1.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 158348])\n",
            "   \ud83d\udd04 Processing batch 502: torch.Size([1, 158348]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 503/727 [11:41<04:55,  1.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 145468])\n",
            "   \ud83d\udd04 Processing batch 503: torch.Size([1, 145468]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 504/727 [11:43<04:37,  1.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 302129])\n",
            "   \ud83d\udd04 Processing batch 504: torch.Size([1, 302129]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 505/727 [11:44<05:15,  1.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 173953])\n",
            "   \ud83d\udd04 Processing batch 505: torch.Size([1, 173953]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 506/727 [11:45<04:20,  1.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 126970])\n",
            "   \ud83d\udd04 Processing batch 506: torch.Size([1, 126970]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 507/727 [11:46<03:42,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 372276])\n",
            "   \ud83d\udd04 Processing batch 507: torch.Size([1, 372276]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 508/727 [11:48<04:53,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 257670])\n",
            "   \ud83d\udd04 Processing batch 508: torch.Size([1, 257670]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 509/727 [11:49<04:55,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 139162])\n",
            "   \ud83d\udd04 Processing batch 509: torch.Size([1, 139162]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 510/727 [11:50<04:28,  1.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 188645])\n",
            "   \ud83d\udd04 Processing batch 510: torch.Size([1, 188645]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 511/727 [11:52<04:56,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 256743])\n",
            "   \ud83d\udd04 Processing batch 511: torch.Size([1, 256743]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 512/727 [11:53<05:09,  1.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 254501])\n",
            "   \ud83d\udd04 Processing batch 512: torch.Size([1, 254501]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 513/727 [11:55<05:16,  1.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 389705])\n",
            "   \ud83d\udd04 Processing batch 513: torch.Size([1, 389705]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 514/727 [11:59<07:31,  2.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 174181])\n",
            "   \ud83d\udd04 Processing batch 514: torch.Size([1, 174181]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 515/727 [12:00<06:55,  1.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 302878])\n",
            "   \ud83d\udd04 Processing batch 515: torch.Size([1, 302878]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 516/727 [12:02<07:09,  2.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 167441])\n",
            "   \ud83d\udd04 Processing batch 516: torch.Size([1, 167441]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 517/727 [12:04<06:24,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 135523])\n",
            "   \ud83d\udd04 Processing batch 517: torch.Size([1, 135523]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 518/727 [12:05<05:48,  1.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 138840])\n",
            "   \ud83d\udd04 Processing batch 518: torch.Size([1, 138840]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 519/727 [12:06<04:41,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 148087])\n",
            "   \ud83d\udd04 Processing batch 519: torch.Size([1, 148087]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 520/727 [12:07<04:23,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 153854])\n",
            "   \ud83d\udd04 Processing batch 520: torch.Size([1, 153854]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 521/727 [12:08<04:07,  1.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 159245])\n",
            "   \ud83d\udd04 Processing batch 521: torch.Size([1, 159245]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 522/727 [12:08<03:30,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 78528])\n",
            "   \ud83d\udd04 Processing batch 522: torch.Size([1, 78528]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 523/727 [12:09<03:20,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 202211])\n",
            "   \ud83d\udd04 Processing batch 523: torch.Size([1, 202211]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 524/727 [12:11<04:01,  1.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 58758])\n",
            "   \ud83d\udd04 Processing batch 524: torch.Size([1, 58758]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 525/727 [12:12<03:25,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 168788])\n",
            "   \ud83d\udd04 Processing batch 525: torch.Size([1, 168788]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 526/727 [12:13<03:44,  1.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 290984])\n",
            "   \ud83d\udd04 Processing batch 526: torch.Size([1, 290984]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 527/727 [12:15<04:25,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 145247])\n",
            "   \ud83d\udd04 Processing batch 527: torch.Size([1, 145247]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 528/727 [12:16<04:19,  1.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 170259])\n",
            "   \ud83d\udd04 Processing batch 528: torch.Size([1, 170259]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 529/727 [12:18<04:38,  1.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 176951])\n",
            "   \ud83d\udd04 Processing batch 529: torch.Size([1, 176951]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 530/727 [12:19<04:52,  1.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 84543])\n",
            "   \ud83d\udd04 Processing batch 530: torch.Size([1, 84543]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 531/727 [12:20<03:57,  1.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 283751])\n",
            "   \ud83d\udd04 Processing batch 531: torch.Size([1, 283751]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 532/727 [12:22<04:52,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 220076])\n",
            "   \ud83d\udd04 Processing batch 532: torch.Size([1, 220076]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 533/727 [12:24<05:04,  1.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 191559])\n",
            "   \ud83d\udd04 Processing batch 533: torch.Size([1, 191559]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 534/727 [12:25<04:52,  1.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 182705])\n",
            "   \ud83d\udd04 Processing batch 534: torch.Size([1, 182705]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 535/727 [12:26<03:57,  1.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 535: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 536/727 [12:31<07:56,  2.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 265974])\n",
            "   \ud83d\udd04 Processing batch 536: torch.Size([1, 265974]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 537/727 [12:34<08:00,  2.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 399262])\n",
            "   \ud83d\udd04 Processing batch 537: torch.Size([1, 399262]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 538/727 [12:43<14:43,  4.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 114895])\n",
            "   \ud83d\udd04 Processing batch 538: torch.Size([1, 114895]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 539/727 [12:45<12:03,  3.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 62185])\n",
            "   \ud83d\udd04 Processing batch 539: torch.Size([1, 62185]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 540/727 [12:46<08:57,  2.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 316382])\n",
            "   \ud83d\udd04 Processing batch 540: torch.Size([1, 316382]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 541/727 [12:54<14:04,  4.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 250297])\n",
            "   \ud83d\udd04 Processing batch 541: torch.Size([1, 250297]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 542/727 [12:55<10:22,  3.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 420855])\n",
            "   \ud83d\udd04 Processing batch 542: torch.Size([1, 420855]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 543/727 [12:58<09:48,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 110491])\n",
            "   \ud83d\udd04 Processing batch 543: torch.Size([1, 110491]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 544/727 [12:59<07:54,  2.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 242414])\n",
            "   \ud83d\udd04 Processing batch 544: torch.Size([1, 242414]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 545/727 [13:00<06:06,  2.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 75467])\n",
            "   \ud83d\udd04 Processing batch 545: torch.Size([1, 75467]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 546/727 [13:00<04:48,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 134499])\n",
            "   \ud83d\udd04 Processing batch 546: torch.Size([1, 134499]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 547/727 [13:02<04:41,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 164448])\n",
            "   \ud83d\udd04 Processing batch 547: torch.Size([1, 164448]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 548/727 [13:03<04:35,  1.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 175332])\n",
            "   \ud83d\udd04 Processing batch 548: torch.Size([1, 175332]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 549/727 [13:04<03:56,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 109237])\n",
            "   \ud83d\udd04 Processing batch 549: torch.Size([1, 109237]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 550/727 [13:05<03:42,  1.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 141834])\n",
            "   \ud83d\udd04 Processing batch 550: torch.Size([1, 141834]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 551/727 [13:06<03:35,  1.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 37369])\n",
            "   \ud83d\udd04 Processing batch 551: torch.Size([1, 37369]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 552/727 [13:07<03:02,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 197576])\n",
            "   \ud83d\udd04 Processing batch 552: torch.Size([1, 197576]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 553/727 [13:08<03:06,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 165948])\n",
            "   \ud83d\udd04 Processing batch 553: torch.Size([1, 165948]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 554/727 [13:09<03:06,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 139162])\n",
            "   \ud83d\udd04 Processing batch 554: torch.Size([1, 139162]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 555/727 [13:10<03:01,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 192744])\n",
            "   \ud83d\udd04 Processing batch 555: torch.Size([1, 192744]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 556/727 [13:12<03:20,  1.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 173395])\n",
            "   \ud83d\udd04 Processing batch 556: torch.Size([1, 173395]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 557/727 [13:13<03:28,  1.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 199914])\n",
            "   \ud83d\udd04 Processing batch 557: torch.Size([1, 199914]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 558/727 [13:15<03:50,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 558: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 559/727 [13:18<05:18,  1.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 152001])\n",
            "   \ud83d\udd04 Processing batch 559: torch.Size([1, 152001]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 560/727 [13:19<04:41,  1.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 285906])\n",
            "   \ud83d\udd04 Processing batch 560: torch.Size([1, 285906]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 561/727 [13:22<05:24,  1.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 275049])\n",
            "   \ud83d\udd04 Processing batch 561: torch.Size([1, 275049]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 562/727 [13:24<05:27,  1.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 187328])\n",
            "   \ud83d\udd04 Processing batch 562: torch.Size([1, 187328]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 563/727 [13:25<05:12,  1.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 142221])\n",
            "   \ud83d\udd04 Processing batch 563: torch.Size([1, 142221]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 564/727 [13:26<04:24,  1.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 177855])\n",
            "   \ud83d\udd04 Processing batch 564: torch.Size([1, 177855]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 565/727 [13:28<04:03,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 140634])\n",
            "   \ud83d\udd04 Processing batch 565: torch.Size([1, 140634]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 566/727 [13:28<03:19,  1.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 291721])\n",
            "   \ud83d\udd04 Processing batch 566: torch.Size([1, 291721]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 567/727 [13:30<04:01,  1.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 208372])\n",
            "   \ud83d\udd04 Processing batch 567: torch.Size([1, 208372]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 568/727 [13:32<04:16,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 230687])\n",
            "   \ud83d\udd04 Processing batch 568: torch.Size([1, 230687]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 569/727 [13:34<04:20,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 197818])\n",
            "   \ud83d\udd04 Processing batch 569: torch.Size([1, 197818]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 570/727 [13:35<04:04,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 357604])\n",
            "   \ud83d\udd04 Processing batch 570: torch.Size([1, 357604]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 571/727 [13:38<05:07,  1.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 88763])\n",
            "   \ud83d\udd04 Processing batch 571: torch.Size([1, 88763]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 572/727 [13:39<04:23,  1.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 160704])\n",
            "   \ud83d\udd04 Processing batch 572: torch.Size([1, 160704]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 573/727 [13:40<03:32,  1.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 212647])\n",
            "   \ud83d\udd04 Processing batch 573: torch.Size([1, 212647]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 574/727 [13:40<02:56,  1.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 308082])\n",
            "   \ud83d\udd04 Processing batch 574: torch.Size([1, 308082]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 575/727 [13:44<04:28,  1.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 138841])\n",
            "   \ud83d\udd04 Processing batch 575: torch.Size([1, 138841]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 576/727 [13:45<04:00,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 200492])\n",
            "   \ud83d\udd04 Processing batch 576: torch.Size([1, 200492]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 577/727 [13:55<10:41,  4.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 80066])\n",
            "   \ud83d\udd04 Processing batch 577: torch.Size([1, 80066]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 578/727 [13:56<07:57,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 173915])\n",
            "   \ud83d\udd04 Processing batch 578: torch.Size([1, 173915]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 579/727 [13:58<07:02,  2.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 579: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 580/727 [14:01<07:07,  2.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 139259])\n",
            "   \ud83d\udd04 Processing batch 580: torch.Size([1, 139259]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 581/727 [14:02<05:43,  2.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 310128])\n",
            "   \ud83d\udd04 Processing batch 581: torch.Size([1, 310128]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 582/727 [14:04<05:27,  2.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 209139])\n",
            "   \ud83d\udd04 Processing batch 582: torch.Size([1, 209139]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 583/727 [14:05<04:15,  1.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 244668])\n",
            "   \ud83d\udd04 Processing batch 583: torch.Size([1, 244668]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 584/727 [14:07<04:26,  1.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 51814])\n",
            "   \ud83d\udd04 Processing batch 584: torch.Size([1, 51814]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 585/727 [14:07<03:26,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 125424])\n",
            "   \ud83d\udd04 Processing batch 585: torch.Size([1, 125424]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 586/727 [14:08<02:49,  1.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 142670])\n",
            "   \ud83d\udd04 Processing batch 586: torch.Size([1, 142670]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 587/727 [14:09<02:50,  1.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 216756])\n",
            "   \ud83d\udd04 Processing batch 587: torch.Size([1, 216756]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 588/727 [14:13<04:22,  1.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 187017])\n",
            "   \ud83d\udd04 Processing batch 588: torch.Size([1, 187017]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 589/727 [14:16<05:02,  2.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 242032])\n",
            "   \ud83d\udd04 Processing batch 589: torch.Size([1, 242032]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 590/727 [14:18<04:51,  2.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 161488])\n",
            "   \ud83d\udd04 Processing batch 590: torch.Size([1, 161488]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 591/727 [14:19<04:25,  1.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 119284])\n",
            "   \ud83d\udd04 Processing batch 591: torch.Size([1, 119284]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 592/727 [14:20<03:49,  1.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 203434])\n",
            "   \ud83d\udd04 Processing batch 592: torch.Size([1, 203434]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 593/727 [14:22<03:47,  1.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 141975])\n",
            "   \ud83d\udd04 Processing batch 593: torch.Size([1, 141975]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 594/727 [14:23<03:33,  1.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 296925])\n",
            "   \ud83d\udd04 Processing batch 594: torch.Size([1, 296925]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 595/727 [14:25<03:48,  1.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 165198])\n",
            "   \ud83d\udd04 Processing batch 595: torch.Size([1, 165198]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 596/727 [14:27<03:43,  1.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 83870])\n",
            "   \ud83d\udd04 Processing batch 596: torch.Size([1, 83870]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 597/727 [14:28<03:18,  1.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 158724])\n",
            "   \ud83d\udd04 Processing batch 597: torch.Size([1, 158724]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 598/727 [14:29<02:43,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 53485])\n",
            "   \ud83d\udd04 Processing batch 598: torch.Size([1, 53485]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 599/727 [14:29<02:07,  1.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 213176])\n",
            "   \ud83d\udd04 Processing batch 599: torch.Size([1, 213176]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 600/727 [14:31<02:45,  1.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 364780])\n",
            "   \ud83d\udd04 Processing batch 600: torch.Size([1, 364780]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 601/727 [14:34<03:26,  1.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 252278])\n",
            "   \ud83d\udd04 Processing batch 601: torch.Size([1, 252278]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 602/727 [14:35<03:29,  1.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 177776])\n",
            "   \ud83d\udd04 Processing batch 602: torch.Size([1, 177776]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 603/727 [14:36<02:59,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 102948])\n",
            "   \ud83d\udd04 Processing batch 603: torch.Size([1, 102948]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 604/727 [14:37<02:38,  1.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 72725])\n",
            "   \ud83d\udd04 Processing batch 604: torch.Size([1, 72725]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 605/727 [14:38<02:03,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 33316])\n",
            "   \ud83d\udd04 Processing batch 605: torch.Size([1, 33316]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 606/727 [14:38<01:47,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 122542])\n",
            "   \ud83d\udd04 Processing batch 606: torch.Size([1, 122542]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 607/727 [14:39<01:46,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 109643])\n",
            "   \ud83d\udd04 Processing batch 607: torch.Size([1, 109643]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 608/727 [14:40<01:30,  1.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 136336])\n",
            "   \ud83d\udd04 Processing batch 608: torch.Size([1, 136336]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 609/727 [14:41<01:39,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 89107])\n",
            "   \ud83d\udd04 Processing batch 609: torch.Size([1, 89107]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 610/727 [14:50<06:52,  3.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 79545])\n",
            "   \ud83d\udd04 Processing batch 610: torch.Size([1, 79545]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 611/727 [14:51<05:17,  2.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 137035])\n",
            "   \ud83d\udd04 Processing batch 611: torch.Size([1, 137035]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 612/727 [14:52<04:14,  2.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 94832])\n",
            "   \ud83d\udd04 Processing batch 612: torch.Size([1, 94832]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 613/727 [14:53<03:19,  1.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 88741])\n",
            "   \ud83d\udd04 Processing batch 613: torch.Size([1, 88741]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 614/727 [14:54<03:01,  1.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 99170])\n",
            "   \ud83d\udd04 Processing batch 614: torch.Size([1, 99170]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 615/727 [15:04<07:33,  4.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 211918])\n",
            "   \ud83d\udd04 Processing batch 615: torch.Size([1, 211918]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 616/727 [15:05<05:35,  3.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 207622])\n",
            "   \ud83d\udd04 Processing batch 616: torch.Size([1, 207622]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 617/727 [15:06<04:43,  2.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 142160])\n",
            "   \ud83d\udd04 Processing batch 617: torch.Size([1, 142160]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 618/727 [15:08<04:17,  2.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 433855])\n",
            "   \ud83d\udd04 Processing batch 618: torch.Size([1, 433855]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 619/727 [15:12<04:59,  2.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 240508])\n",
            "   \ud83d\udd04 Processing batch 619: torch.Size([1, 240508]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 620/727 [15:14<04:31,  2.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 78388])\n",
            "   \ud83d\udd04 Processing batch 620: torch.Size([1, 78388]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 621/727 [15:14<03:29,  1.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 152213])\n",
            "   \ud83d\udd04 Processing batch 621: torch.Size([1, 152213]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 622/727 [15:15<02:44,  1.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 261867])\n",
            "   \ud83d\udd04 Processing batch 622: torch.Size([1, 261867]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 623/727 [15:17<02:51,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 327909])\n",
            "   \ud83d\udd04 Processing batch 623: torch.Size([1, 327909]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 624/727 [15:17<02:18,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 56216])\n",
            "   \ud83d\udd04 Processing batch 624: torch.Size([1, 56216]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 625/727 [15:18<01:55,  1.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 71948])\n",
            "   \ud83d\udd04 Processing batch 625: torch.Size([1, 71948]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 626/727 [15:18<01:30,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 273226])\n",
            "   \ud83d\udd04 Processing batch 626: torch.Size([1, 273226]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 627/727 [15:21<02:17,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 128816])\n",
            "   \ud83d\udd04 Processing batch 627: torch.Size([1, 128816]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 628/727 [15:22<01:54,  1.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 71027])\n",
            "   \ud83d\udd04 Processing batch 628: torch.Size([1, 71027]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 629/727 [15:22<01:37,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 75794])\n",
            "   \ud83d\udd04 Processing batch 629: torch.Size([1, 75794]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 630/727 [15:23<01:25,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 630: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 631/727 [15:25<01:59,  1.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 146666])\n",
            "   \ud83d\udd04 Processing batch 631: torch.Size([1, 146666]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 632/727 [15:26<01:40,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 116788])\n",
            "   \ud83d\udd04 Processing batch 632: torch.Size([1, 116788]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 633/727 [15:27<01:50,  1.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 111745])\n",
            "   \ud83d\udd04 Processing batch 633: torch.Size([1, 111745]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 634/727 [15:28<01:34,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 177906])\n",
            "   \ud83d\udd04 Processing batch 634: torch.Size([1, 177906]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 635/727 [15:29<01:53,  1.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 75285])\n",
            "   \ud83d\udd04 Processing batch 635: torch.Size([1, 75285]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 636/727 [15:30<01:43,  1.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 160970])\n",
            "   \ud83d\udd04 Processing batch 636: torch.Size([1, 160970]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 637/727 [15:31<01:22,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 138797])\n",
            "   \ud83d\udd04 Processing batch 637: torch.Size([1, 138797]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 638/727 [15:32<01:19,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 132780])\n",
            "   \ud83d\udd04 Processing batch 638: torch.Size([1, 132780]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 639/727 [15:33<01:38,  1.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 48326])\n",
            "   \ud83d\udd04 Processing batch 639: torch.Size([1, 48326]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 640/727 [15:34<01:28,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 132157])\n",
            "   \ud83d\udd04 Processing batch 640: torch.Size([1, 132157]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 641/727 [15:35<01:25,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 386977])\n",
            "   \ud83d\udd04 Processing batch 641: torch.Size([1, 386977]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 642/727 [15:38<02:16,  1.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 194176])\n",
            "   \ud83d\udd04 Processing batch 642: torch.Size([1, 194176]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 643/727 [15:39<02:08,  1.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 90243])\n",
            "   \ud83d\udd04 Processing batch 643: torch.Size([1, 90243]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 644/727 [15:40<01:53,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 88860])\n",
            "   \ud83d\udd04 Processing batch 644: torch.Size([1, 88860]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 645/727 [15:41<01:36,  1.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 109998])\n",
            "   \ud83d\udd04 Processing batch 645: torch.Size([1, 109998]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 646/727 [15:42<01:29,  1.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 62937])\n",
            "   \ud83d\udd04 Processing batch 646: torch.Size([1, 62937]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 647/727 [15:43<01:16,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 137998])\n",
            "   \ud83d\udd04 Processing batch 647: torch.Size([1, 137998]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 648/727 [15:44<01:29,  1.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 261107])\n",
            "   \ud83d\udd04 Processing batch 648: torch.Size([1, 261107]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 649/727 [15:46<01:46,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 83282])\n",
            "   \ud83d\udd04 Processing batch 649: torch.Size([1, 83282]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 650/727 [15:47<01:30,  1.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 136008])\n",
            "   \ud83d\udd04 Processing batch 650: torch.Size([1, 136008]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 651/727 [15:47<01:17,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 198696])\n",
            "   \ud83d\udd04 Processing batch 651: torch.Size([1, 198696]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 652/727 [15:48<01:07,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 235890])\n",
            "   \ud83d\udd04 Processing batch 652: torch.Size([1, 235890]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 653/727 [15:50<01:31,  1.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 174138])\n",
            "   \ud83d\udd04 Processing batch 653: torch.Size([1, 174138]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 654/727 [15:51<01:26,  1.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 190093])\n",
            "   \ud83d\udd04 Processing batch 654: torch.Size([1, 190093]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 655/727 [15:52<01:22,  1.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 54646])\n",
            "   \ud83d\udd04 Processing batch 655: torch.Size([1, 54646]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 656/727 [15:53<01:15,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 158857])\n",
            "   \ud83d\udd04 Processing batch 656: torch.Size([1, 158857]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 657/727 [15:54<01:22,  1.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 153972])\n",
            "   \ud83d\udd04 Processing batch 657: torch.Size([1, 153972]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 658/727 [15:56<01:27,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 40568])\n",
            "   \ud83d\udd04 Processing batch 658: torch.Size([1, 40568]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 659/727 [15:57<01:12,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 429360])\n",
            "   \ud83d\udd04 Processing batch 659: torch.Size([1, 429360]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 660/727 [15:59<01:37,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 71320])\n",
            "   \ud83d\udd04 Processing batch 660: torch.Size([1, 71320]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 661/727 [16:00<01:28,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 114456])\n",
            "   \ud83d\udd04 Processing batch 661: torch.Size([1, 114456]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 662/727 [16:01<01:22,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 84393])\n",
            "   \ud83d\udd04 Processing batch 662: torch.Size([1, 84393]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 663/727 [16:02<01:22,  1.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 127940])\n",
            "   \ud83d\udd04 Processing batch 663: torch.Size([1, 127940]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 664/727 [16:03<01:15,  1.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 138133])\n",
            "   \ud83d\udd04 Processing batch 664: torch.Size([1, 138133]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 665/727 [16:04<01:11,  1.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 162305])\n",
            "   \ud83d\udd04 Processing batch 665: torch.Size([1, 162305]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 666/727 [16:06<01:23,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 124987])\n",
            "   \ud83d\udd04 Processing batch 666: torch.Size([1, 124987]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 667/727 [16:16<03:52,  3.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 40684])\n",
            "   \ud83d\udd04 Processing batch 667: torch.Size([1, 40684]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 668/727 [16:16<02:46,  2.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 74668])\n",
            "   \ud83d\udd04 Processing batch 668: torch.Size([1, 74668]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 669/727 [16:17<02:08,  2.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 11246])\n",
            "   \ud83d\udd04 Processing batch 669: torch.Size([1, 11246]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 670/727 [16:18<01:39,  1.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 41345])\n",
            "   \ud83d\udd04 Processing batch 670: torch.Size([1, 41345]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 671/727 [16:18<01:18,  1.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 8590])\n",
            "   \ud83d\udd04 Processing batch 671: torch.Size([1, 8590]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 672/727 [16:19<01:03,  1.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 52762])\n",
            "   \ud83d\udd04 Processing batch 672: torch.Size([1, 52762]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 673/727 [16:19<00:49,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 41238])\n",
            "   \ud83d\udd04 Processing batch 673: torch.Size([1, 41238]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 674/727 [16:20<00:40,  1.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 33075])\n",
            "   \ud83d\udd04 Processing batch 674: torch.Size([1, 33075]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 675/727 [16:20<00:36,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 184763])\n",
            "   \ud83d\udd04 Processing batch 675: torch.Size([1, 184763]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 676/727 [16:21<00:34,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 43389])\n",
            "   \ud83d\udd04 Processing batch 676: torch.Size([1, 43389]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 677/727 [16:22<00:32,  1.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 150632])\n",
            "   \ud83d\udd04 Processing batch 677: torch.Size([1, 150632]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 678/727 [16:22<00:31,  1.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 33960])\n",
            "   \ud83d\udd04 Processing batch 678: torch.Size([1, 33960]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 679/727 [16:23<00:27,  1.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 43558])\n",
            "   \ud83d\udd04 Processing batch 679: torch.Size([1, 43558]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 680/727 [16:23<00:27,  1.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 39842])\n",
            "   \ud83d\udd04 Processing batch 680: torch.Size([1, 39842]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 681/727 [16:24<00:23,  1.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 28322])\n",
            "   \ud83d\udd04 Processing batch 681: torch.Size([1, 28322]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 682/727 [16:24<00:21,  2.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 42537])\n",
            "   \ud83d\udd04 Processing batch 682: torch.Size([1, 42537]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 683/727 [16:24<00:19,  2.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 30539])\n",
            "   \ud83d\udd04 Processing batch 683: torch.Size([1, 30539]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 684/727 [16:25<00:18,  2.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 684: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 685/727 [16:29<01:10,  1.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 41074])\n",
            "   \ud83d\udd04 Processing batch 685: torch.Size([1, 41074]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 686/727 [16:30<00:53,  1.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 686: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 687/727 [16:40<02:42,  4.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 39695])\n",
            "   \ud83d\udd04 Processing batch 687: torch.Size([1, 39695]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 688/727 [16:41<01:57,  3.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 43358])\n",
            "   \ud83d\udd04 Processing batch 688: torch.Size([1, 43358]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 689/727 [16:42<01:28,  2.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 43118])\n",
            "   \ud83d\udd04 Processing batch 689: torch.Size([1, 43118]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 690/727 [16:42<01:07,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 40751])\n",
            "   \ud83d\udd04 Processing batch 690: torch.Size([1, 40751]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 691/727 [16:43<00:49,  1.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 43101])\n",
            "   \ud83d\udd04 Processing batch 691: torch.Size([1, 43101]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 692/727 [16:43<00:40,  1.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 33732])\n",
            "   \ud83d\udd04 Processing batch 692: torch.Size([1, 33732]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 693/727 [16:44<00:33,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 57164])\n",
            "   \ud83d\udd04 Processing batch 693: torch.Size([1, 57164]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 694/727 [16:44<00:26,  1.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 31382])\n",
            "   \ud83d\udd04 Processing batch 694: torch.Size([1, 31382]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 695/727 [16:45<00:22,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 37314])\n",
            "   \ud83d\udd04 Processing batch 695: torch.Size([1, 37314]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 696/727 [16:45<00:20,  1.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 38682])\n",
            "   \ud83d\udd04 Processing batch 696: torch.Size([1, 38682]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 697/727 [16:46<00:18,  1.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 58409])\n",
            "   \ud83d\udd04 Processing batch 697: torch.Size([1, 58409]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 698/727 [16:46<00:17,  1.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 27549])\n",
            "   \ud83d\udd04 Processing batch 698: torch.Size([1, 27549]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 699/727 [16:56<01:33,  3.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 72612])\n",
            "   \ud83d\udd04 Processing batch 699: torch.Size([1, 72612]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 700/727 [16:57<01:08,  2.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 43632])\n",
            "   \ud83d\udd04 Processing batch 700: torch.Size([1, 43632]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 701/727 [16:57<00:48,  1.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 40071])\n",
            "   \ud83d\udd04 Processing batch 701: torch.Size([1, 40071]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 702/727 [16:57<00:36,  1.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 31305])\n",
            "   \ud83d\udd04 Processing batch 702: torch.Size([1, 31305]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 703/727 [16:58<00:28,  1.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 35149])\n",
            "   \ud83d\udd04 Processing batch 703: torch.Size([1, 35149]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 704/727 [16:58<00:21,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 38510])\n",
            "   \ud83d\udd04 Processing batch 704: torch.Size([1, 38510]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 705/727 [16:59<00:16,  1.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 39923])\n",
            "   \ud83d\udd04 Processing batch 705: torch.Size([1, 39923]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 706/727 [16:59<00:13,  1.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 42152])\n",
            "   \ud83d\udd04 Processing batch 706: torch.Size([1, 42152]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 707/727 [16:59<00:11,  1.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 42196])\n",
            "   \ud83d\udd04 Processing batch 707: torch.Size([1, 42196]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 708/727 [17:00<00:09,  1.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 42602])\n",
            "   \ud83d\udd04 Processing batch 708: torch.Size([1, 42602]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 709/727 [17:00<00:08,  2.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 37140])\n",
            "   \ud83d\udd04 Processing batch 709: torch.Size([1, 37140]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 710/727 [17:01<00:08,  1.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 36600])\n",
            "   \ud83d\udd04 Processing batch 710: torch.Size([1, 36600]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 711/727 [17:01<00:07,  2.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 26158])\n",
            "   \ud83d\udd04 Processing batch 711: torch.Size([1, 26158]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 712/727 [17:02<00:07,  2.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 31407])\n",
            "   \ud83d\udd04 Processing batch 712: torch.Size([1, 31407]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 713/727 [17:02<00:07,  1.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 21645])\n",
            "   \ud83d\udd04 Processing batch 713: torch.Size([1, 21645]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 714/727 [17:03<00:06,  2.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 32541])\n",
            "   \ud83d\udd04 Processing batch 714: torch.Size([1, 32541]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 715/727 [17:03<00:05,  2.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 41025])\n",
            "   \ud83d\udd04 Processing batch 715: torch.Size([1, 41025]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 716/727 [17:03<00:04,  2.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 30708])\n",
            "   \ud83d\udd04 Processing batch 716: torch.Size([1, 30708]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 717/727 [17:04<00:04,  2.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 41189])\n",
            "   \ud83d\udd04 Processing batch 717: torch.Size([1, 41189]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 718/727 [17:04<00:04,  2.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 30638])\n",
            "   \ud83d\udd04 Processing batch 718: torch.Size([1, 30638]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 719/727 [17:05<00:03,  2.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 36863])\n",
            "   \ud83d\udd04 Processing batch 719: torch.Size([1, 36863]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 720/727 [17:05<00:03,  2.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 42174])\n",
            "   \ud83d\udd04 Processing batch 720: torch.Size([1, 42174]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 721/727 [17:06<00:02,  2.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 39618])\n",
            "   \ud83d\udd04 Processing batch 721: torch.Size([1, 39618]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 722/727 [17:06<00:02,  1.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 42630])\n",
            "   \ud83d\udd04 Processing batch 722: torch.Size([1, 42630]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 723/727 [17:07<00:02,  1.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 39595])\n",
            "   \ud83d\udd04 Processing batch 723: torch.Size([1, 39595]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 724/727 [17:08<00:01,  1.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 33057])\n",
            "   \ud83d\udd04 Processing batch 724: torch.Size([1, 33057]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 725/727 [17:08<00:01,  1.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 31781])\n",
            "   \ud83d\udd04 Processing batch 725: torch.Size([1, 31781]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAlgeria: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 726/727 [17:09<00:00,  1.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 35173])\n",
            "   \ud83d\udd04 Processing batch 726: torch.Size([1, 35173]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Algeria: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 727/727 [17:09<00:00,  1.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   \u2705 Completed: 727 transcriptions\n",
            "   \ud83d\udcca Successful batches: 727, Failed batches: 0\n",
            "   \u2705 Added 727 results from Algeria\n",
            "   \ud83d\udcca Total results so far: 727\n",
            "\n",
            "[2/8] Starting Egypt...\n",
            "\n",
            "\ud83d\udcc2 Processing Egypt...\n",
            "   \ud83d\udd04 Loading dataset for Egypt...\n",
            "   \ud83d\udcca 1,600 samples loaded\n",
            "   \ud83d\udd0d Sample keys: ['audio', 'ID', 'duration']\n",
            "   \ud83c\udfb5 Audio keys: ['path', 'array', 'sampling_rate']\n",
            "   \ud83d\udcd0 Audio shape: (41932,)\n",
            "   \ud83d\udce6 Created data loader with 1600 batches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   0%|          | 0/1600 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 41932])\n",
            "   \ud83d\udd04 Processing batch 0: torch.Size([1, 41932]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   0%|          | 1/1600 [00:00<14:37,  1.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 38470])\n",
            "   \ud83d\udd04 Processing batch 1: torch.Size([1, 38470]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   0%|          | 2/1600 [00:01<15:36,  1.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 316971])\n",
            "   \ud83d\udd04 Processing batch 2: torch.Size([1, 316971]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   0%|          | 3/1600 [00:04<44:02,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 57382])\n",
            "   \ud83d\udd04 Processing batch 3: torch.Size([1, 57382]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   0%|          | 4/1600 [00:04<32:13,  1.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 55421])\n",
            "   \ud83d\udd04 Processing batch 4: torch.Size([1, 55421]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   0%|          | 5/1600 [00:05<26:31,  1.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 255303])\n",
            "   \ud83d\udd04 Processing batch 5: torch.Size([1, 255303]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   0%|          | 6/1600 [00:05<20:49,  1.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 49791])\n",
            "   \ud83d\udd04 Processing batch 6: torch.Size([1, 49791]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   0%|          | 7/1600 [00:05<17:09,  1.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 75879])\n",
            "   \ud83d\udd04 Processing batch 7: torch.Size([1, 75879]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   0%|          | 8/1600 [00:06<18:03,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 221499])\n",
            "   \ud83d\udd04 Processing batch 8: torch.Size([1, 221499]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   1%|          | 9/1600 [00:08<28:03,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 109909])\n",
            "   \ud83d\udd04 Processing batch 9: torch.Size([1, 109909]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   1%|          | 10/1600 [00:09<26:31,  1.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 252273])\n",
            "   \ud83d\udd04 Processing batch 10: torch.Size([1, 252273]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   1%|          | 11/1600 [00:10<23:21,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 126827])\n",
            "   \ud83d\udd04 Processing batch 11: torch.Size([1, 126827]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   1%|          | 12/1600 [00:10<22:30,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 181473])\n",
            "   \ud83d\udd04 Processing batch 12: torch.Size([1, 181473]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   1%|          | 13/1600 [00:12<31:12,  1.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 281906])\n",
            "   \ud83d\udd04 Processing batch 13: torch.Size([1, 281906]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   1%|          | 14/1600 [00:15<39:22,  1.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 33210])\n",
            "   \ud83d\udd04 Processing batch 14: torch.Size([1, 33210]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   1%|          | 15/1600 [00:15<30:17,  1.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 64931])\n",
            "   \ud83d\udd04 Processing batch 15: torch.Size([1, 64931]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   1%|          | 16/1600 [00:15<26:06,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 208074])\n",
            "   \ud83d\udd04 Processing batch 16: torch.Size([1, 208074]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   1%|          | 17/1600 [00:17<29:40,  1.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 145936])\n",
            "   \ud83d\udd04 Processing batch 17: torch.Size([1, 145936]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   1%|          | 18/1600 [00:18<29:00,  1.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 39777])\n",
            "   \ud83d\udd04 Processing batch 18: torch.Size([1, 39777]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   1%|          | 19/1600 [00:19<24:52,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 50863])\n",
            "   \ud83d\udd04 Processing batch 19: torch.Size([1, 50863]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   1%|\u258f         | 20/1600 [00:19<21:54,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 34525])\n",
            "   \ud83d\udd04 Processing batch 20: torch.Size([1, 34525]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   1%|\u258f         | 21/1600 [00:20<18:25,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 467474])\n",
            "   \ud83d\udd04 Processing batch 21: torch.Size([1, 467474]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   1%|\u258f         | 22/1600 [00:23<42:54,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 108098])\n",
            "   \ud83d\udd04 Processing batch 22: torch.Size([1, 108098]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   1%|\u258f         | 23/1600 [00:24<38:58,  1.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 126310])\n",
            "   \ud83d\udd04 Processing batch 23: torch.Size([1, 126310]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   2%|\u258f         | 24/1600 [00:26<36:09,  1.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 80223])\n",
            "   \ud83d\udd04 Processing batch 24: torch.Size([1, 80223]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   2%|\u258f         | 25/1600 [00:26<28:54,  1.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 40608])\n",
            "   \ud83d\udd04 Processing batch 25: torch.Size([1, 40608]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   2%|\u258f         | 26/1600 [00:26<23:24,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 119003])\n",
            "   \ud83d\udd04 Processing batch 26: torch.Size([1, 119003]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   2%|\u258f         | 27/1600 [00:27<21:09,  1.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 107243])\n",
            "   \ud83d\udd04 Processing batch 27: torch.Size([1, 107243]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   2%|\u258f         | 28/1600 [00:28<25:05,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 382938])\n",
            "   \ud83d\udd04 Processing batch 28: torch.Size([1, 382938]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   2%|\u258f         | 29/1600 [00:38<1:33:39,  3.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 141672])\n",
            "   \ud83d\udd04 Processing batch 29: torch.Size([1, 141672]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   2%|\u258f         | 30/1600 [00:39<1:16:03,  2.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 77941])\n",
            "   \ud83d\udd04 Processing batch 30: torch.Size([1, 77941]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   2%|\u258f         | 31/1600 [00:40<58:25,  2.23s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 68412])\n",
            "   \ud83d\udd04 Processing batch 31: torch.Size([1, 68412]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   2%|\u258f         | 32/1600 [00:41<48:49,  1.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 143428])\n",
            "   \ud83d\udd04 Processing batch 32: torch.Size([1, 143428]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   2%|\u258f         | 33/1600 [00:42<44:49,  1.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 29061])\n",
            "   \ud83d\udd04 Processing batch 33: torch.Size([1, 29061]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   2%|\u258f         | 34/1600 [00:43<35:31,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 71976])\n",
            "   \ud83d\udd04 Processing batch 34: torch.Size([1, 71976]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   2%|\u258f         | 35/1600 [00:44<30:40,  1.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 201446])\n",
            "   \ud83d\udd04 Processing batch 35: torch.Size([1, 201446]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   2%|\u258f         | 36/1600 [00:45<35:02,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 67027])\n",
            "   \ud83d\udd04 Processing batch 36: torch.Size([1, 67027]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   2%|\u258f         | 37/1600 [00:46<29:25,  1.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 150009])\n",
            "   \ud83d\udd04 Processing batch 37: torch.Size([1, 150009]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   2%|\u258f         | 38/1600 [00:47<25:25,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 87499])\n",
            "   \ud83d\udd04 Processing batch 38: torch.Size([1, 87499]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   2%|\u258f         | 39/1600 [00:48<30:51,  1.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 35677])\n",
            "   \ud83d\udd04 Processing batch 39: torch.Size([1, 35677]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   2%|\u258e         | 40/1600 [00:49<25:44,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 34578])\n",
            "   \ud83d\udd04 Processing batch 40: torch.Size([1, 34578]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   3%|\u258e         | 41/1600 [00:50<22:44,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 191265])\n",
            "   \ud83d\udd04 Processing batch 41: torch.Size([1, 191265]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   3%|\u258e         | 42/1600 [00:52<32:04,  1.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 115666])\n",
            "   \ud83d\udd04 Processing batch 42: torch.Size([1, 115666]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   3%|\u258e         | 43/1600 [00:53<30:17,  1.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 89842])\n",
            "   \ud83d\udd04 Processing batch 43: torch.Size([1, 89842]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   3%|\u258e         | 44/1600 [00:54<28:13,  1.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 387427])\n",
            "   \ud83d\udd04 Processing batch 44: torch.Size([1, 387427]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   3%|\u258e         | 45/1600 [00:55<33:37,  1.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 212433])\n",
            "   \ud83d\udd04 Processing batch 45: torch.Size([1, 212433]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   3%|\u258e         | 46/1600 [00:57<34:45,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 34423])\n",
            "   \ud83d\udd04 Processing batch 46: torch.Size([1, 34423]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   3%|\u258e         | 47/1600 [00:57<27:03,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 55498])\n",
            "   \ud83d\udd04 Processing batch 47: torch.Size([1, 55498]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   3%|\u258e         | 48/1600 [00:58<24:18,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 213252])\n",
            "   \ud83d\udd04 Processing batch 48: torch.Size([1, 213252]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   3%|\u258e         | 49/1600 [01:00<33:12,  1.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 70121])\n",
            "   \ud83d\udd04 Processing batch 49: torch.Size([1, 70121]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   3%|\u258e         | 50/1600 [01:00<28:00,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 247119])\n",
            "   \ud83d\udd04 Processing batch 50: torch.Size([1, 247119]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   3%|\u258e         | 51/1600 [01:02<33:27,  1.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 124018])\n",
            "   \ud83d\udd04 Processing batch 51: torch.Size([1, 124018]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   3%|\u258e         | 52/1600 [01:04<34:30,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 60661])\n",
            "   \ud83d\udd04 Processing batch 52: torch.Size([1, 60661]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   3%|\u258e         | 53/1600 [01:04<29:52,  1.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 153666])\n",
            "   \ud83d\udd04 Processing batch 53: torch.Size([1, 153666]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   3%|\u258e         | 54/1600 [01:06<34:38,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 213856])\n",
            "   \ud83d\udd04 Processing batch 54: torch.Size([1, 213856]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   3%|\u258e         | 55/1600 [01:08<40:11,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 72428])\n",
            "   \ud83d\udd04 Processing batch 55: torch.Size([1, 72428]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   4%|\u258e         | 56/1600 [01:09<32:12,  1.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 102067])\n",
            "   \ud83d\udd04 Processing batch 56: torch.Size([1, 102067]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   4%|\u258e         | 57/1600 [01:10<29:15,  1.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 147811])\n",
            "   \ud83d\udd04 Processing batch 57: torch.Size([1, 147811]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   4%|\u258e         | 58/1600 [01:10<24:54,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 75390])\n",
            "   \ud83d\udd04 Processing batch 58: torch.Size([1, 75390]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   4%|\u258e         | 59/1600 [01:11<20:11,  1.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 298414])\n",
            "   \ud83d\udd04 Processing batch 59: torch.Size([1, 298414]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   4%|\u258d         | 60/1600 [01:12<27:54,  1.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 331176])\n",
            "   \ud83d\udd04 Processing batch 60: torch.Size([1, 331176]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   4%|\u258d         | 61/1600 [01:15<37:35,  1.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 294693])\n",
            "   \ud83d\udd04 Processing batch 61: torch.Size([1, 294693]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   4%|\u258d         | 62/1600 [01:24<1:40:29,  3.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 31170])\n",
            "   \ud83d\udd04 Processing batch 62: torch.Size([1, 31170]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   4%|\u258d         | 63/1600 [01:25<1:14:55,  2.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 40544])\n",
            "   \ud83d\udd04 Processing batch 63: torch.Size([1, 40544]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   4%|\u258d         | 64/1600 [01:26<57:13,  2.24s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 69971])\n",
            "   \ud83d\udd04 Processing batch 64: torch.Size([1, 69971]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   4%|\u258d         | 65/1600 [01:26<42:45,  1.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 105268])\n",
            "   \ud83d\udd04 Processing batch 65: torch.Size([1, 105268]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   4%|\u258d         | 66/1600 [01:27<37:38,  1.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 46521])\n",
            "   \ud83d\udd04 Processing batch 66: torch.Size([1, 46521]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   4%|\u258d         | 67/1600 [01:28<31:06,  1.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 452333])\n",
            "   \ud83d\udd04 Processing batch 67: torch.Size([1, 452333]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   4%|\u258d         | 68/1600 [01:31<44:31,  1.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 170631])\n",
            "   \ud83d\udd04 Processing batch 68: torch.Size([1, 170631]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   4%|\u258d         | 69/1600 [01:33<46:56,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 405184])\n",
            "   \ud83d\udd04 Processing batch 69: torch.Size([1, 405184]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   4%|\u258d         | 70/1600 [01:38<1:10:41,  2.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 140327])\n",
            "   \ud83d\udd04 Processing batch 70: torch.Size([1, 140327]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   4%|\u258d         | 71/1600 [01:38<54:15,  2.13s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 185808])\n",
            "   \ud83d\udd04 Processing batch 71: torch.Size([1, 185808]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   4%|\u258d         | 72/1600 [01:41<56:28,  2.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 276284])\n",
            "   \ud83d\udd04 Processing batch 72: torch.Size([1, 276284]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   5%|\u258d         | 73/1600 [01:43<54:32,  2.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 260731])\n",
            "   \ud83d\udd04 Processing batch 73: torch.Size([1, 260731]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   5%|\u258d         | 74/1600 [01:52<1:52:05,  4.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 86110])\n",
            "   \ud83d\udd04 Processing batch 74: torch.Size([1, 86110]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   5%|\u258d         | 75/1600 [01:54<1:27:43,  3.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 119224])\n",
            "   \ud83d\udd04 Processing batch 75: torch.Size([1, 119224]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   5%|\u258d         | 76/1600 [01:55<1:11:46,  2.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 92621])\n",
            "   \ud83d\udd04 Processing batch 76: torch.Size([1, 92621]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   5%|\u258d         | 77/1600 [01:56<57:45,  2.28s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 76899])\n",
            "   \ud83d\udd04 Processing batch 77: torch.Size([1, 76899]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   5%|\u258d         | 78/1600 [01:57<46:18,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 24109])\n",
            "   \ud83d\udd04 Processing batch 78: torch.Size([1, 24109]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   5%|\u258d         | 79/1600 [01:57<35:08,  1.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 173173])\n",
            "   \ud83d\udd04 Processing batch 79: torch.Size([1, 173173]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   5%|\u258c         | 80/1600 [01:58<29:19,  1.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 155659])\n",
            "   \ud83d\udd04 Processing batch 80: torch.Size([1, 155659]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   5%|\u258c         | 81/1600 [02:07<1:33:51,  3.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 142440])\n",
            "   \ud83d\udd04 Processing batch 81: torch.Size([1, 142440]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   5%|\u258c         | 82/1600 [02:09<1:17:31,  3.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 58051])\n",
            "   \ud83d\udd04 Processing batch 82: torch.Size([1, 58051]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   5%|\u258c         | 83/1600 [02:10<58:55,  2.33s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 35939])\n",
            "   \ud83d\udd04 Processing batch 83: torch.Size([1, 35939]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   5%|\u258c         | 84/1600 [02:10<43:54,  1.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 131674])\n",
            "   \ud83d\udd04 Processing batch 84: torch.Size([1, 131674]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   5%|\u258c         | 85/1600 [02:12<43:31,  1.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 149718])\n",
            "   \ud83d\udd04 Processing batch 85: torch.Size([1, 149718]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   5%|\u258c         | 86/1600 [02:13<39:08,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 38940])\n",
            "   \ud83d\udd04 Processing batch 86: torch.Size([1, 38940]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   5%|\u258c         | 87/1600 [02:13<31:57,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 298794])\n",
            "   \ud83d\udd04 Processing batch 87: torch.Size([1, 298794]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   6%|\u258c         | 88/1600 [02:17<46:30,  1.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 155179])\n",
            "   \ud83d\udd04 Processing batch 88: torch.Size([1, 155179]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   6%|\u258c         | 89/1600 [02:18<43:37,  1.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 195954])\n",
            "   \ud83d\udd04 Processing batch 89: torch.Size([1, 195954]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   6%|\u258c         | 90/1600 [02:19<40:38,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 200539])\n",
            "   \ud83d\udd04 Processing batch 90: torch.Size([1, 200539]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   6%|\u258c         | 91/1600 [02:20<33:11,  1.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 270756])\n",
            "   \ud83d\udd04 Processing batch 91: torch.Size([1, 270756]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   6%|\u258c         | 92/1600 [02:22<42:04,  1.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 113591])\n",
            "   \ud83d\udd04 Processing batch 92: torch.Size([1, 113591]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   6%|\u258c         | 93/1600 [02:24<38:17,  1.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 242869])\n",
            "   \ud83d\udd04 Processing batch 93: torch.Size([1, 242869]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   6%|\u258c         | 94/1600 [02:24<31:33,  1.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 30139])\n",
            "   \ud83d\udd04 Processing batch 94: torch.Size([1, 30139]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   6%|\u258c         | 95/1600 [02:25<25:43,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 102785])\n",
            "   \ud83d\udd04 Processing batch 95: torch.Size([1, 102785]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   6%|\u258c         | 96/1600 [02:27<31:36,  1.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 56406])\n",
            "   \ud83d\udd04 Processing batch 96: torch.Size([1, 56406]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   6%|\u258c         | 97/1600 [02:27<26:47,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 65642])\n",
            "   \ud83d\udd04 Processing batch 97: torch.Size([1, 65642]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   6%|\u258c         | 98/1600 [02:28<23:21,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 39579])\n",
            "   \ud83d\udd04 Processing batch 98: torch.Size([1, 39579]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   6%|\u258c         | 99/1600 [02:28<21:33,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 142185])\n",
            "   \ud83d\udd04 Processing batch 99: torch.Size([1, 142185]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   6%|\u258b         | 100/1600 [02:30<27:29,  1.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 100: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   6%|\u258b         | 101/1600 [02:34<46:59,  1.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 180156])\n",
            "   \ud83d\udd04 Processing batch 101: torch.Size([1, 180156]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   6%|\u258b         | 102/1600 [02:34<37:34,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 139737])\n",
            "   \ud83d\udd04 Processing batch 102: torch.Size([1, 139737]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   6%|\u258b         | 103/1600 [02:35<30:04,  1.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 215270])\n",
            "   \ud83d\udd04 Processing batch 103: torch.Size([1, 215270]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   6%|\u258b         | 104/1600 [02:37<37:54,  1.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 101009])\n",
            "   \ud83d\udd04 Processing batch 104: torch.Size([1, 101009]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   7%|\u258b         | 105/1600 [02:38<33:18,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 155643])\n",
            "   \ud83d\udd04 Processing batch 105: torch.Size([1, 155643]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   7%|\u258b         | 106/1600 [02:39<28:01,  1.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 178021])\n",
            "   \ud83d\udd04 Processing batch 106: torch.Size([1, 178021]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   7%|\u258b         | 107/1600 [02:40<27:27,  1.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 121062])\n",
            "   \ud83d\udd04 Processing batch 107: torch.Size([1, 121062]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   7%|\u258b         | 108/1600 [02:41<28:38,  1.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 238926])\n",
            "   \ud83d\udd04 Processing batch 108: torch.Size([1, 238926]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   7%|\u258b         | 109/1600 [02:43<35:01,  1.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 169951])\n",
            "   \ud83d\udd04 Processing batch 109: torch.Size([1, 169951]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   7%|\u258b         | 110/1600 [02:44<29:05,  1.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 83832])\n",
            "   \ud83d\udd04 Processing batch 110: torch.Size([1, 83832]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   7%|\u258b         | 111/1600 [02:44<25:50,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 198007])\n",
            "   \ud83d\udd04 Processing batch 111: torch.Size([1, 198007]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   7%|\u258b         | 112/1600 [02:47<34:40,  1.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 136116])\n",
            "   \ud83d\udd04 Processing batch 112: torch.Size([1, 136116]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   7%|\u258b         | 113/1600 [02:48<35:55,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 60872])\n",
            "   \ud83d\udd04 Processing batch 113: torch.Size([1, 60872]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   7%|\u258b         | 114/1600 [02:49<30:01,  1.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 30477])\n",
            "   \ud83d\udd04 Processing batch 114: torch.Size([1, 30477]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   7%|\u258b         | 115/1600 [02:50<25:38,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 413956])\n",
            "   \ud83d\udd04 Processing batch 115: torch.Size([1, 413956]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   7%|\u258b         | 116/1600 [02:52<39:01,  1.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 172184])\n",
            "   \ud83d\udd04 Processing batch 116: torch.Size([1, 172184]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   7%|\u258b         | 117/1600 [02:54<40:14,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 40922])\n",
            "   \ud83d\udd04 Processing batch 117: torch.Size([1, 40922]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   7%|\u258b         | 118/1600 [02:55<32:09,  1.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 262009])\n",
            "   \ud83d\udd04 Processing batch 118: torch.Size([1, 262009]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   7%|\u258b         | 119/1600 [02:55<25:54,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 352387])\n",
            "   \ud83d\udd04 Processing batch 119: torch.Size([1, 352387]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   8%|\u258a         | 120/1600 [02:58<41:16,  1.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 120: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   8%|\u258a         | 121/1600 [03:01<52:27,  2.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 401990])\n",
            "   \ud83d\udd04 Processing batch 121: torch.Size([1, 401990]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   8%|\u258a         | 122/1600 [03:02<41:27,  1.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 111074])\n",
            "   \ud83d\udd04 Processing batch 122: torch.Size([1, 111074]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   8%|\u258a         | 123/1600 [03:03<33:37,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 39430])\n",
            "   \ud83d\udd04 Processing batch 123: torch.Size([1, 39430]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   8%|\u258a         | 124/1600 [03:03<28:48,  1.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 187807])\n",
            "   \ud83d\udd04 Processing batch 124: torch.Size([1, 187807]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   8%|\u258a         | 125/1600 [03:05<32:25,  1.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 190811])\n",
            "   \ud83d\udd04 Processing batch 125: torch.Size([1, 190811]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   8%|\u258a         | 126/1600 [03:07<40:03,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 218996])\n",
            "   \ud83d\udd04 Processing batch 126: torch.Size([1, 218996]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   8%|\u258a         | 127/1600 [03:08<33:18,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 127: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   8%|\u258a         | 128/1600 [03:09<30:35,  1.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 195928])\n",
            "   \ud83d\udd04 Processing batch 128: torch.Size([1, 195928]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   8%|\u258a         | 129/1600 [03:11<38:19,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 47132])\n",
            "   \ud83d\udd04 Processing batch 129: torch.Size([1, 47132]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   8%|\u258a         | 130/1600 [03:12<31:23,  1.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 85333])\n",
            "   \ud83d\udd04 Processing batch 130: torch.Size([1, 85333]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   8%|\u258a         | 131/1600 [03:13<26:28,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 405321])\n",
            "   \ud83d\udd04 Processing batch 131: torch.Size([1, 405321]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   8%|\u258a         | 132/1600 [03:16<41:00,  1.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 123229])\n",
            "   \ud83d\udd04 Processing batch 132: torch.Size([1, 123229]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   8%|\u258a         | 133/1600 [03:26<1:40:08,  4.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 103070])\n",
            "   \ud83d\udd04 Processing batch 133: torch.Size([1, 103070]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   8%|\u258a         | 134/1600 [03:26<1:16:07,  3.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 96862])\n",
            "   \ud83d\udd04 Processing batch 134: torch.Size([1, 96862]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   8%|\u258a         | 135/1600 [03:27<1:01:08,  2.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 183360])\n",
            "   \ud83d\udd04 Processing batch 135: torch.Size([1, 183360]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   8%|\u258a         | 136/1600 [03:29<57:18,  2.35s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 136: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   9%|\u258a         | 137/1600 [03:34<1:12:33,  2.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 252677])\n",
            "   \ud83d\udd04 Processing batch 137: torch.Size([1, 252677]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   9%|\u258a         | 138/1600 [03:36<1:06:13,  2.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 349575])\n",
            "   \ud83d\udd04 Processing batch 138: torch.Size([1, 349575]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   9%|\u258a         | 139/1600 [03:39<1:07:35,  2.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 100447])\n",
            "   \ud83d\udd04 Processing batch 139: torch.Size([1, 100447]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   9%|\u2589         | 140/1600 [03:40<53:58,  2.22s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 131947])\n",
            "   \ud83d\udd04 Processing batch 140: torch.Size([1, 131947]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   9%|\u2589         | 141/1600 [03:40<42:54,  1.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 125447])\n",
            "   \ud83d\udd04 Processing batch 141: torch.Size([1, 125447]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   9%|\u2589         | 142/1600 [03:42<43:00,  1.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 64367])\n",
            "   \ud83d\udd04 Processing batch 142: torch.Size([1, 64367]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   9%|\u2589         | 143/1600 [03:43<34:56,  1.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 50862])\n",
            "   \ud83d\udd04 Processing batch 143: torch.Size([1, 50862]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   9%|\u2589         | 144/1600 [03:43<26:59,  1.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 215897])\n",
            "   \ud83d\udd04 Processing batch 144: torch.Size([1, 215897]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   9%|\u2589         | 145/1600 [03:44<23:29,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 215995])\n",
            "   \ud83d\udd04 Processing batch 145: torch.Size([1, 215995]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   9%|\u2589         | 146/1600 [03:46<29:59,  1.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 146: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   9%|\u2589         | 147/1600 [03:50<49:50,  2.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 52525])\n",
            "   \ud83d\udd04 Processing batch 147: torch.Size([1, 52525]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   9%|\u2589         | 148/1600 [03:50<39:23,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 82387])\n",
            "   \ud83d\udd04 Processing batch 148: torch.Size([1, 82387]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   9%|\u2589         | 149/1600 [03:51<34:47,  1.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 174386])\n",
            "   \ud83d\udd04 Processing batch 149: torch.Size([1, 174386]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   9%|\u2589         | 150/1600 [03:53<32:30,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 150: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:   9%|\u2589         | 151/1600 [03:55<43:33,  1.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 32290])\n",
            "   \ud83d\udd04 Processing batch 151: torch.Size([1, 32290]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  10%|\u2589         | 152/1600 [03:56<33:04,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 90370])\n",
            "   \ud83d\udd04 Processing batch 152: torch.Size([1, 90370]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  10%|\u2589         | 153/1600 [03:56<28:11,  1.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 131682])\n",
            "   \ud83d\udd04 Processing batch 153: torch.Size([1, 131682]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  10%|\u2589         | 154/1600 [03:58<28:52,  1.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 75037])\n",
            "   \ud83d\udd04 Processing batch 154: torch.Size([1, 75037]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  10%|\u2589         | 155/1600 [03:58<24:40,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 372678])\n",
            "   \ud83d\udd04 Processing batch 155: torch.Size([1, 372678]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  10%|\u2589         | 156/1600 [04:01<34:07,  1.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 230573])\n",
            "   \ud83d\udd04 Processing batch 156: torch.Size([1, 230573]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  10%|\u2589         | 157/1600 [04:02<35:33,  1.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 138932])\n",
            "   \ud83d\udd04 Processing batch 157: torch.Size([1, 138932]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  10%|\u2589         | 158/1600 [04:05<44:40,  1.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 127871])\n",
            "   \ud83d\udd04 Processing batch 158: torch.Size([1, 127871]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  10%|\u2589         | 159/1600 [04:06<35:41,  1.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 99570])\n",
            "   \ud83d\udd04 Processing batch 159: torch.Size([1, 99570]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  10%|\u2588         | 160/1600 [04:07<34:09,  1.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 35077])\n",
            "   \ud83d\udd04 Processing batch 160: torch.Size([1, 35077]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  10%|\u2588         | 161/1600 [04:08<28:22,  1.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 142615])\n",
            "   \ud83d\udd04 Processing batch 161: torch.Size([1, 142615]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  10%|\u2588         | 162/1600 [04:09<29:15,  1.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 127236])\n",
            "   \ud83d\udd04 Processing batch 162: torch.Size([1, 127236]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  10%|\u2588         | 163/1600 [04:10<28:53,  1.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 270515])\n",
            "   \ud83d\udd04 Processing batch 163: torch.Size([1, 270515]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  10%|\u2588         | 164/1600 [04:12<37:17,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 120031])\n",
            "   \ud83d\udd04 Processing batch 164: torch.Size([1, 120031]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  10%|\u2588         | 165/1600 [04:22<1:35:37,  4.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 382088])\n",
            "   \ud83d\udd04 Processing batch 165: torch.Size([1, 382088]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  10%|\u2588         | 166/1600 [04:23<1:11:29,  2.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 164743])\n",
            "   \ud83d\udd04 Processing batch 166: torch.Size([1, 164743]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  10%|\u2588         | 167/1600 [04:24<59:38,  2.50s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 45816])\n",
            "   \ud83d\udd04 Processing batch 167: torch.Size([1, 45816]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  10%|\u2588         | 168/1600 [04:25<46:04,  1.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 177072])\n",
            "   \ud83d\udd04 Processing batch 168: torch.Size([1, 177072]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  11%|\u2588         | 169/1600 [04:35<1:43:43,  4.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 195706])\n",
            "   \ud83d\udd04 Processing batch 169: torch.Size([1, 195706]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  11%|\u2588         | 170/1600 [04:35<1:17:00,  3.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 267731])\n",
            "   \ud83d\udd04 Processing batch 170: torch.Size([1, 267731]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  11%|\u2588         | 171/1600 [04:38<1:12:35,  3.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 149103])\n",
            "   \ud83d\udd04 Processing batch 171: torch.Size([1, 149103]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  11%|\u2588         | 172/1600 [04:39<58:01,  2.44s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 39038])\n",
            "   \ud83d\udd04 Processing batch 172: torch.Size([1, 39038]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  11%|\u2588         | 173/1600 [04:40<44:39,  1.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 448598])\n",
            "   \ud83d\udd04 Processing batch 173: torch.Size([1, 448598]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  11%|\u2588         | 174/1600 [04:43<56:09,  2.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 295287])\n",
            "   \ud83d\udd04 Processing batch 174: torch.Size([1, 295287]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  11%|\u2588         | 175/1600 [04:50<1:31:36,  3.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 99656])\n",
            "   \ud83d\udd04 Processing batch 175: torch.Size([1, 99656]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  11%|\u2588         | 176/1600 [04:51<1:09:58,  2.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 67508])\n",
            "   \ud83d\udd04 Processing batch 176: torch.Size([1, 67508]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  11%|\u2588         | 177/1600 [04:52<53:21,  2.25s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 43616])\n",
            "   \ud83d\udd04 Processing batch 177: torch.Size([1, 43616]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  11%|\u2588         | 178/1600 [04:52<39:54,  1.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 127264])\n",
            "   \ud83d\udd04 Processing batch 178: torch.Size([1, 127264]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  11%|\u2588         | 179/1600 [04:53<30:48,  1.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 373850])\n",
            "   \ud83d\udd04 Processing batch 179: torch.Size([1, 373850]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  11%|\u2588\u258f        | 180/1600 [04:55<41:04,  1.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 194295])\n",
            "   \ud83d\udd04 Processing batch 180: torch.Size([1, 194295]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  11%|\u2588\u258f        | 181/1600 [04:57<40:48,  1.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 296406])\n",
            "   \ud83d\udd04 Processing batch 181: torch.Size([1, 296406]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  11%|\u2588\u258f        | 182/1600 [05:01<56:07,  2.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 182: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  11%|\u2588\u258f        | 183/1600 [05:06<1:13:52,  3.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 97322])\n",
            "   \ud83d\udd04 Processing batch 183: torch.Size([1, 97322]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  12%|\u2588\u258f        | 184/1600 [05:07<58:56,  2.50s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 228056])\n",
            "   \ud83d\udd04 Processing batch 184: torch.Size([1, 228056]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  12%|\u2588\u258f        | 185/1600 [05:07<45:43,  1.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 145795])\n",
            "   \ud83d\udd04 Processing batch 185: torch.Size([1, 145795]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  12%|\u2588\u258f        | 186/1600 [05:09<40:37,  1.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 57675])\n",
            "   \ud83d\udd04 Processing batch 186: torch.Size([1, 57675]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  12%|\u2588\u258f        | 187/1600 [05:10<34:22,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 111219])\n",
            "   \ud83d\udd04 Processing batch 187: torch.Size([1, 111219]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  12%|\u2588\u258f        | 188/1600 [05:11<31:27,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 179143])\n",
            "   \ud83d\udd04 Processing batch 188: torch.Size([1, 179143]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  12%|\u2588\u258f        | 189/1600 [05:12<33:06,  1.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 104037])\n",
            "   \ud83d\udd04 Processing batch 189: torch.Size([1, 104037]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  12%|\u2588\u258f        | 190/1600 [05:13<30:02,  1.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 377840])\n",
            "   \ud83d\udd04 Processing batch 190: torch.Size([1, 377840]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  12%|\u2588\u258f        | 191/1600 [05:17<48:16,  2.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 254365])\n",
            "   \ud83d\udd04 Processing batch 191: torch.Size([1, 254365]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  12%|\u2588\u258f        | 192/1600 [05:18<38:09,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 237733])\n",
            "   \ud83d\udd04 Processing batch 192: torch.Size([1, 237733]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  12%|\u2588\u258f        | 193/1600 [05:20<41:26,  1.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 215173])\n",
            "   \ud83d\udd04 Processing batch 193: torch.Size([1, 215173]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  12%|\u2588\u258f        | 194/1600 [05:22<44:30,  1.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 225053])\n",
            "   \ud83d\udd04 Processing batch 194: torch.Size([1, 225053]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  12%|\u2588\u258f        | 195/1600 [05:24<46:51,  2.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 211599])\n",
            "   \ud83d\udd04 Processing batch 195: torch.Size([1, 211599]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  12%|\u2588\u258f        | 196/1600 [05:26<47:08,  2.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 147398])\n",
            "   \ud83d\udd04 Processing batch 196: torch.Size([1, 147398]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  12%|\u2588\u258f        | 197/1600 [05:27<40:52,  1.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 39457])\n",
            "   \ud83d\udd04 Processing batch 197: torch.Size([1, 39457]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  12%|\u2588\u258f        | 198/1600 [05:28<33:26,  1.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 83298])\n",
            "   \ud83d\udd04 Processing batch 198: torch.Size([1, 83298]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  12%|\u2588\u258f        | 199/1600 [05:29<27:46,  1.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 69348])\n",
            "   \ud83d\udd04 Processing batch 199: torch.Size([1, 69348]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  12%|\u2588\u258e        | 200/1600 [05:29<24:01,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 185250])\n",
            "   \ud83d\udd04 Processing batch 200: torch.Size([1, 185250]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  13%|\u2588\u258e        | 201/1600 [05:32<34:16,  1.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 191420])\n",
            "   \ud83d\udd04 Processing batch 201: torch.Size([1, 191420]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  13%|\u2588\u258e        | 202/1600 [05:32<28:21,  1.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 106385])\n",
            "   \ud83d\udd04 Processing batch 202: torch.Size([1, 106385]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  13%|\u2588\u258e        | 203/1600 [05:33<27:10,  1.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 100302])\n",
            "   \ud83d\udd04 Processing batch 203: torch.Size([1, 100302]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  13%|\u2588\u258e        | 204/1600 [05:34<23:18,  1.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 80128])\n",
            "   \ud83d\udd04 Processing batch 204: torch.Size([1, 80128]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  13%|\u2588\u258e        | 205/1600 [05:35<22:25,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 175867])\n",
            "   \ud83d\udd04 Processing batch 205: torch.Size([1, 175867]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  13%|\u2588\u258e        | 206/1600 [05:37<27:06,  1.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 117576])\n",
            "   \ud83d\udd04 Processing batch 206: torch.Size([1, 117576]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  13%|\u2588\u258e        | 207/1600 [05:37<23:12,  1.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 227162])\n",
            "   \ud83d\udd04 Processing batch 207: torch.Size([1, 227162]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  13%|\u2588\u258e        | 208/1600 [05:39<29:33,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 169755])\n",
            "   \ud83d\udd04 Processing batch 208: torch.Size([1, 169755]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  13%|\u2588\u258e        | 209/1600 [05:41<30:36,  1.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 386054])\n",
            "   \ud83d\udd04 Processing batch 209: torch.Size([1, 386054]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  13%|\u2588\u258e        | 210/1600 [05:44<45:17,  1.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 170267])\n",
            "   \ud83d\udd04 Processing batch 210: torch.Size([1, 170267]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  13%|\u2588\u258e        | 211/1600 [05:46<42:19,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 26174])\n",
            "   \ud83d\udd04 Processing batch 211: torch.Size([1, 26174]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  13%|\u2588\u258e        | 212/1600 [05:46<32:03,  1.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 35203])\n",
            "   \ud83d\udd04 Processing batch 212: torch.Size([1, 35203]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  13%|\u2588\u258e        | 213/1600 [05:46<24:51,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 137394])\n",
            "   \ud83d\udd04 Processing batch 213: torch.Size([1, 137394]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  13%|\u2588\u258e        | 214/1600 [05:48<26:14,  1.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 256546])\n",
            "   \ud83d\udd04 Processing batch 214: torch.Size([1, 256546]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  13%|\u2588\u258e        | 215/1600 [05:50<35:47,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 142778])\n",
            "   \ud83d\udd04 Processing batch 215: torch.Size([1, 142778]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  14%|\u2588\u258e        | 216/1600 [05:50<27:16,  1.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 121904])\n",
            "   \ud83d\udd04 Processing batch 216: torch.Size([1, 121904]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  14%|\u2588\u258e        | 217/1600 [05:52<30:14,  1.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 146700])\n",
            "   \ud83d\udd04 Processing batch 217: torch.Size([1, 146700]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  14%|\u2588\u258e        | 218/1600 [05:54<32:36,  1.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 67712])\n",
            "   \ud83d\udd04 Processing batch 218: torch.Size([1, 67712]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  14%|\u2588\u258e        | 219/1600 [05:54<27:42,  1.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 180518])\n",
            "   \ud83d\udd04 Processing batch 219: torch.Size([1, 180518]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  14%|\u2588\u258d        | 220/1600 [05:56<33:24,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 93901])\n",
            "   \ud83d\udd04 Processing batch 220: torch.Size([1, 93901]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  14%|\u2588\u258d        | 221/1600 [05:57<29:54,  1.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 122758])\n",
            "   \ud83d\udd04 Processing batch 221: torch.Size([1, 122758]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  14%|\u2588\u258d        | 222/1600 [05:58<28:58,  1.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 398042])\n",
            "   \ud83d\udd04 Processing batch 222: torch.Size([1, 398042]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  14%|\u2588\u258d        | 223/1600 [06:08<1:27:23,  3.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 176976])\n",
            "   \ud83d\udd04 Processing batch 223: torch.Size([1, 176976]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  14%|\u2588\u258d        | 224/1600 [06:18<2:07:26,  5.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 127984])\n",
            "   \ud83d\udd04 Processing batch 224: torch.Size([1, 127984]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  14%|\u2588\u258d        | 225/1600 [06:19<1:37:49,  4.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 38449])\n",
            "   \ud83d\udd04 Processing batch 225: torch.Size([1, 38449]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  14%|\u2588\u258d        | 226/1600 [06:20<1:12:42,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 39688])\n",
            "   \ud83d\udd04 Processing batch 226: torch.Size([1, 39688]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  14%|\u2588\u258d        | 227/1600 [06:20<55:02,  2.41s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 143718])\n",
            "   \ud83d\udd04 Processing batch 227: torch.Size([1, 143718]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  14%|\u2588\u258d        | 228/1600 [06:22<47:43,  2.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 228: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  14%|\u2588\u258d        | 229/1600 [06:26<1:02:59,  2.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 85144])\n",
            "   \ud83d\udd04 Processing batch 229: torch.Size([1, 85144]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  14%|\u2588\u258d        | 230/1600 [06:27<50:01,  2.19s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 95437])\n",
            "   \ud83d\udd04 Processing batch 230: torch.Size([1, 95437]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  14%|\u2588\u258d        | 231/1600 [06:28<42:25,  1.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 57803])\n",
            "   \ud83d\udd04 Processing batch 231: torch.Size([1, 57803]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  14%|\u2588\u258d        | 232/1600 [06:29<34:14,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 37567])\n",
            "   \ud83d\udd04 Processing batch 232: torch.Size([1, 37567]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  15%|\u2588\u258d        | 233/1600 [06:29<28:09,  1.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 113245])\n",
            "   \ud83d\udd04 Processing batch 233: torch.Size([1, 113245]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  15%|\u2588\u258d        | 234/1600 [06:39<1:25:52,  3.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 75782])\n",
            "   \ud83d\udd04 Processing batch 234: torch.Size([1, 75782]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  15%|\u2588\u258d        | 235/1600 [06:40<1:04:38,  2.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 143165])\n",
            "   \ud83d\udd04 Processing batch 235: torch.Size([1, 143165]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  15%|\u2588\u258d        | 236/1600 [06:41<57:00,  2.51s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 365377])\n",
            "   \ud83d\udd04 Processing batch 236: torch.Size([1, 365377]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  15%|\u2588\u258d        | 237/1600 [06:44<56:08,  2.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 28666])\n",
            "   \ud83d\udd04 Processing batch 237: torch.Size([1, 28666]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  15%|\u2588\u258d        | 238/1600 [06:44<43:33,  1.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 40950])\n",
            "   \ud83d\udd04 Processing batch 238: torch.Size([1, 40950]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  15%|\u2588\u258d        | 239/1600 [06:45<33:27,  1.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 172063])\n",
            "   \ud83d\udd04 Processing batch 239: torch.Size([1, 172063]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  15%|\u2588\u258c        | 240/1600 [06:45<25:51,  1.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 32047])\n",
            "   \ud83d\udd04 Processing batch 240: torch.Size([1, 32047]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  15%|\u2588\u258c        | 241/1600 [06:46<20:31,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 214086])\n",
            "   \ud83d\udd04 Processing batch 241: torch.Size([1, 214086]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  15%|\u2588\u258c        | 242/1600 [06:46<18:38,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 102254])\n",
            "   \ud83d\udd04 Processing batch 242: torch.Size([1, 102254]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  15%|\u2588\u258c        | 243/1600 [06:47<18:43,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 78113])\n",
            "   \ud83d\udd04 Processing batch 243: torch.Size([1, 78113]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  15%|\u2588\u258c        | 244/1600 [06:48<19:00,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 135346])\n",
            "   \ud83d\udd04 Processing batch 244: torch.Size([1, 135346]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  15%|\u2588\u258c        | 245/1600 [06:49<22:21,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 77816])\n",
            "   \ud83d\udd04 Processing batch 245: torch.Size([1, 77816]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  15%|\u2588\u258c        | 246/1600 [06:50<20:22,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 31335])\n",
            "   \ud83d\udd04 Processing batch 246: torch.Size([1, 31335]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  15%|\u2588\u258c        | 247/1600 [06:50<17:15,  1.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 134367])\n",
            "   \ud83d\udd04 Processing batch 247: torch.Size([1, 134367]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  16%|\u2588\u258c        | 248/1600 [06:52<20:53,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 123350])\n",
            "   \ud83d\udd04 Processing batch 248: torch.Size([1, 123350]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  16%|\u2588\u258c        | 249/1600 [06:52<18:47,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 110152])\n",
            "   \ud83d\udd04 Processing batch 249: torch.Size([1, 110152]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  16%|\u2588\u258c        | 250/1600 [06:53<20:34,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 96165])\n",
            "   \ud83d\udd04 Processing batch 250: torch.Size([1, 96165]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  16%|\u2588\u258c        | 251/1600 [06:54<19:24,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 45559])\n",
            "   \ud83d\udd04 Processing batch 251: torch.Size([1, 45559]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  16%|\u2588\u258c        | 252/1600 [06:55<16:32,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 68988])\n",
            "   \ud83d\udd04 Processing batch 252: torch.Size([1, 68988]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  16%|\u2588\u258c        | 253/1600 [06:55<16:07,  1.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 58433])\n",
            "   \ud83d\udd04 Processing batch 253: torch.Size([1, 58433]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  16%|\u2588\u258c        | 254/1600 [06:56<13:41,  1.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 74757])\n",
            "   \ud83d\udd04 Processing batch 254: torch.Size([1, 74757]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  16%|\u2588\u258c        | 255/1600 [06:56<15:08,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 75449])\n",
            "   \ud83d\udd04 Processing batch 255: torch.Size([1, 75449]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  16%|\u2588\u258c        | 256/1600 [06:57<15:51,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 176032])\n",
            "   \ud83d\udd04 Processing batch 256: torch.Size([1, 176032]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  16%|\u2588\u258c        | 257/1600 [07:09<1:30:18,  4.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 238154])\n",
            "   \ud83d\udd04 Processing batch 257: torch.Size([1, 238154]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  16%|\u2588\u258c        | 258/1600 [07:11<1:17:43,  3.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 349864])\n",
            "   \ud83d\udd04 Processing batch 258: torch.Size([1, 349864]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  16%|\u2588\u258c        | 259/1600 [07:14<1:13:13,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 97983])\n",
            "   \ud83d\udd04 Processing batch 259: torch.Size([1, 97983]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  16%|\u2588\u258b        | 260/1600 [07:14<53:38,  2.40s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 347912])\n",
            "   \ud83d\udd04 Processing batch 260: torch.Size([1, 347912]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  16%|\u2588\u258b        | 261/1600 [07:18<1:04:39,  2.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 71823])\n",
            "   \ud83d\udd04 Processing batch 261: torch.Size([1, 71823]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  16%|\u2588\u258b        | 262/1600 [07:19<51:26,  2.31s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 235966])\n",
            "   \ud83d\udd04 Processing batch 262: torch.Size([1, 235966]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  16%|\u2588\u258b        | 263/1600 [07:22<53:11,  2.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 93202])\n",
            "   \ud83d\udd04 Processing batch 263: torch.Size([1, 93202]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  16%|\u2588\u258b        | 264/1600 [07:23<45:56,  2.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 53684])\n",
            "   \ud83d\udd04 Processing batch 264: torch.Size([1, 53684]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  17%|\u2588\u258b        | 265/1600 [07:24<36:39,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 212972])\n",
            "   \ud83d\udd04 Processing batch 265: torch.Size([1, 212972]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  17%|\u2588\u258b        | 266/1600 [07:26<41:00,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 383600])\n",
            "   \ud83d\udd04 Processing batch 266: torch.Size([1, 383600]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  17%|\u2588\u258b        | 267/1600 [07:30<55:41,  2.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 39197])\n",
            "   \ud83d\udd04 Processing batch 267: torch.Size([1, 39197]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  17%|\u2588\u258b        | 268/1600 [07:31<42:12,  1.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 71213])\n",
            "   \ud83d\udd04 Processing batch 268: torch.Size([1, 71213]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  17%|\u2588\u258b        | 269/1600 [07:32<37:15,  1.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 87023])\n",
            "   \ud83d\udd04 Processing batch 269: torch.Size([1, 87023]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  17%|\u2588\u258b        | 270/1600 [07:33<30:43,  1.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 188826])\n",
            "   \ud83d\udd04 Processing batch 270: torch.Size([1, 188826]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  17%|\u2588\u258b        | 271/1600 [07:35<38:34,  1.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 29078])\n",
            "   \ud83d\udd04 Processing batch 271: torch.Size([1, 29078]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  17%|\u2588\u258b        | 272/1600 [07:36<29:19,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 19556])\n",
            "   \ud83d\udd04 Processing batch 272: torch.Size([1, 19556]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  17%|\u2588\u258b        | 273/1600 [07:36<22:51,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 129880])\n",
            "   \ud83d\udd04 Processing batch 273: torch.Size([1, 129880]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  17%|\u2588\u258b        | 274/1600 [07:37<24:43,  1.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 151691])\n",
            "   \ud83d\udd04 Processing batch 274: torch.Size([1, 151691]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  17%|\u2588\u258b        | 275/1600 [07:38<21:23,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 23361])\n",
            "   \ud83d\udd04 Processing batch 275: torch.Size([1, 23361]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  17%|\u2588\u258b        | 276/1600 [07:38<18:26,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 276: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  17%|\u2588\u258b        | 277/1600 [07:50<1:32:01,  4.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 478836])\n",
            "   \ud83d\udd04 Processing batch 277: torch.Size([1, 478836]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  17%|\u2588\u258b        | 278/1600 [07:54<1:27:56,  3.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 72904])\n",
            "   \ud83d\udd04 Processing batch 278: torch.Size([1, 72904]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  17%|\u2588\u258b        | 279/1600 [07:55<1:08:22,  3.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 97184])\n",
            "   \ud83d\udd04 Processing batch 279: torch.Size([1, 97184]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  18%|\u2588\u258a        | 280/1600 [07:56<52:30,  2.39s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 138976])\n",
            "   \ud83d\udd04 Processing batch 280: torch.Size([1, 138976]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  18%|\u2588\u258a        | 281/1600 [07:56<40:45,  1.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 103826])\n",
            "   \ud83d\udd04 Processing batch 281: torch.Size([1, 103826]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  18%|\u2588\u258a        | 282/1600 [08:06<1:32:18,  4.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 83848])\n",
            "   \ud83d\udd04 Processing batch 282: torch.Size([1, 83848]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  18%|\u2588\u258a        | 283/1600 [08:07<1:08:40,  3.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 180003])\n",
            "   \ud83d\udd04 Processing batch 283: torch.Size([1, 180003]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  18%|\u2588\u258a        | 284/1600 [08:07<52:12,  2.38s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 207811])\n",
            "   \ud83d\udd04 Processing batch 284: torch.Size([1, 207811]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  18%|\u2588\u258a        | 285/1600 [08:09<49:47,  2.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 339834])\n",
            "   \ud83d\udd04 Processing batch 285: torch.Size([1, 339834]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  18%|\u2588\u258a        | 286/1600 [08:13<57:47,  2.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 286: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  18%|\u2588\u258a        | 287/1600 [08:13<43:37,  1.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 53438])\n",
            "   \ud83d\udd04 Processing batch 287: torch.Size([1, 53438]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  18%|\u2588\u258a        | 288/1600 [08:14<36:10,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 35146])\n",
            "   \ud83d\udd04 Processing batch 288: torch.Size([1, 35146]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  18%|\u2588\u258a        | 289/1600 [08:14<27:40,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 130560])\n",
            "   \ud83d\udd04 Processing batch 289: torch.Size([1, 130560]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  18%|\u2588\u258a        | 290/1600 [08:15<24:33,  1.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 33441])\n",
            "   \ud83d\udd04 Processing batch 290: torch.Size([1, 33441]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  18%|\u2588\u258a        | 291/1600 [08:16<20:55,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 143215])\n",
            "   \ud83d\udd04 Processing batch 291: torch.Size([1, 143215]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  18%|\u2588\u258a        | 292/1600 [08:16<18:44,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 40143])\n",
            "   \ud83d\udd04 Processing batch 292: torch.Size([1, 40143]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  18%|\u2588\u258a        | 293/1600 [08:17<17:58,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 43018])\n",
            "   \ud83d\udd04 Processing batch 293: torch.Size([1, 43018]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  18%|\u2588\u258a        | 294/1600 [08:18<16:35,  1.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 246410])\n",
            "   \ud83d\udd04 Processing batch 294: torch.Size([1, 246410]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  18%|\u2588\u258a        | 295/1600 [08:20<27:45,  1.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 295: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  18%|\u2588\u258a        | 296/1600 [08:26<54:55,  2.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 265846])\n",
            "   \ud83d\udd04 Processing batch 296: torch.Size([1, 265846]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  19%|\u2588\u258a        | 297/1600 [08:27<48:53,  2.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 176463])\n",
            "   \ud83d\udd04 Processing batch 297: torch.Size([1, 176463]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  19%|\u2588\u258a        | 298/1600 [08:29<44:45,  2.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 97823])\n",
            "   \ud83d\udd04 Processing batch 298: torch.Size([1, 97823]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  19%|\u2588\u258a        | 299/1600 [08:30<35:18,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 58120])\n",
            "   \ud83d\udd04 Processing batch 299: torch.Size([1, 58120]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  19%|\u2588\u2589        | 300/1600 [08:30<29:18,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 186117])\n",
            "   \ud83d\udd04 Processing batch 300: torch.Size([1, 186117]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  19%|\u2588\u2589        | 301/1600 [08:31<24:37,  1.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 210459])\n",
            "   \ud83d\udd04 Processing batch 301: torch.Size([1, 210459]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  19%|\u2588\u2589        | 302/1600 [08:41<1:20:01,  3.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 259809])\n",
            "   \ud83d\udd04 Processing batch 302: torch.Size([1, 259809]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  19%|\u2588\u2589        | 303/1600 [08:42<1:06:32,  3.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 27837])\n",
            "   \ud83d\udd04 Processing batch 303: torch.Size([1, 27837]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  19%|\u2588\u2589        | 304/1600 [08:43<48:56,  2.27s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 395609])\n",
            "   \ud83d\udd04 Processing batch 304: torch.Size([1, 395609]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  19%|\u2588\u2589        | 305/1600 [08:45<50:56,  2.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 125915])\n",
            "   \ud83d\udd04 Processing batch 305: torch.Size([1, 125915]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  19%|\u2588\u2589        | 306/1600 [08:45<38:00,  1.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 139308])\n",
            "   \ud83d\udd04 Processing batch 306: torch.Size([1, 139308]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  19%|\u2588\u2589        | 307/1600 [08:47<35:07,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 151891])\n",
            "   \ud83d\udd04 Processing batch 307: torch.Size([1, 151891]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  19%|\u2588\u2589        | 308/1600 [08:47<28:35,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 30966])\n",
            "   \ud83d\udd04 Processing batch 308: torch.Size([1, 30966]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  19%|\u2588\u2589        | 309/1600 [08:48<23:59,  1.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 168529])\n",
            "   \ud83d\udd04 Processing batch 309: torch.Size([1, 168529]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  19%|\u2588\u2589        | 310/1600 [08:49<20:49,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 134395])\n",
            "   \ud83d\udd04 Processing batch 310: torch.Size([1, 134395]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  19%|\u2588\u2589        | 311/1600 [08:50<21:00,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 208251])\n",
            "   \ud83d\udd04 Processing batch 311: torch.Size([1, 208251]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  20%|\u2588\u2589        | 312/1600 [08:51<21:08,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 103662])\n",
            "   \ud83d\udd04 Processing batch 312: torch.Size([1, 103662]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  20%|\u2588\u2589        | 313/1600 [08:51<18:47,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 108036])\n",
            "   \ud83d\udd04 Processing batch 313: torch.Size([1, 108036]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  20%|\u2588\u2589        | 314/1600 [08:53<23:08,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 189903])\n",
            "   \ud83d\udd04 Processing batch 314: torch.Size([1, 189903]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  20%|\u2588\u2589        | 315/1600 [08:54<26:13,  1.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 395828])\n",
            "   \ud83d\udd04 Processing batch 315: torch.Size([1, 395828]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  20%|\u2588\u2589        | 316/1600 [08:57<35:56,  1.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 154579])\n",
            "   \ud83d\udd04 Processing batch 316: torch.Size([1, 154579]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  20%|\u2588\u2589        | 317/1600 [08:58<29:04,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 134936])\n",
            "   \ud83d\udd04 Processing batch 317: torch.Size([1, 134936]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  20%|\u2588\u2589        | 318/1600 [08:59<28:18,  1.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 80277])\n",
            "   \ud83d\udd04 Processing batch 318: torch.Size([1, 80277]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  20%|\u2588\u2589        | 319/1600 [09:00<24:30,  1.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 159484])\n",
            "   \ud83d\udd04 Processing batch 319: torch.Size([1, 159484]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  20%|\u2588\u2588        | 320/1600 [09:02<28:50,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 455185])\n",
            "   \ud83d\udd04 Processing batch 320: torch.Size([1, 455185]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  20%|\u2588\u2588        | 321/1600 [09:06<46:21,  2.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 127190])\n",
            "   \ud83d\udd04 Processing batch 321: torch.Size([1, 127190]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  20%|\u2588\u2588        | 322/1600 [09:07<38:18,  1.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 280053])\n",
            "   \ud83d\udd04 Processing batch 322: torch.Size([1, 280053]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  20%|\u2588\u2588        | 323/1600 [09:16<1:28:27,  4.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 323558])\n",
            "   \ud83d\udd04 Processing batch 323: torch.Size([1, 323558]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  20%|\u2588\u2588        | 324/1600 [09:19<1:21:49,  3.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 39851])\n",
            "   \ud83d\udd04 Processing batch 324: torch.Size([1, 39851]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  20%|\u2588\u2588        | 325/1600 [09:20<1:00:58,  2.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 114319])\n",
            "   \ud83d\udd04 Processing batch 325: torch.Size([1, 114319]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  20%|\u2588\u2588        | 326/1600 [09:21<48:28,  2.28s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 135038])\n",
            "   \ud83d\udd04 Processing batch 326: torch.Size([1, 135038]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  20%|\u2588\u2588        | 327/1600 [09:21<37:48,  1.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 441447])\n",
            "   \ud83d\udd04 Processing batch 327: torch.Size([1, 441447]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  20%|\u2588\u2588        | 328/1600 [09:24<45:25,  2.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 30682])\n",
            "   \ud83d\udd04 Processing batch 328: torch.Size([1, 30682]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  21%|\u2588\u2588        | 329/1600 [09:25<34:02,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 177643])\n",
            "   \ud83d\udd04 Processing batch 329: torch.Size([1, 177643]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  21%|\u2588\u2588        | 330/1600 [09:27<36:06,  1.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 155272])\n",
            "   \ud83d\udd04 Processing batch 330: torch.Size([1, 155272]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  21%|\u2588\u2588        | 331/1600 [09:28<33:54,  1.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 78954])\n",
            "   \ud83d\udd04 Processing batch 331: torch.Size([1, 78954]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  21%|\u2588\u2588        | 332/1600 [09:29<27:38,  1.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 131218])\n",
            "   \ud83d\udd04 Processing batch 332: torch.Size([1, 131218]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  21%|\u2588\u2588        | 333/1600 [09:30<28:22,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 53745])\n",
            "   \ud83d\udd04 Processing batch 333: torch.Size([1, 53745]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  21%|\u2588\u2588        | 334/1600 [09:31<23:44,  1.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 59538])\n",
            "   \ud83d\udd04 Processing batch 334: torch.Size([1, 59538]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  21%|\u2588\u2588        | 335/1600 [09:31<19:40,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 57674])\n",
            "   \ud83d\udd04 Processing batch 335: torch.Size([1, 57674]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  21%|\u2588\u2588        | 336/1600 [09:32<17:36,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 134191])\n",
            "   \ud83d\udd04 Processing batch 336: torch.Size([1, 134191]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  21%|\u2588\u2588        | 337/1600 [09:33<17:18,  1.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 181331])\n",
            "   \ud83d\udd04 Processing batch 337: torch.Size([1, 181331]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  21%|\u2588\u2588        | 338/1600 [09:35<26:35,  1.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 323965])\n",
            "   \ud83d\udd04 Processing batch 338: torch.Size([1, 323965]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  21%|\u2588\u2588        | 339/1600 [09:39<41:41,  1.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 88239])\n",
            "   \ud83d\udd04 Processing batch 339: torch.Size([1, 88239]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  21%|\u2588\u2588\u258f       | 340/1600 [09:40<37:28,  1.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 67682])\n",
            "   \ud83d\udd04 Processing batch 340: torch.Size([1, 67682]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  21%|\u2588\u2588\u258f       | 341/1600 [09:41<31:41,  1.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 187466])\n",
            "   \ud83d\udd04 Processing batch 341: torch.Size([1, 187466]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  21%|\u2588\u2588\u258f       | 342/1600 [09:43<33:51,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 279102])\n",
            "   \ud83d\udd04 Processing batch 342: torch.Size([1, 279102]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  21%|\u2588\u2588\u258f       | 343/1600 [09:45<40:04,  1.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 86610])\n",
            "   \ud83d\udd04 Processing batch 343: torch.Size([1, 86610]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  22%|\u2588\u2588\u258f       | 344/1600 [09:46<34:17,  1.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 160215])\n",
            "   \ud83d\udd04 Processing batch 344: torch.Size([1, 160215]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  22%|\u2588\u2588\u258f       | 345/1600 [09:48<33:51,  1.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 63048])\n",
            "   \ud83d\udd04 Processing batch 345: torch.Size([1, 63048]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  22%|\u2588\u2588\u258f       | 346/1600 [09:49<27:47,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 216882])\n",
            "   \ud83d\udd04 Processing batch 346: torch.Size([1, 216882]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  22%|\u2588\u2588\u258f       | 347/1600 [09:51<33:56,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 96608])\n",
            "   \ud83d\udd04 Processing batch 347: torch.Size([1, 96608]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  22%|\u2588\u2588\u258f       | 348/1600 [09:52<29:39,  1.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 145390])\n",
            "   \ud83d\udd04 Processing batch 348: torch.Size([1, 145390]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  22%|\u2588\u2588\u258f       | 349/1600 [09:53<28:50,  1.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 164703])\n",
            "   \ud83d\udd04 Processing batch 349: torch.Size([1, 164703]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  22%|\u2588\u2588\u258f       | 350/1600 [09:54<28:16,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 154005])\n",
            "   \ud83d\udd04 Processing batch 350: torch.Size([1, 154005]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  22%|\u2588\u2588\u258f       | 351/1600 [09:55<26:17,  1.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 393941])\n",
            "   \ud83d\udd04 Processing batch 351: torch.Size([1, 393941]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  22%|\u2588\u2588\u258f       | 352/1600 [10:00<45:07,  2.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 93714])\n",
            "   \ud83d\udd04 Processing batch 352: torch.Size([1, 93714]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  22%|\u2588\u2588\u258f       | 353/1600 [10:00<35:23,  1.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 417372])\n",
            "   \ud83d\udd04 Processing batch 353: torch.Size([1, 417372]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  22%|\u2588\u2588\u258f       | 354/1600 [10:01<27:36,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 303819])\n",
            "   \ud83d\udd04 Processing batch 354: torch.Size([1, 303819]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  22%|\u2588\u2588\u258f       | 355/1600 [10:03<31:19,  1.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 76185])\n",
            "   \ud83d\udd04 Processing batch 355: torch.Size([1, 76185]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  22%|\u2588\u2588\u258f       | 356/1600 [10:03<25:44,  1.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 264203])\n",
            "   \ud83d\udd04 Processing batch 356: torch.Size([1, 264203]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  22%|\u2588\u2588\u258f       | 357/1600 [10:04<21:54,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 110456])\n",
            "   \ud83d\udd04 Processing batch 357: torch.Size([1, 110456]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  22%|\u2588\u2588\u258f       | 358/1600 [10:05<21:03,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 448536])\n",
            "   \ud83d\udd04 Processing batch 358: torch.Size([1, 448536]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  22%|\u2588\u2588\u258f       | 359/1600 [10:16<1:20:59,  3.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 98931])\n",
            "   \ud83d\udd04 Processing batch 359: torch.Size([1, 98931]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  22%|\u2588\u2588\u258e       | 360/1600 [10:16<1:02:06,  3.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 123824])\n",
            "   \ud83d\udd04 Processing batch 360: torch.Size([1, 123824]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  23%|\u2588\u2588\u258e       | 361/1600 [10:17<47:19,  2.29s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 63706])\n",
            "   \ud83d\udd04 Processing batch 361: torch.Size([1, 63706]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  23%|\u2588\u2588\u258e       | 362/1600 [10:18<36:37,  1.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 209098])\n",
            "   \ud83d\udd04 Processing batch 362: torch.Size([1, 209098]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  23%|\u2588\u2588\u258e       | 363/1600 [10:20<38:55,  1.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 42727])\n",
            "   \ud83d\udd04 Processing batch 363: torch.Size([1, 42727]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  23%|\u2588\u2588\u258e       | 364/1600 [10:20<31:34,  1.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 154285])\n",
            "   \ud83d\udd04 Processing batch 364: torch.Size([1, 154285]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  23%|\u2588\u2588\u258e       | 365/1600 [10:22<32:00,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 254959])\n",
            "   \ud83d\udd04 Processing batch 365: torch.Size([1, 254959]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  23%|\u2588\u2588\u258e       | 366/1600 [10:24<33:27,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 106619])\n",
            "   \ud83d\udd04 Processing batch 366: torch.Size([1, 106619]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  23%|\u2588\u2588\u258e       | 367/1600 [10:25<31:50,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 140885])\n",
            "   \ud83d\udd04 Processing batch 367: torch.Size([1, 140885]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  23%|\u2588\u2588\u258e       | 368/1600 [10:27<32:22,  1.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 129806])\n",
            "   \ud83d\udd04 Processing batch 368: torch.Size([1, 129806]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  23%|\u2588\u2588\u258e       | 369/1600 [10:28<31:23,  1.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 41614])\n",
            "   \ud83d\udd04 Processing batch 369: torch.Size([1, 41614]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  23%|\u2588\u2588\u258e       | 370/1600 [10:29<25:13,  1.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 110617])\n",
            "   \ud83d\udd04 Processing batch 370: torch.Size([1, 110617]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  23%|\u2588\u2588\u258e       | 371/1600 [10:30<26:47,  1.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 224786])\n",
            "   \ud83d\udd04 Processing batch 371: torch.Size([1, 224786]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  23%|\u2588\u2588\u258e       | 372/1600 [10:32<31:43,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 348338])\n",
            "   \ud83d\udd04 Processing batch 372: torch.Size([1, 348338]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  23%|\u2588\u2588\u258e       | 373/1600 [10:36<41:00,  2.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 461741])\n",
            "   \ud83d\udd04 Processing batch 373: torch.Size([1, 461741]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  23%|\u2588\u2588\u258e       | 374/1600 [10:39<52:54,  2.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 352442])\n",
            "   \ud83d\udd04 Processing batch 374: torch.Size([1, 352442]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  23%|\u2588\u2588\u258e       | 375/1600 [10:40<40:35,  1.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 188266])\n",
            "   \ud83d\udd04 Processing batch 375: torch.Size([1, 188266]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  24%|\u2588\u2588\u258e       | 376/1600 [10:42<39:41,  1.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 44999])\n",
            "   \ud83d\udd04 Processing batch 376: torch.Size([1, 44999]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  24%|\u2588\u2588\u258e       | 377/1600 [10:42<31:16,  1.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 145634])\n",
            "   \ud83d\udd04 Processing batch 377: torch.Size([1, 145634]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  24%|\u2588\u2588\u258e       | 378/1600 [10:43<25:36,  1.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 38744])\n",
            "   \ud83d\udd04 Processing batch 378: torch.Size([1, 38744]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  24%|\u2588\u2588\u258e       | 379/1600 [10:44<21:42,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 360165])\n",
            "   \ud83d\udd04 Processing batch 379: torch.Size([1, 360165]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  24%|\u2588\u2588\u258d       | 380/1600 [10:46<31:30,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 146443])\n",
            "   \ud83d\udd04 Processing batch 380: torch.Size([1, 146443]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  24%|\u2588\u2588\u258d       | 381/1600 [10:47<25:48,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 223092])\n",
            "   \ud83d\udd04 Processing batch 381: torch.Size([1, 223092]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  24%|\u2588\u2588\u258d       | 382/1600 [10:48<22:12,  1.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 91487])\n",
            "   \ud83d\udd04 Processing batch 382: torch.Size([1, 91487]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  24%|\u2588\u2588\u258d       | 383/1600 [10:48<17:42,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 258666])\n",
            "   \ud83d\udd04 Processing batch 383: torch.Size([1, 258666]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  24%|\u2588\u2588\u258d       | 384/1600 [10:50<22:22,  1.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 267156])\n",
            "   \ud83d\udd04 Processing batch 384: torch.Size([1, 267156]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  24%|\u2588\u2588\u258d       | 385/1600 [10:51<25:39,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 62851])\n",
            "   \ud83d\udd04 Processing batch 385: torch.Size([1, 62851]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  24%|\u2588\u2588\u258d       | 386/1600 [10:52<23:00,  1.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 206227])\n",
            "   \ud83d\udd04 Processing batch 386: torch.Size([1, 206227]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  24%|\u2588\u2588\u258d       | 387/1600 [10:54<24:46,  1.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 35867])\n",
            "   \ud83d\udd04 Processing batch 387: torch.Size([1, 35867]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  24%|\u2588\u2588\u258d       | 388/1600 [10:54<21:03,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 93484])\n",
            "   \ud83d\udd04 Processing batch 388: torch.Size([1, 93484]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  24%|\u2588\u2588\u258d       | 389/1600 [10:55<18:25,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 198158])\n",
            "   \ud83d\udd04 Processing batch 389: torch.Size([1, 198158]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  24%|\u2588\u2588\u258d       | 390/1600 [10:57<24:11,  1.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 55246])\n",
            "   \ud83d\udd04 Processing batch 390: torch.Size([1, 55246]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  24%|\u2588\u2588\u258d       | 391/1600 [10:57<21:13,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 151060])\n",
            "   \ud83d\udd04 Processing batch 391: torch.Size([1, 151060]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  24%|\u2588\u2588\u258d       | 392/1600 [10:59<25:20,  1.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 392: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  25%|\u2588\u2588\u258d       | 393/1600 [11:03<42:29,  2.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 189949])\n",
            "   \ud83d\udd04 Processing batch 393: torch.Size([1, 189949]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  25%|\u2588\u2588\u258d       | 394/1600 [11:05<39:04,  1.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 39057])\n",
            "   \ud83d\udd04 Processing batch 394: torch.Size([1, 39057]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  25%|\u2588\u2588\u258d       | 395/1600 [11:05<29:58,  1.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 165095])\n",
            "   \ud83d\udd04 Processing batch 395: torch.Size([1, 165095]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  25%|\u2588\u2588\u258d       | 396/1600 [11:07<28:48,  1.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 433666])\n",
            "   \ud83d\udd04 Processing batch 396: torch.Size([1, 433666]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  25%|\u2588\u2588\u258d       | 397/1600 [11:13<58:19,  2.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 135710])\n",
            "   \ud83d\udd04 Processing batch 397: torch.Size([1, 135710]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  25%|\u2588\u2588\u258d       | 398/1600 [11:15<53:27,  2.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 274685])\n",
            "   \ud83d\udd04 Processing batch 398: torch.Size([1, 274685]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  25%|\u2588\u2588\u258d       | 399/1600 [11:25<1:35:16,  4.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 223955])\n",
            "   \ud83d\udd04 Processing batch 399: torch.Size([1, 223955]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  25%|\u2588\u2588\u258c       | 400/1600 [11:26<1:16:28,  3.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 136897])\n",
            "   \ud83d\udd04 Processing batch 400: torch.Size([1, 136897]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  25%|\u2588\u2588\u258c       | 401/1600 [11:27<59:55,  3.00s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 25188])\n",
            "   \ud83d\udd04 Processing batch 401: torch.Size([1, 25188]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  25%|\u2588\u2588\u258c       | 402/1600 [11:28<45:33,  2.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 194582])\n",
            "   \ud83d\udd04 Processing batch 402: torch.Size([1, 194582]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  25%|\u2588\u2588\u258c       | 403/1600 [11:29<40:44,  2.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 403: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  25%|\u2588\u2588\u258c       | 404/1600 [11:32<46:12,  2.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 188634])\n",
            "   \ud83d\udd04 Processing batch 404: torch.Size([1, 188634]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  25%|\u2588\u2588\u258c       | 405/1600 [11:34<44:26,  2.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 226657])\n",
            "   \ud83d\udd04 Processing batch 405: torch.Size([1, 226657]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  25%|\u2588\u2588\u258c       | 406/1600 [11:35<34:50,  1.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 180019])\n",
            "   \ud83d\udd04 Processing batch 406: torch.Size([1, 180019]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  25%|\u2588\u2588\u258c       | 407/1600 [11:36<28:03,  1.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 405183])\n",
            "   \ud83d\udd04 Processing batch 407: torch.Size([1, 405183]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  26%|\u2588\u2588\u258c       | 408/1600 [11:41<53:38,  2.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 170158])\n",
            "   \ud83d\udd04 Processing batch 408: torch.Size([1, 170158]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  26%|\u2588\u2588\u258c       | 409/1600 [11:42<41:58,  2.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 65539])\n",
            "   \ud83d\udd04 Processing batch 409: torch.Size([1, 65539]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  26%|\u2588\u2588\u258c       | 410/1600 [11:43<32:57,  1.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 115786])\n",
            "   \ud83d\udd04 Processing batch 410: torch.Size([1, 115786]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  26%|\u2588\u2588\u258c       | 411/1600 [11:43<26:42,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 138583])\n",
            "   \ud83d\udd04 Processing batch 411: torch.Size([1, 138583]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  26%|\u2588\u2588\u258c       | 412/1600 [11:45<27:29,  1.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 268265])\n",
            "   \ud83d\udd04 Processing batch 412: torch.Size([1, 268265]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  26%|\u2588\u2588\u258c       | 413/1600 [11:47<34:53,  1.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 199486])\n",
            "   \ud83d\udd04 Processing batch 413: torch.Size([1, 199486]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  26%|\u2588\u2588\u258c       | 414/1600 [11:49<35:52,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 298747])\n",
            "   \ud83d\udd04 Processing batch 414: torch.Size([1, 298747]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  26%|\u2588\u2588\u258c       | 415/1600 [11:52<41:42,  2.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 177847])\n",
            "   \ud83d\udd04 Processing batch 415: torch.Size([1, 177847]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  26%|\u2588\u2588\u258c       | 416/1600 [11:54<39:38,  2.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 161477])\n",
            "   \ud83d\udd04 Processing batch 416: torch.Size([1, 161477]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  26%|\u2588\u2588\u258c       | 417/1600 [11:56<37:07,  1.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 203642])\n",
            "   \ud83d\udd04 Processing batch 417: torch.Size([1, 203642]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  26%|\u2588\u2588\u258c       | 418/1600 [11:57<34:01,  1.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 108455])\n",
            "   \ud83d\udd04 Processing batch 418: torch.Size([1, 108455]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  26%|\u2588\u2588\u258c       | 419/1600 [11:58<29:14,  1.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 387568])\n",
            "   \ud83d\udd04 Processing batch 419: torch.Size([1, 387568]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  26%|\u2588\u2588\u258b       | 420/1600 [12:01<39:23,  2.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 67786])\n",
            "   \ud83d\udd04 Processing batch 420: torch.Size([1, 67786]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  26%|\u2588\u2588\u258b       | 421/1600 [12:02<31:15,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 337591])\n",
            "   \ud83d\udd04 Processing batch 421: torch.Size([1, 337591]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  26%|\u2588\u2588\u258b       | 422/1600 [12:05<38:42,  1.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 342825])\n",
            "   \ud83d\udd04 Processing batch 422: torch.Size([1, 342825]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  26%|\u2588\u2588\u258b       | 423/1600 [12:14<1:24:28,  4.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 296577])\n",
            "   \ud83d\udd04 Processing batch 423: torch.Size([1, 296577]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  26%|\u2588\u2588\u258b       | 424/1600 [12:17<1:14:36,  3.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 133534])\n",
            "   \ud83d\udd04 Processing batch 424: torch.Size([1, 133534]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  27%|\u2588\u2588\u258b       | 425/1600 [12:18<55:47,  2.85s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 451983])\n",
            "   \ud83d\udd04 Processing batch 425: torch.Size([1, 451983]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  27%|\u2588\u2588\u258b       | 426/1600 [12:20<55:15,  2.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 127844])\n",
            "   \ud83d\udd04 Processing batch 426: torch.Size([1, 127844]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  27%|\u2588\u2588\u258b       | 427/1600 [12:21<40:47,  2.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 427425])\n",
            "   \ud83d\udd04 Processing batch 427: torch.Size([1, 427425]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  27%|\u2588\u2588\u258b       | 428/1600 [12:25<51:40,  2.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 66939])\n",
            "   \ud83d\udd04 Processing batch 428: torch.Size([1, 66939]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  27%|\u2588\u2588\u258b       | 429/1600 [12:25<39:44,  2.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 185401])\n",
            "   \ud83d\udd04 Processing batch 429: torch.Size([1, 185401]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  27%|\u2588\u2588\u258b       | 430/1600 [12:27<39:27,  2.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 152700])\n",
            "   \ud83d\udd04 Processing batch 430: torch.Size([1, 152700]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  27%|\u2588\u2588\u258b       | 431/1600 [12:28<33:42,  1.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 248995])\n",
            "   \ud83d\udd04 Processing batch 431: torch.Size([1, 248995]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  27%|\u2588\u2588\u258b       | 432/1600 [12:29<30:27,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 307919])\n",
            "   \ud83d\udd04 Processing batch 432: torch.Size([1, 307919]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  27%|\u2588\u2588\u258b       | 433/1600 [12:30<24:00,  1.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 276317])\n",
            "   \ud83d\udd04 Processing batch 433: torch.Size([1, 276317]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  27%|\u2588\u2588\u258b       | 434/1600 [12:32<31:34,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 76277])\n",
            "   \ud83d\udd04 Processing batch 434: torch.Size([1, 76277]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  27%|\u2588\u2588\u258b       | 435/1600 [12:33<25:38,  1.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 475240])\n",
            "   \ud83d\udd04 Processing batch 435: torch.Size([1, 475240]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  27%|\u2588\u2588\u258b       | 436/1600 [12:35<31:09,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 188986])\n",
            "   \ud83d\udd04 Processing batch 436: torch.Size([1, 188986]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  27%|\u2588\u2588\u258b       | 437/1600 [12:37<30:06,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 213970])\n",
            "   \ud83d\udd04 Processing batch 437: torch.Size([1, 213970]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  27%|\u2588\u2588\u258b       | 438/1600 [12:39<34:18,  1.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 160594])\n",
            "   \ud83d\udd04 Processing batch 438: torch.Size([1, 160594]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  27%|\u2588\u2588\u258b       | 439/1600 [12:41<34:05,  1.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 161115])\n",
            "   \ud83d\udd04 Processing batch 439: torch.Size([1, 161115]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  28%|\u2588\u2588\u258a       | 440/1600 [12:43<36:40,  1.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 110892])\n",
            "   \ud83d\udd04 Processing batch 440: torch.Size([1, 110892]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  28%|\u2588\u2588\u258a       | 441/1600 [12:44<29:11,  1.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 269731])\n",
            "   \ud83d\udd04 Processing batch 441: torch.Size([1, 269731]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  28%|\u2588\u2588\u258a       | 442/1600 [12:45<28:14,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 216308])\n",
            "   \ud83d\udd04 Processing batch 442: torch.Size([1, 216308]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  28%|\u2588\u2588\u258a       | 443/1600 [12:47<31:17,  1.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 69272])\n",
            "   \ud83d\udd04 Processing batch 443: torch.Size([1, 69272]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  28%|\u2588\u2588\u258a       | 444/1600 [12:48<25:26,  1.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 59123])\n",
            "   \ud83d\udd04 Processing batch 444: torch.Size([1, 59123]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  28%|\u2588\u2588\u258a       | 445/1600 [12:57<1:13:49,  3.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 351002])\n",
            "   \ud83d\udd04 Processing batch 445: torch.Size([1, 351002]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  28%|\u2588\u2588\u258a       | 446/1600 [13:01<1:10:27,  3.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 25653])\n",
            "   \ud83d\udd04 Processing batch 446: torch.Size([1, 25653]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  28%|\u2588\u2588\u258a       | 447/1600 [13:01<52:33,  2.74s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 62921])\n",
            "   \ud83d\udd04 Processing batch 447: torch.Size([1, 62921]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  28%|\u2588\u2588\u258a       | 448/1600 [13:02<40:21,  2.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 49265])\n",
            "   \ud83d\udd04 Processing batch 448: torch.Size([1, 49265]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  28%|\u2588\u2588\u258a       | 449/1600 [13:02<32:14,  1.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 52600])\n",
            "   \ud83d\udd04 Processing batch 449: torch.Size([1, 52600]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  28%|\u2588\u2588\u258a       | 450/1600 [13:03<26:33,  1.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 265740])\n",
            "   \ud83d\udd04 Processing batch 450: torch.Size([1, 265740]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  28%|\u2588\u2588\u258a       | 451/1600 [13:05<29:52,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 41780])\n",
            "   \ud83d\udd04 Processing batch 451: torch.Size([1, 41780]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  28%|\u2588\u2588\u258a       | 452/1600 [13:06<24:26,  1.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 35624])\n",
            "   \ud83d\udd04 Processing batch 452: torch.Size([1, 35624]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  28%|\u2588\u2588\u258a       | 453/1600 [13:06<20:39,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 82866])\n",
            "   \ud83d\udd04 Processing batch 453: torch.Size([1, 82866]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  28%|\u2588\u2588\u258a       | 454/1600 [13:07<18:03,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 454: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  28%|\u2588\u2588\u258a       | 455/1600 [13:08<20:38,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 175755])\n",
            "   \ud83d\udd04 Processing batch 455: torch.Size([1, 175755]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  28%|\u2588\u2588\u258a       | 456/1600 [13:09<17:59,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 100786])\n",
            "   \ud83d\udd04 Processing batch 456: torch.Size([1, 100786]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  29%|\u2588\u2588\u258a       | 457/1600 [13:10<17:51,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 61656])\n",
            "   \ud83d\udd04 Processing batch 457: torch.Size([1, 61656]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  29%|\u2588\u2588\u258a       | 458/1600 [13:11<16:01,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 182001])\n",
            "   \ud83d\udd04 Processing batch 458: torch.Size([1, 182001]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  29%|\u2588\u2588\u258a       | 459/1600 [13:12<20:51,  1.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 353915])\n",
            "   \ud83d\udd04 Processing batch 459: torch.Size([1, 353915]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  29%|\u2588\u2588\u2589       | 460/1600 [13:15<32:37,  1.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 204318])\n",
            "   \ud83d\udd04 Processing batch 460: torch.Size([1, 204318]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  29%|\u2588\u2588\u2589       | 461/1600 [13:17<30:22,  1.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 254677])\n",
            "   \ud83d\udd04 Processing batch 461: torch.Size([1, 254677]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  29%|\u2588\u2588\u2589       | 462/1600 [13:18<28:56,  1.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 62731])\n",
            "   \ud83d\udd04 Processing batch 462: torch.Size([1, 62731]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  29%|\u2588\u2588\u2589       | 463/1600 [13:19<24:17,  1.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 136210])\n",
            "   \ud83d\udd04 Processing batch 463: torch.Size([1, 136210]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  29%|\u2588\u2588\u2589       | 464/1600 [13:20<25:05,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 92109])\n",
            "   \ud83d\udd04 Processing batch 464: torch.Size([1, 92109]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  29%|\u2588\u2588\u2589       | 465/1600 [13:21<20:17,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 39303])\n",
            "   \ud83d\udd04 Processing batch 465: torch.Size([1, 39303]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  29%|\u2588\u2588\u2589       | 466/1600 [13:21<17:43,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 147110])\n",
            "   \ud83d\udd04 Processing batch 466: torch.Size([1, 147110]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  29%|\u2588\u2588\u2589       | 467/1600 [13:31<1:07:07,  3.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 90325])\n",
            "   \ud83d\udd04 Processing batch 467: torch.Size([1, 90325]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  29%|\u2588\u2588\u2589       | 468/1600 [13:41<1:41:51,  5.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 298411])\n",
            "   \ud83d\udd04 Processing batch 468: torch.Size([1, 298411]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  29%|\u2588\u2588\u2589       | 469/1600 [13:43<1:23:41,  4.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 131391])\n",
            "   \ud83d\udd04 Processing batch 469: torch.Size([1, 131391]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  29%|\u2588\u2588\u2589       | 470/1600 [13:44<1:05:53,  3.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 217849])\n",
            "   \ud83d\udd04 Processing batch 470: torch.Size([1, 217849]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  29%|\u2588\u2588\u2589       | 471/1600 [13:54<1:41:00,  5.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 208045])\n",
            "   \ud83d\udd04 Processing batch 471: torch.Size([1, 208045]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  30%|\u2588\u2588\u2589       | 472/1600 [13:56<1:20:57,  4.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 472: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  30%|\u2588\u2588\u2589       | 473/1600 [14:00<1:21:56,  4.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 203440])\n",
            "   \ud83d\udd04 Processing batch 473: torch.Size([1, 203440]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  30%|\u2588\u2588\u2589       | 474/1600 [14:02<1:05:49,  3.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 190449])\n",
            "   \ud83d\udd04 Processing batch 474: torch.Size([1, 190449]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  30%|\u2588\u2588\u2589       | 475/1600 [14:04<58:34,  3.12s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 123302])\n",
            "   \ud83d\udd04 Processing batch 475: torch.Size([1, 123302]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  30%|\u2588\u2588\u2589       | 476/1600 [14:05<44:28,  2.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 156634])\n",
            "   \ud83d\udd04 Processing batch 476: torch.Size([1, 156634]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  30%|\u2588\u2588\u2589       | 477/1600 [14:06<41:17,  2.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 34702])\n",
            "   \ud83d\udd04 Processing batch 477: torch.Size([1, 34702]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  30%|\u2588\u2588\u2589       | 478/1600 [14:07<32:24,  1.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 268649])\n",
            "   \ud83d\udd04 Processing batch 478: torch.Size([1, 268649]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  30%|\u2588\u2588\u2589       | 479/1600 [14:11<42:52,  2.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 68979])\n",
            "   \ud83d\udd04 Processing batch 479: torch.Size([1, 68979]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  30%|\u2588\u2588\u2588       | 480/1600 [14:11<33:26,  1.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 32161])\n",
            "   \ud83d\udd04 Processing batch 480: torch.Size([1, 32161]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  30%|\u2588\u2588\u2588       | 481/1600 [14:12<26:21,  1.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 280030])\n",
            "   \ud83d\udd04 Processing batch 481: torch.Size([1, 280030]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  30%|\u2588\u2588\u2588       | 482/1600 [14:14<31:47,  1.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 153161])\n",
            "   \ud83d\udd04 Processing batch 482: torch.Size([1, 153161]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  30%|\u2588\u2588\u2588       | 483/1600 [14:15<29:19,  1.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 110213])\n",
            "   \ud83d\udd04 Processing batch 483: torch.Size([1, 110213]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  30%|\u2588\u2588\u2588       | 484/1600 [14:16<23:59,  1.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 106249])\n",
            "   \ud83d\udd04 Processing batch 484: torch.Size([1, 106249]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  30%|\u2588\u2588\u2588       | 485/1600 [14:17<22:51,  1.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 485: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  30%|\u2588\u2588\u2588       | 486/1600 [14:22<42:20,  2.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 63000])\n",
            "   \ud83d\udd04 Processing batch 486: torch.Size([1, 63000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  30%|\u2588\u2588\u2588       | 487/1600 [14:23<34:31,  1.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 123897])\n",
            "   \ud83d\udd04 Processing batch 487: torch.Size([1, 123897]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  30%|\u2588\u2588\u2588       | 488/1600 [14:23<27:36,  1.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 198583])\n",
            "   \ud83d\udd04 Processing batch 488: torch.Size([1, 198583]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  31%|\u2588\u2588\u2588       | 489/1600 [14:25<26:23,  1.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 322764])\n",
            "   \ud83d\udd04 Processing batch 489: torch.Size([1, 322764]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  31%|\u2588\u2588\u2588       | 490/1600 [14:28<36:30,  1.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 134131])\n",
            "   \ud83d\udd04 Processing batch 490: torch.Size([1, 134131]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  31%|\u2588\u2588\u2588       | 491/1600 [14:29<28:57,  1.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 56149])\n",
            "   \ud83d\udd04 Processing batch 491: torch.Size([1, 56149]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  31%|\u2588\u2588\u2588       | 492/1600 [14:29<24:22,  1.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 27723])\n",
            "   \ud83d\udd04 Processing batch 492: torch.Size([1, 27723]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  31%|\u2588\u2588\u2588       | 493/1600 [14:30<19:03,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 384295])\n",
            "   \ud83d\udd04 Processing batch 493: torch.Size([1, 384295]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  31%|\u2588\u2588\u2588       | 494/1600 [14:30<16:52,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 494: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  31%|\u2588\u2588\u2588       | 495/1600 [14:33<24:59,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 160975])\n",
            "   \ud83d\udd04 Processing batch 495: torch.Size([1, 160975]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  31%|\u2588\u2588\u2588       | 496/1600 [14:35<30:18,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 64602])\n",
            "   \ud83d\udd04 Processing batch 496: torch.Size([1, 64602]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  31%|\u2588\u2588\u2588       | 497/1600 [14:36<25:07,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 51756])\n",
            "   \ud83d\udd04 Processing batch 497: torch.Size([1, 51756]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  31%|\u2588\u2588\u2588       | 498/1600 [14:37<22:05,  1.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 33543])\n",
            "   \ud83d\udd04 Processing batch 498: torch.Size([1, 33543]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  31%|\u2588\u2588\u2588       | 499/1600 [14:37<17:56,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 73396])\n",
            "   \ud83d\udd04 Processing batch 499: torch.Size([1, 73396]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  31%|\u2588\u2588\u2588\u258f      | 500/1600 [14:38<18:03,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 42529])\n",
            "   \ud83d\udd04 Processing batch 500: torch.Size([1, 42529]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  31%|\u2588\u2588\u2588\u258f      | 501/1600 [14:38<14:23,  1.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 26477])\n",
            "   \ud83d\udd04 Processing batch 501: torch.Size([1, 26477]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  31%|\u2588\u2588\u2588\u258f      | 502/1600 [14:39<12:03,  1.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 502: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  31%|\u2588\u2588\u2588\u258f      | 503/1600 [14:44<39:56,  2.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 156612])\n",
            "   \ud83d\udd04 Processing batch 503: torch.Size([1, 156612]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  32%|\u2588\u2588\u2588\u258f      | 504/1600 [14:46<35:20,  1.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 118960])\n",
            "   \ud83d\udd04 Processing batch 504: torch.Size([1, 118960]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  32%|\u2588\u2588\u2588\u258f      | 505/1600 [14:46<28:09,  1.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 266911])\n",
            "   \ud83d\udd04 Processing batch 505: torch.Size([1, 266911]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  32%|\u2588\u2588\u2588\u258f      | 506/1600 [14:47<23:08,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 75032])\n",
            "   \ud83d\udd04 Processing batch 506: torch.Size([1, 75032]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  32%|\u2588\u2588\u2588\u258f      | 507/1600 [14:48<20:57,  1.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 35763])\n",
            "   \ud83d\udd04 Processing batch 507: torch.Size([1, 35763]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  32%|\u2588\u2588\u2588\u258f      | 508/1600 [14:49<18:34,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 176983])\n",
            "   \ud83d\udd04 Processing batch 508: torch.Size([1, 176983]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  32%|\u2588\u2588\u2588\u258f      | 509/1600 [14:50<19:30,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 180948])\n",
            "   \ud83d\udd04 Processing batch 509: torch.Size([1, 180948]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  32%|\u2588\u2588\u2588\u258f      | 510/1600 [14:50<17:01,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 445576])\n",
            "   \ud83d\udd04 Processing batch 510: torch.Size([1, 445576]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  32%|\u2588\u2588\u2588\u258f      | 511/1600 [15:10<1:57:49,  6.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 51664])\n",
            "   \ud83d\udd04 Processing batch 511: torch.Size([1, 51664]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  32%|\u2588\u2588\u2588\u258f      | 512/1600 [15:10<1:24:49,  4.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 138301])\n",
            "   \ud83d\udd04 Processing batch 512: torch.Size([1, 138301]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  32%|\u2588\u2588\u2588\u258f      | 513/1600 [15:14<1:16:43,  4.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 261059])\n",
            "   \ud83d\udd04 Processing batch 513: torch.Size([1, 261059]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  32%|\u2588\u2588\u2588\u258f      | 514/1600 [15:14<57:37,  3.18s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 306973])\n",
            "   \ud83d\udd04 Processing batch 514: torch.Size([1, 306973]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  32%|\u2588\u2588\u2588\u258f      | 515/1600 [15:17<55:49,  3.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 133674])\n",
            "   \ud83d\udd04 Processing batch 515: torch.Size([1, 133674]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  32%|\u2588\u2588\u2588\u258f      | 516/1600 [15:19<47:13,  2.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 195791])\n",
            "   \ud83d\udd04 Processing batch 516: torch.Size([1, 195791]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  32%|\u2588\u2588\u2588\u258f      | 517/1600 [15:20<41:43,  2.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 480000])\n",
            "   \ud83d\udd04 Processing batch 517: torch.Size([1, 480000]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  32%|\u2588\u2588\u2588\u258f      | 518/1600 [15:25<53:11,  2.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 103037])\n",
            "   \ud83d\udd04 Processing batch 518: torch.Size([1, 103037]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  32%|\u2588\u2588\u2588\u258f      | 519/1600 [15:25<40:34,  2.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 50884])\n",
            "   \ud83d\udd04 Processing batch 519: torch.Size([1, 50884]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  32%|\u2588\u2588\u2588\u258e      | 520/1600 [15:26<31:44,  1.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 252594])\n",
            "   \ud83d\udd04 Processing batch 520: torch.Size([1, 252594]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  33%|\u2588\u2588\u2588\u258e      | 521/1600 [15:28<34:16,  1.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 48546])\n",
            "   \ud83d\udd04 Processing batch 521: torch.Size([1, 48546]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  33%|\u2588\u2588\u2588\u258e      | 522/1600 [15:29<26:47,  1.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 67054])\n",
            "   \ud83d\udd04 Processing batch 522: torch.Size([1, 67054]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  33%|\u2588\u2588\u2588\u258e      | 523/1600 [15:29<22:05,  1.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 86409])\n",
            "   \ud83d\udd04 Processing batch 523: torch.Size([1, 86409]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  33%|\u2588\u2588\u2588\u258e      | 524/1600 [15:30<21:18,  1.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 235881])\n",
            "   \ud83d\udd04 Processing batch 524: torch.Size([1, 235881]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  33%|\u2588\u2588\u2588\u258e      | 525/1600 [15:32<25:09,  1.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 333750])\n",
            "   \ud83d\udd04 Processing batch 525: torch.Size([1, 333750]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  33%|\u2588\u2588\u2588\u258e      | 526/1600 [15:35<33:06,  1.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 158097])\n",
            "   \ud83d\udd04 Processing batch 526: torch.Size([1, 158097]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  33%|\u2588\u2588\u2588\u258e      | 527/1600 [15:36<28:35,  1.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 158154])\n",
            "   \ud83d\udd04 Processing batch 527: torch.Size([1, 158154]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  33%|\u2588\u2588\u2588\u258e      | 528/1600 [15:37<23:46,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 108030])\n",
            "   \ud83d\udd04 Processing batch 528: torch.Size([1, 108030]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  33%|\u2588\u2588\u2588\u258e      | 529/1600 [15:38<19:58,  1.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 342817])\n",
            "   \ud83d\udd04 Processing batch 529: torch.Size([1, 342817]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  33%|\u2588\u2588\u2588\u258e      | 530/1600 [15:41<30:32,  1.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 231777])\n",
            "   \ud83d\udd04 Processing batch 530: torch.Size([1, 231777]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  33%|\u2588\u2588\u2588\u258e      | 531/1600 [15:50<1:12:55,  4.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 162073])\n",
            "   \ud83d\udd04 Processing batch 531: torch.Size([1, 162073]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  33%|\u2588\u2588\u2588\u258e      | 532/1600 [15:51<53:27,  3.00s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 168906])\n",
            "   \ud83d\udd04 Processing batch 532: torch.Size([1, 168906]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  33%|\u2588\u2588\u2588\u258e      | 533/1600 [15:52<45:14,  2.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 101391])\n",
            "   \ud83d\udd04 Processing batch 533: torch.Size([1, 101391]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  33%|\u2588\u2588\u2588\u258e      | 534/1600 [15:53<37:40,  2.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 159109])\n",
            "   \ud83d\udd04 Processing batch 534: torch.Size([1, 159109]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  33%|\u2588\u2588\u2588\u258e      | 535/1600 [15:55<35:08,  1.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 227999])\n",
            "   \ud83d\udd04 Processing batch 535: torch.Size([1, 227999]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  34%|\u2588\u2588\u2588\u258e      | 536/1600 [15:58<38:10,  2.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 113075])\n",
            "   \ud83d\udd04 Processing batch 536: torch.Size([1, 113075]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  34%|\u2588\u2588\u2588\u258e      | 537/1600 [15:58<29:58,  1.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 193591])\n",
            "   \ud83d\udd04 Processing batch 537: torch.Size([1, 193591]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  34%|\u2588\u2588\u2588\u258e      | 538/1600 [16:08<1:12:32,  4.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 64799])\n",
            "   \ud83d\udd04 Processing batch 538: torch.Size([1, 64799]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  34%|\u2588\u2588\u2588\u258e      | 539/1600 [16:09<54:12,  3.07s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 33043])\n",
            "   \ud83d\udd04 Processing batch 539: torch.Size([1, 33043]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  34%|\u2588\u2588\u2588\u258d      | 540/1600 [16:09<40:16,  2.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 119442])\n",
            "   \ud83d\udd04 Processing batch 540: torch.Size([1, 119442]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  34%|\u2588\u2588\u2588\u258d      | 541/1600 [16:28<2:10:39,  7.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 190275])\n",
            "   \ud83d\udd04 Processing batch 541: torch.Size([1, 190275]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  34%|\u2588\u2588\u2588\u258d      | 542/1600 [16:30<1:38:56,  5.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 104209])\n",
            "   \ud83d\udd04 Processing batch 542: torch.Size([1, 104209]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  34%|\u2588\u2588\u2588\u258d      | 543/1600 [16:31<1:13:46,  4.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 245050])\n",
            "   \ud83d\udd04 Processing batch 543: torch.Size([1, 245050]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  34%|\u2588\u2588\u2588\u258d      | 544/1600 [16:32<1:00:36,  3.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 213483])\n",
            "   \ud83d\udd04 Processing batch 544: torch.Size([1, 213483]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  34%|\u2588\u2588\u2588\u258d      | 545/1600 [16:34<51:39,  2.94s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 43377])\n",
            "   \ud83d\udd04 Processing batch 545: torch.Size([1, 43377]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  34%|\u2588\u2588\u2588\u258d      | 546/1600 [16:35<39:22,  2.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 164789])\n",
            "   \ud83d\udd04 Processing batch 546: torch.Size([1, 164789]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  34%|\u2588\u2588\u2588\u258d      | 547/1600 [16:45<1:21:26,  4.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 264872])\n",
            "   \ud83d\udd04 Processing batch 547: torch.Size([1, 264872]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  34%|\u2588\u2588\u2588\u258d      | 548/1600 [16:47<1:09:30,  3.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 155755])\n",
            "   \ud83d\udd04 Processing batch 548: torch.Size([1, 155755]) with 1 IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEgypt:  34%|\u2588\u2588\u2588\u258d      | 549/1600 [16:49<55:54,  3.19s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    \ud83d\udd0d Processing batch with 1 samples\n",
            "    \u2705 Batch processed: torch.Size([1, 194931])\n",
            "   \ud83d\udd04 Processing batch 549: torch.Size([1, 194931]) with 1 IDs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoProcessor\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"nvidia/audio-flamingo-3\", torch_dtype=torch.float16).to(\"cuda\")\n",
        "processor = AutoProcessor.from_pretrained(\"nvidia/audio-flamingo-3\")\n",
        "\n",
        "def transcribe(files):\n",
        "    inputs = processor(text=\"Transcribe in Arabic\", audios=files, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=512)\n",
        "    return processor.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "9vFkJ1PwraJC",
        "executionInfo": {
          "status": "error",
          "timestamp": 1753801236248,
          "user_tz": -180,
          "elapsed": 10771,
          "user": {
            "displayName": "MD.RAFIUL BISWAS",
            "userId": "15801257354586832441"
          }
        },
        "outputId": "1f7e3a2e-92c7-4b94-8dc6-ad7783960458"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The checkpoint you are trying to load has model type `llava_llama` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1265\u001b[0;31m                 \u001b[0mconfig_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1266\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'llava_llama'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-2915724368.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nvidia/audio-flamingo-3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nvidia/audio-flamingo-3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"quantization_config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    548\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m                 \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1265\u001b[0m                 \u001b[0mconfig_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                     \u001b[0;34mf\"The checkpoint you are trying to load has model type `{config_dict['model_type']}` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m                     \u001b[0;34m\"but Transformers does not recognize this architecture. This could be because of an \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The checkpoint you are trying to load has model type `llava_llama` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y transformers\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "-Omsc6FprmgZ",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1753801181032,
          "user_tz": -180,
          "elapsed": 31784,
          "user": {
            "displayName": "MD.RAFIUL BISWAS",
            "userId": "15801257354586832441"
          }
        },
        "outputId": "778489d0-90f1-4bef-ea66-665ad740ac7f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.54.0\n",
            "Uninstalling transformers-4.54.0:\n",
            "  Successfully uninstalled transformers-4.54.0\n",
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-0lqosaly\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-0lqosaly\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 1ad216bd7dced23e7e38ab4881a9528486588bd0\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.55.0.dev0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.55.0.dev0) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.55.0.dev0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.55.0.dev0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.55.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.55.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.55.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.55.0.dev0) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.55.0.dev0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.55.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.55.0.dev0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.55.0.dev0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.55.0.dev0) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.55.0.dev0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.55.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.55.0.dev0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.55.0.dev0) (2025.7.14)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.55.0.dev0-py3-none-any.whl size=12116892 sha256=2d7224619b45b4f90263213f50f5266d64d7e45590ee95ecc0f8709780d96f2c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-brxiielz/wheels/32/4b/78/f195c684dd3a9ed21f3b39fe8f85b48df7918581b6437be143\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.55.0.dev0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "2d7d813991094fd79eca8382d18d07ef"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "id": "2LWaMwytscnH",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1753802741131,
          "user_tz": -180,
          "elapsed": 36123,
          "user": {
            "displayName": "MD.RAFIUL BISWAS",
            "userId": "15801257354586832441"
          }
        },
        "outputId": "d7904b74-5905-46f1-f596-3d2fd4270088"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-2pl7tiva\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-2pl7tiva\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit c4e20698985887215f7e91a02621265f047af2d7\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.55.0.dev0) (3.18.0)\n",
            "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers==4.55.0.dev0)\n",
            "  Downloading huggingface_hub-0.34.3-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.55.0.dev0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.55.0.dev0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.55.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.55.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.55.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.55.0.dev0) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.55.0.dev0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.55.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.55.0.dev0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.55.0.dev0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.55.0.dev0) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.55.0.dev0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.55.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.55.0.dev0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.55.0.dev0) (2025.7.14)\n",
            "Downloading huggingface_hub-0.34.3-py3-none-any.whl (558 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.55.0.dev0-py3-none-any.whl size=12117010 sha256=e0162cc1394a23262750abc54556b121e1400a2c7dfc3f507ccfa175a7d13e26\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-y4jcojxn/wheels/32/4b/78/f195c684dd3a9ed21f3b39fe8f85b48df7918581b6437be143\n",
            "Successfully built transformers\n",
            "Installing collected packages: huggingface-hub, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.33.5\n",
            "    Uninstalling huggingface-hub-0.33.5:\n",
            "      Successfully uninstalled huggingface-hub-0.33.5\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.53.3\n",
            "    Uninstalling transformers-4.53.3:\n",
            "      Successfully uninstalled transformers-4.53.3\n",
            "Successfully installed huggingface-hub-0.34.3 transformers-4.55.0.dev0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "huggingface_hub",
                  "transformers"
                ]
              },
              "id": "74245e39bad34bbebb15998bbfc2a83d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torch torchvision accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37o9KOnDyjWU",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1753802762622,
          "user_tz": -180,
          "elapsed": 4749,
          "user": {
            "displayName": "MD.RAFIUL BISWAS",
            "userId": "15801257354586832441"
          }
        },
        "outputId": "5d85ec7a-366c-48e9-e736-8966b66581b2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.7.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.22.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.1)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.34.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoProcessor\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"nvidia/audio-flamingo-3\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16\n",
        ").to(device)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"nvidia/audio-flamingo-3\", trust_remote_code=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "uHkXTWHMuwcR",
        "executionInfo": {
          "status": "error",
          "timestamp": 1753802781680,
          "user_tz": -180,
          "elapsed": 11207,
          "user": {
            "displayName": "MD.RAFIUL BISWAS",
            "userId": "15801257354586832441"
          }
        },
        "outputId": "30c329bf-a813-48f8-948b-efafcedbea53"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The checkpoint you are trying to load has model type `llava_llama` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1265\u001b[0;31m                 \u001b[0mconfig_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1266\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'llava_llama'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-661117059.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"nvidia/audio-flamingo-3\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"quantization_config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    548\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m                 \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1265\u001b[0m                 \u001b[0mconfig_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                     \u001b[0;34mf\"The checkpoint you are trying to load has model type `{config_dict['model_type']}` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m                     \u001b[0;34m\"but Transformers does not recognize this architecture. This could be because of an \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The checkpoint you are trying to load has model type `llava_llama` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torch torchvision accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Lf1B00IvOcv",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1753802585148,
          "user_tz": -180,
          "elapsed": 163563,
          "user": {
            "displayName": "MD.RAFIUL BISWAS",
            "userId": "15801257354586832441"
          }
        },
        "outputId": "543ae502-72d2-4375-a189-50f23e1c1854"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting torch\n",
            "  Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n",
            "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.3.1 (from torch)\n",
            "  Downloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.33.5)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.7.14)\n",
            "Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (821.2 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m117.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl (7.5 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m112.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.1 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 sympy-1.14.0 torch-2.7.1 torchvision-0.22.1 triton-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-dWM3lfTxTNc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
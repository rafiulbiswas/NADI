{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNAqfYp4VkWkmPN9o+FU9hO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u7a84K65ruw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install git+https://github.com/speechbrain/speechbrain.git@develop\n",
        "!pip install datasets==3.5.0"
      ],
      "metadata": {
        "id": "ZQNJ7WQzvM4B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1753616265860,
          "user_tz": -180,
          "elapsed": 94512,
          "user": {
            "displayName": "MD.RAFIUL BISWAS",
            "userId": "15801257354586832441"
          }
        },
        "outputId": "f0b30a2a-8899-4484-9077-56381bef5874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/speechbrain/speechbrain.git@develop\n",
            "  Cloning https://github.com/speechbrain/speechbrain.git (to revision develop) to /tmp/pip-req-build-4hgrm5v2\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/speechbrain/speechbrain.git /tmp/pip-req-build-4hgrm5v2\n",
            "  Resolved https://github.com/speechbrain/speechbrain.git to commit ec1425368dd3fc9dd41edc7b50a9148cd463abec\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hyperpyyaml (from speechbrain==1.0.3)\n",
            "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from speechbrain==1.0.3) (1.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from speechbrain==1.0.3) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from speechbrain==1.0.3) (25.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from speechbrain==1.0.3) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from speechbrain==1.0.3) (0.2.0)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.11/dist-packages (from speechbrain==1.0.3) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (from speechbrain==1.0.3) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from speechbrain==1.0.3) (4.67.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from speechbrain==1.0.3) (0.33.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain==1.0.3) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain==1.0.3) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain==1.0.3) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain==1.0.3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain==1.0.3) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.9->speechbrain==1.0.3)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.9->speechbrain==1.0.3)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.9->speechbrain==1.0.3)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.9->speechbrain==1.0.3)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.9->speechbrain==1.0.3)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.9->speechbrain==1.0.3)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.9->speechbrain==1.0.3)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.9->speechbrain==1.0.3)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.9->speechbrain==1.0.3)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain==1.0.3) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain==1.0.3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain==1.0.3) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.9->speechbrain==1.0.3)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain==1.0.3) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain==1.0.3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9->speechbrain==1.0.3) (1.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->speechbrain==1.0.3) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->speechbrain==1.0.3) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->speechbrain==1.0.3) (1.1.5)\n",
            "Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain==1.0.3)\n",
            "  Downloading ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain==1.0.3)\n",
            "  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9->speechbrain==1.0.3) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->speechbrain==1.0.3) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->speechbrain==1.0.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->speechbrain==1.0.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->speechbrain==1.0.3) (2025.7.14)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m131.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
            "Downloading ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: speechbrain\n",
            "  Building wheel for speechbrain (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for speechbrain: filename=speechbrain-1.0.3-py3-none-any.whl size=766407 sha256=09a9b8063a5e24ebc1b52acf9cc326169c478917b4045f060bca2e1c0d3da88d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-orl1wr7s/wheels/cc/6d/c4/338bedb17934b628dbf16a28c019cf0143822325c4bdf2afda\n",
            "Successfully built speechbrain\n",
            "Installing collected packages: ruamel.yaml.clib, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ruamel.yaml, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, hyperpyyaml, speechbrain\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed hyperpyyaml-1.2.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ruamel.yaml-0.18.14 ruamel.yaml.clib-0.2.12 speechbrain-1.0.3\n",
            "Collecting datasets==3.5.0\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (0.70.16)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets==3.5.0)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (3.12.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (0.33.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.0) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.0) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets==3.5.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets==3.5.0) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.5.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.5.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.5.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.5.0) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.5.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.5.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.5.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.5.0) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 fsspec-2024.12.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "datasets",
                  "fsspec"
                ]
              },
              "id": "9feb5c2809ae41c4afbb3abdea30446c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, LinearLR, SequentialLR\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, LinearLR, SequentialLR\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from datasets import load_dataset\n",
        "from speechbrain.inference.classifiers import EncoderClassifier\n",
        "from speechbrain.dataio.encoder import CategoricalEncoder\n",
        "from speechbrain.nnet.linear import Linear\n",
        "from speechbrain.nnet.normalization import BatchNorm1d\n",
        "from speechbrain.nnet.activations import Swish\n",
        "from tqdm.notebook import tqdm\n",
        "import logging\n",
        "import zipfile\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n"
      ],
      "metadata": {
        "id": "08sOUgAFCIVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete Enhanced ECAPA-TDNN for Arabic Dialect Identification\n",
        "# Fixed version with all imports and proper structure\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, LinearLR, SequentialLR\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import logging\n",
        "import zipfile\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from datasets import load_dataset\n",
        "from speechbrain.inference.classifiers import EncoderClassifier\n",
        "from speechbrain.dataio.encoder import CategoricalEncoder\n",
        "from speechbrain.nnet.linear import Linear\n",
        "from speechbrain.nnet.normalization import BatchNorm1d\n",
        "from speechbrain.nnet.activations import Swish\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class Config:\n",
        "    countries = ['Algeria', 'Egypt', 'Jordan', 'Mauritania', 'Morocco', 'Palestine', 'UAE', 'Yemen']\n",
        "    num_classes = len(countries)\n",
        "    embedding_dim = 512\n",
        "    hidden_dims = [256, 128]  # Multi-layer classification head\n",
        "    dropout_rate = 0.3\n",
        "    batch_size = 8  # Increased batch size\n",
        "    learning_rate = 5e-5  # Lower learning rate for fine-tuning\n",
        "    max_steps = 25000\n",
        "    warmup_steps = 2500\n",
        "    eval_steps = 2000\n",
        "    logging_steps = 500\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Enhanced Classification Head\n",
        "class EnhancedClassificationHead(nn.Module):\n",
        "    \"\"\"Enhanced multi-layer classification head with self-attention and residual connections\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dims, num_classes, dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Multi-layer MLP with residual connections\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        for i, hidden_dim in enumerate(hidden_dims):\n",
        "            layers.extend([\n",
        "                Linear(n_neurons=hidden_dim, input_size=prev_dim, bias=True),\n",
        "                BatchNorm1d(input_size=hidden_dim),\n",
        "                Swish(),\n",
        "                nn.Dropout(dropout_rate)\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        self.feature_layers = nn.Sequential(*layers)\n",
        "\n",
        "        # Self-attention mechanism for feature refinement\n",
        "        final_dim = hidden_dims[-1] if hidden_dims else input_dim\n",
        "        self.feature_attention = nn.Sequential(\n",
        "            nn.Linear(final_dim, final_dim // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(final_dim // 4, final_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Final classification layer\n",
        "        self.classifier = Linear(n_neurons=num_classes, input_size=final_dim, bias=True)\n",
        "\n",
        "        # Residual connection if dimensions match\n",
        "        self.use_residual = input_dim == final_dim\n",
        "        if not self.use_residual and len(hidden_dims) > 0:\n",
        "            self.residual_proj = Linear(n_neurons=final_dim, input_size=input_dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, embedding_dim)\n",
        "        residual = x\n",
        "\n",
        "        # Pass through feature layers\n",
        "        if len(self.feature_layers) > 0:\n",
        "            features = self.feature_layers(x)\n",
        "        else:\n",
        "            features = x\n",
        "\n",
        "        # Apply self-attention for feature importance weighting\n",
        "        attention_weights = self.feature_attention(features)\n",
        "        features = features * attention_weights\n",
        "\n",
        "        # Residual connection\n",
        "        if self.use_residual:\n",
        "            features = features + residual\n",
        "        elif hasattr(self, 'residual_proj'):\n",
        "            features = features + self.residual_proj(residual)\n",
        "\n",
        "        # Final classification\n",
        "        logits = self.classifier(features)\n",
        "        return logits\n",
        "\n",
        "# Data Loading and Preprocessing\n",
        "def load_datasets():\n",
        "    \"\"\"Load train, validation, and test datasets\"\"\"\n",
        "    logger.info(\"Loading datasets...\")\n",
        "\n",
        "    # Load train/val dataset\n",
        "    ds_train_val = load_dataset(\"UBC-NLP/NADI2025_subtask1_SLID\", token=\"hf_token\")\n",
        "    ds_train_val.set_format(\"torch\")\n",
        "\n",
        "    # Load test dataset\n",
        "    ds_test = load_dataset(\"UBC-NLP/NADI2025_subtask1_ADI_Test\", token=\"hf_token\")\n",
        "    ds_test.set_format(\"torch\")\n",
        "\n",
        "    return ds_train_val, ds_test\n",
        "\n",
        "# Enhanced collate function with test support\n",
        "def enhanced_collate_fn(samples, augment=False, is_test=False):\n",
        "    \"\"\"Enhanced collate function with optional augmentation and test mode support\"\"\"\n",
        "    arrays = [sample['audio']['array'] for sample in samples]\n",
        "    lengths = list(map(len, arrays))\n",
        "\n",
        "    # Convert to tensors\n",
        "    arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
        "\n",
        "    # Optional audio augmentation during training\n",
        "    if augment:\n",
        "        arrays = [add_noise(arr) for arr in arrays]\n",
        "\n",
        "    # Pad sequences\n",
        "    packed = pad_sequence(arrays, batch_first=False)  # (max_len, batch_size)\n",
        "\n",
        "    # Handle test set without 'country' labels\n",
        "    if not is_test:\n",
        "        # For train/val: extract real country labels\n",
        "        countries = torch.tensor([config.countries.index(sample['country']) for sample in samples])\n",
        "    else:\n",
        "        # For test: create dummy labels (won't be used for loss)\n",
        "        countries = torch.zeros(len(samples), dtype=torch.long)\n",
        "\n",
        "    return packed, lengths, countries\n",
        "\n",
        "def add_noise(audio, noise_factor=0.005):\n",
        "    \"\"\"Add small amount of gaussian noise for augmentation\"\"\"\n",
        "    noise = torch.randn_like(audio) * noise_factor\n",
        "    return audio + noise\n",
        "\n",
        "# Enhanced model setup\n",
        "def setup_enhanced_model():\n",
        "    \"\"\"Setup model with enhanced classification head\"\"\"\n",
        "    logger.info(\"Setting up enhanced ECAPA-TDNN model...\")\n",
        "\n",
        "    # Load pretrained model\n",
        "    model = EncoderClassifier.from_hparams(\n",
        "        source=\"speechbrain/lang-id-voxlingua107-ecapa\",\n",
        "        savedir=\"tmp\"\n",
        "    )\n",
        "\n",
        "    # Replace classification head with enhanced version\n",
        "    enhanced_head = EnhancedClassificationHead(\n",
        "        input_dim=config.embedding_dim,\n",
        "        hidden_dims=config.hidden_dims,\n",
        "        num_classes=config.num_classes,\n",
        "        dropout_rate=config.dropout_rate\n",
        "    )\n",
        "\n",
        "    model.get_submodule('mods.classifier')['out'] = enhanced_head\n",
        "\n",
        "    # Update label encoder\n",
        "    encoder = CategoricalEncoder()\n",
        "    encoder.update_from_iterable(config.countries)\n",
        "    model.hparams.label_encoder = encoder\n",
        "\n",
        "    return model\n",
        "\n",
        "# Training utilities\n",
        "def freeze_backbone(model):\n",
        "    \"\"\"Freeze all parameters except classification head\"\"\"\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'classifier' not in name:\n",
        "            param.requires_grad = False\n",
        "        else:\n",
        "            param.requires_grad = True\n",
        "\n",
        "def unfreeze_top_layers(model, num_layers=2):\n",
        "    \"\"\"Unfreeze top layers of the backbone for fine-tuning\"\"\"\n",
        "    # This is model-specific - adjust based on ECAPA-TDNN architecture\n",
        "    for name, param in model.named_parameters():\n",
        "        if any(layer in name for layer in ['tdnn5', 'tdnn6', 'classifier']):\n",
        "            param.requires_grad = True\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"Print number of trainable parameters\"\"\"\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    all_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "    logger.info(f\"Trainable params: {trainable_params:,} || \"\n",
        "                f\"All params: {all_params:,} || \"\n",
        "                f\"Trainable%: {100 * trainable_params / all_params:.2f}\")\n",
        "\n",
        "    return trainable_params, all_params\n",
        "\n",
        "# Enhanced evaluation with detailed metrics\n",
        "def enhanced_eval_loop(model, loader, device):\n",
        "    \"\"\"Enhanced evaluation with detailed metrics\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_logits = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
        "            wavs, lengths, labels = batch\n",
        "            wavs = wavs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(wavs.transpose(1, 0))\n",
        "            logits = outputs[0]  # Raw logits\n",
        "            preds = outputs[2]   # Predictions\n",
        "\n",
        "            # Store results\n",
        "            all_logits.append(logits.cpu())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds) * 100\n",
        "\n",
        "    # Calculate average cost (your original metric)\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "    all_labels_tensor = torch.tensor(all_labels)\n",
        "    cost, fpr, fnr = compute_ave_cost(all_logits, all_labels_tensor)\n",
        "\n",
        "    # Detailed classification report\n",
        "    report = classification_report(\n",
        "        all_labels, all_preds,\n",
        "        target_names=config.countries,\n",
        "        output_dict=True\n",
        "    )\n",
        "\n",
        "    model.train()\n",
        "    return accuracy, cost, fpr, fnr, report\n",
        "\n",
        "# Your original cost computation functions (kept unchanged)\n",
        "def llr(logits):\n",
        "    classes = logits.shape[1]\n",
        "    l2 = logits.unsqueeze(dim=1)\n",
        "    l = logits.unsqueeze(dim=1)\n",
        "    l = l.repeat(1, 8, 1)\n",
        "    l2 = l2.repeat(1, 8, 1)\n",
        "    l2 = l2.permute((0, 2, 1))\n",
        "    dif = l - l2\n",
        "    e = torch.exp(dif)\n",
        "    for i in range(len(e)):\n",
        "        e[i].fill_diagonal_(0)\n",
        "    return -torch.log(torch.sum(e, dim=-1) / (classes - 1))\n",
        "\n",
        "def compute_actual_cost(scores, labels, p_target, c_miss=1, c_fa=1):\n",
        "    beta = c_fa * (1 - p_target) / (c_miss * p_target)\n",
        "    decisions = (scores >= np.log(beta)).astype('i')\n",
        "    num_targets = np.sum(labels)\n",
        "    fp = np.sum(decisions * (1 - labels))\n",
        "    num_nontargets = np.sum(1 - labels)\n",
        "    fn = np.sum((1 - decisions) * labels)\n",
        "    fpr = fp / num_nontargets if num_nontargets > 0 else np.nan\n",
        "    fnr = fn / num_targets if num_targets > 0 else np.nan\n",
        "    return fnr + beta * fpr, fpr, fnr\n",
        "\n",
        "def compute_ave_cost(logits, labels, num_l=8):\n",
        "    llratio = llr(logits)\n",
        "    llratio = llratio.numpy()\n",
        "    labels = labels.numpy()\n",
        "    order = labels.argsort()\n",
        "    labels.sort()\n",
        "    llratio = llratio[order]\n",
        "    indices = np.where(labels[:-1] != labels[1:])[0]\n",
        "    indices = np.append(indices, [-1])\n",
        "    one_hot = np.eye(8)[labels]\n",
        "    fprs = []\n",
        "    fnrs = []\n",
        "    last = 0\n",
        "    for i in indices:\n",
        "        _, fpr, fnr = compute_actual_cost(llratio[last:i+1], one_hot[last:i+1], 0.5)\n",
        "        fprs.append(fpr)\n",
        "        fnrs.append(fnr)\n",
        "        last = i + 1\n",
        "    fpr = sum(fprs) / num_l\n",
        "    fnr = sum(fnrs) / num_l\n",
        "    cost = fpr + fnr\n",
        "    return cost, fpr, fnr\n",
        "\n",
        "# Basic test prediction function\n",
        "def generate_test_predictions(model, test_loader, device):\n",
        "    \"\"\"Generate predictions for test set\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Clear previous files\n",
        "    open('logits.tsv', 'w').close()\n",
        "    open('predictions.tsv', 'w').close()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Generating predictions\"):\n",
        "            wavs, lengths, _ = batch  # Ignore dummy labels for test set\n",
        "            wavs = wavs.to(device)\n",
        "\n",
        "            outputs = model(wavs.transpose(1, 0))\n",
        "            logits = outputs[0].cpu()\n",
        "            predictions = outputs[2].cpu()\n",
        "\n",
        "            # Write logits\n",
        "            with open('logits.tsv', 'a') as f:\n",
        "                for logit_row in logits:\n",
        "                    line = '\\t'.join([f\"{val.item():.6f}\" for val in logit_row])\n",
        "                    f.write(line + '\\n')\n",
        "\n",
        "            # Write predictions\n",
        "            with open('predictions.tsv', 'a') as f:\n",
        "                for pred in predictions:\n",
        "                    f.write(f\"{pred.item()}\\n\")\n",
        "\n",
        "    # Create submission zip\n",
        "    with zipfile.ZipFile('submission.zip', 'w') as zipf:\n",
        "        zipf.write('logits.tsv')\n",
        "        zipf.write('predictions.tsv')\n",
        "\n",
        "    logger.info(\"Test predictions saved to submission.zip\")\n",
        "\n",
        "# Enhancement Functions\n",
        "def test_time_augmentation_predict(model, test_loader, device, num_augmentations=5):\n",
        "    \"\"\"Apply test time augmentation for more robust predictions\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_logits_tta = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"TTA Predictions\"):\n",
        "            wavs, lengths, _ = batch  # Test loader returns 3 items\n",
        "            batch_logits = []\n",
        "\n",
        "            for aug_idx in range(num_augmentations):\n",
        "                # Apply different augmentations\n",
        "                augmented_wavs = wavs.clone()\n",
        "\n",
        "                if aug_idx == 0:\n",
        "                    # Original (no augmentation)\n",
        "                    pass\n",
        "                elif aug_idx == 1:\n",
        "                    # Add slight noise\n",
        "                    noise = torch.randn_like(augmented_wavs) * 0.01\n",
        "                    augmented_wavs = augmented_wavs + noise\n",
        "                elif aug_idx == 2:\n",
        "                    # Volume change (quieter)\n",
        "                    augmented_wavs = augmented_wavs * 0.9\n",
        "                elif aug_idx == 3:\n",
        "                    # Volume change (louder)\n",
        "                    augmented_wavs = augmented_wavs * 1.1\n",
        "                elif aug_idx == 4:\n",
        "                    # Slight noise + volume\n",
        "                    noise = torch.randn_like(augmented_wavs) * 0.005\n",
        "                    augmented_wavs = (augmented_wavs + noise) * 0.95\n",
        "\n",
        "                augmented_wavs = augmented_wavs.to(device)\n",
        "                outputs = model(augmented_wavs.transpose(1, 0))\n",
        "                batch_logits.append(outputs[0].cpu())\n",
        "\n",
        "            # Average predictions across augmentations\n",
        "            avg_logits = torch.stack(batch_logits).mean(dim=0)\n",
        "            all_logits_tta.append(avg_logits)\n",
        "\n",
        "    return torch.cat(all_logits_tta, dim=0)\n",
        "\n",
        "def generate_enhanced_predictions(model, test_loader, device, output_dir=\"./\", use_tta=True, num_augmentations=5):\n",
        "    \"\"\"Generate predictions with optional TTA\"\"\"\n",
        "\n",
        "    if use_tta:\n",
        "        print(f\"Using Test Time Augmentation with {num_augmentations} augmentations...\")\n",
        "        all_logits = test_time_augmentation_predict(model, test_loader, device, num_augmentations)\n",
        "    else:\n",
        "        print(\"Standard prediction...\")\n",
        "        model.eval()\n",
        "        all_logits = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(test_loader, desc=\"Generating predictions\"):\n",
        "                wavs, lengths, _ = batch\n",
        "                wavs = wavs.to(device)\n",
        "\n",
        "                outputs = model(wavs.transpose(1, 0))\n",
        "                logits = outputs[0].cpu()\n",
        "                all_logits.append(logits)\n",
        "\n",
        "        all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "    # Generate predictions from logits\n",
        "    predictions = torch.argmax(all_logits, dim=1)\n",
        "\n",
        "    # Save files\n",
        "    logits_file = os.path.join(output_dir, \"logits.tsv\")\n",
        "    preds_file = os.path.join(output_dir, \"predictions.tsv\")\n",
        "\n",
        "    with open(logits_file, 'w') as f_logits, open(preds_file, 'w') as f_preds:\n",
        "        for logit_row, pred in zip(all_logits, predictions):\n",
        "            # Write logits\n",
        "            logit_line = '\\t'.join([f\"{val:.6f}\" for val in logit_row]) + '\\n'\n",
        "            f_logits.write(logit_line)\n",
        "\n",
        "            # Write predictions\n",
        "            f_preds.write(f\"{pred.item()}\\n\")\n",
        "\n",
        "    print(f\"Enhanced predictions saved to {logits_file} and {preds_file}\")\n",
        "    print(f\"Total predictions: {len(predictions)}\")\n",
        "    return logits_file, preds_file\n",
        "\n",
        "def save_current_model(model, accuracy, save_name=\"current_best\"):\n",
        "    \"\"\"Save the current model with accuracy info\"\"\"\n",
        "    save_path = f\"{save_name}_{accuracy:.2f}percent.pth\"\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'countries': config.countries,\n",
        "        'accuracy': accuracy,\n",
        "        'config': config\n",
        "    }, save_path)\n",
        "    print(f\"\u2705 Model saved: {save_path}\")\n",
        "    return save_path\n",
        "\n",
        "# Enhanced training loop\n",
        "def train_model():\n",
        "    \"\"\"Main training function\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    logger.info(f\"Using device: {device}\")\n",
        "\n",
        "    # Load datasets\n",
        "    ds_train_val, ds_test = load_datasets()\n",
        "\n",
        "    # Setup model\n",
        "    model = setup_enhanced_model()\n",
        "    model = model.to(device)\n",
        "    model.device = device\n",
        "\n",
        "    # Initially freeze backbone\n",
        "    freeze_backbone(model)\n",
        "    print_trainable_parameters(model)\n",
        "\n",
        "    # Setup data loaders with proper is_test parameter\n",
        "    train_loader = DataLoader(\n",
        "        ds_train_val['train'],\n",
        "        shuffle=True,\n",
        "        collate_fn=lambda x: enhanced_collate_fn(x, augment=True, is_test=False),\n",
        "        batch_size=config.batch_size,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        ds_train_val['validation'],\n",
        "        shuffle=False,\n",
        "        collate_fn=lambda x: enhanced_collate_fn(x, augment=False, is_test=False),\n",
        "        batch_size=config.batch_size,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        ds_test['test'],\n",
        "        shuffle=False,\n",
        "        collate_fn=lambda x: enhanced_collate_fn(x, augment=False, is_test=True),\n",
        "        batch_size=config.batch_size,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    # Setup optimizer and scheduler\n",
        "    optimizer = AdamW(\n",
        "        filter(lambda p: p.requires_grad, model.parameters()),\n",
        "        lr=config.learning_rate,\n",
        "        weight_decay=0.01\n",
        "    )\n",
        "\n",
        "    # Enhanced learning rate scheduling\n",
        "    warmup_scheduler = LinearLR(\n",
        "        optimizer,\n",
        "        start_factor=0.1,\n",
        "        end_factor=1.0,\n",
        "        total_iters=config.warmup_steps\n",
        "    )\n",
        "\n",
        "    cosine_scheduler = CosineAnnealingWarmRestarts(\n",
        "        optimizer,\n",
        "        T_0=config.max_steps - config.warmup_steps,\n",
        "        T_mult=1,\n",
        "        eta_min=1e-7\n",
        "    )\n",
        "\n",
        "    scheduler = SequentialLR(\n",
        "        optimizer,\n",
        "        [warmup_scheduler, cosine_scheduler],\n",
        "        milestones=[config.warmup_steps]\n",
        "    )\n",
        "\n",
        "    # Loss function with label smoothing\n",
        "    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    step = 0\n",
        "    best_accuracy = 0\n",
        "    running_loss = 0\n",
        "\n",
        "    logger.info(\"Starting training...\")\n",
        "\n",
        "    while step < config.max_steps:\n",
        "        for batch in train_loader:\n",
        "            if step >= config.max_steps:\n",
        "                break\n",
        "\n",
        "            wavs, lengths, labels = batch\n",
        "            wavs = wavs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(wavs.transpose(1, 0))\n",
        "            logits = outputs[0]\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_fn(logits, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            step += 1\n",
        "\n",
        "            # Logging\n",
        "            if step % config.logging_steps == 0:\n",
        "                avg_loss = running_loss / config.logging_steps\n",
        "                current_lr = scheduler.get_last_lr()[0]\n",
        "                logger.info(f\"Step {step}: Loss {avg_loss:.4f}, LR {current_lr:.2e}\")\n",
        "                running_loss = 0\n",
        "\n",
        "            # Evaluation\n",
        "            if step % config.eval_steps == 0:\n",
        "                accuracy, cost, fpr, fnr, report = enhanced_eval_loop(model, val_loader, device)\n",
        "                logger.info(f\"Step {step} - Accuracy: {accuracy:.2f}%, \"\n",
        "                           f\"AvgCost: {cost:.4f}, FPR: {fpr:.4f}, FNR: {fnr:.4f}\")\n",
        "\n",
        "                # Save best model\n",
        "                if accuracy > best_accuracy:\n",
        "                    best_accuracy = accuracy\n",
        "                    torch.save(model.state_dict(), 'best_model.pth')\n",
        "                    logger.info(f\"New best model saved! Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "            # Unfreeze top layers after warmup\n",
        "            if step == config.warmup_steps:\n",
        "                logger.info(\"Unfreezing top layers...\")\n",
        "                unfreeze_top_layers(model)\n",
        "                print_trainable_parameters(model)\n",
        "\n",
        "    # Final evaluation\n",
        "    logger.info(\"Final evaluation on validation set...\")\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    accuracy, cost, fpr, fnr, report = enhanced_eval_loop(model, val_loader, device)\n",
        "    logger.info(f\"Final Validation - Accuracy: {accuracy:.2f}%, \"\n",
        "                f\"AvgCost: {cost:.4f}, FPR: {fpr:.4f}, FNR: {fnr:.4f}\")\n",
        "\n",
        "    # Generate predictions on test set\n",
        "    logger.info(\"Generating predictions on test set...\")\n",
        "    generate_test_predictions(model, test_loader, device)\n",
        "\n",
        "    # FIXED: Return model and data loaders for enhancements\n",
        "    return model, train_loader, val_loader, test_loader, device"
      ],
      "metadata": {
        "id": "Ppy8lWuDQssd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ADVANCED ACCURACY BOOST - TARGET 97% PERFORMANCE\n",
        "# Run this cell AFTER training your base model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# 1. FOCAL LOSS FOR HANDLING CLASS IMBALANCE\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2.5, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss\n",
        "\n",
        "# 2. ADVANCED DATA AUGMENTATION FOR 97% ACCURACY\n",
        "class AdvancedAudioAugmentation:\n",
        "    def __init__(self):\n",
        "        self.augmentations = [\n",
        "            self.add_noise,\n",
        "            self.change_volume,\n",
        "            self.time_stretch,\n",
        "            self.pitch_shift_simulate,\n",
        "            self.add_reverb_simulate,\n",
        "            self.frequency_mask,\n",
        "            self.time_mask\n",
        "        ]\n",
        "\n",
        "    def add_noise(self, audio, prob=0.3):\n",
        "        if torch.rand(1) < prob:\n",
        "            noise_level = torch.rand(1) * 0.02 + 0.005  # 0.005-0.025\n",
        "            noise = torch.randn_like(audio) * noise_level\n",
        "            return audio + noise\n",
        "        return audio\n",
        "\n",
        "    def change_volume(self, audio, prob=0.4):\n",
        "        if torch.rand(1) < prob:\n",
        "            volume_factor = torch.rand(1) * 0.6 + 0.7  # 0.7-1.3\n",
        "            return audio * volume_factor\n",
        "        return audio\n",
        "\n",
        "    def time_stretch(self, audio, prob=0.2):\n",
        "        if torch.rand(1) < prob:\n",
        "            stretch_factor = torch.rand(1) * 0.2 + 0.9  # 0.9-1.1\n",
        "            new_length = int(len(audio) * stretch_factor)\n",
        "            if new_length > 0:\n",
        "                indices = torch.linspace(0, len(audio)-1, new_length)\n",
        "                return torch.gather(audio, 0, indices.long())\n",
        "        return audio\n",
        "\n",
        "    def pitch_shift_simulate(self, audio, prob=0.15):\n",
        "        # Simple pitch shift simulation through resampling\n",
        "        if torch.rand(1) < prob:\n",
        "            shift_factor = torch.rand(1) * 0.1 + 0.95  # 0.95-1.05\n",
        "            new_length = int(len(audio) * shift_factor)\n",
        "            if new_length > 0:\n",
        "                indices = torch.linspace(0, len(audio)-1, new_length)\n",
        "                return torch.gather(audio, 0, indices.long())\n",
        "        return audio\n",
        "\n",
        "    def add_reverb_simulate(self, audio, prob=0.1):\n",
        "        # Simple reverb simulation with exponential decay\n",
        "        if torch.rand(1) < prob:\n",
        "            decay = torch.rand(1) * 0.3 + 0.1  # 0.1-0.4\n",
        "            reverb_length = min(1000, len(audio) // 10)\n",
        "            reverb = torch.exp(-torch.arange(reverb_length) * decay / 100)\n",
        "            reverb = reverb / reverb.sum()\n",
        "\n",
        "            # Simple convolution (reverb effect)\n",
        "            if len(audio) > reverb_length:\n",
        "                padded = torch.nn.functional.pad(audio, (0, reverb_length-1))\n",
        "                reverbed = torch.nn.functional.conv1d(\n",
        "                    padded.unsqueeze(0).unsqueeze(0),\n",
        "                    reverb.unsqueeze(0).unsqueeze(0)\n",
        "                ).squeeze()\n",
        "                return reverbed[:len(audio)]\n",
        "        return audio\n",
        "\n",
        "    def frequency_mask(self, audio, prob=0.2, max_mask_pct=0.1):\n",
        "        # Simulate frequency masking in time domain\n",
        "        if torch.rand(1) < prob:\n",
        "            mask_length = int(len(audio) * torch.rand(1) * max_mask_pct)\n",
        "            if mask_length > 0:\n",
        "                start = torch.randint(0, len(audio) - mask_length, (1,))\n",
        "                audio_copy = audio.clone()\n",
        "                audio_copy[start:start+mask_length] *= 0.1  # Reduce amplitude\n",
        "                return audio_copy\n",
        "        return audio\n",
        "\n",
        "    def time_mask(self, audio, prob=0.2, max_mask_pct=0.05):\n",
        "        if torch.rand(1) < prob:\n",
        "            mask_length = int(len(audio) * torch.rand(1) * max_mask_pct)\n",
        "            if mask_length > 0:\n",
        "                start = torch.randint(0, len(audio) - mask_length, (1,))\n",
        "                audio_copy = audio.clone()\n",
        "                audio_copy[start:start+mask_length] = 0  # Complete silence\n",
        "                return audio_copy\n",
        "        return audio\n",
        "\n",
        "    def apply_random_augmentation(self, audio, num_augs=1):\n",
        "        \"\"\"Apply random combination of augmentations\"\"\"\n",
        "        for _ in range(num_augs):\n",
        "            aug_fn = torch.randint(0, len(self.augmentations), (1,)).item()\n",
        "            audio = self.augmentations[aug_fn](audio)\n",
        "        return audio\n",
        "\n",
        "# 3. ENHANCED COLLATE FUNCTION WITH ADVANCED AUGMENTATION\n",
        "def create_advanced_collate_fn(training=True, is_test=False):\n",
        "    augmenter = AdvancedAudioAugmentation()\n",
        "\n",
        "    def advanced_collate_fn(samples):\n",
        "        arrays = []\n",
        "        lengths = []\n",
        "        countries_list = []\n",
        "\n",
        "        for sample in samples:\n",
        "            audio_array = sample['audio']['array'].clone()\n",
        "\n",
        "            # Apply advanced augmentation during training\n",
        "            if training and not is_test:\n",
        "                # Apply multiple augmentations with varying probability\n",
        "                num_augs = torch.randint(1, 4, (1,)).item()  # 1-3 augmentations\n",
        "                audio_array = augmenter.apply_random_augmentation(audio_array, num_augs)\n",
        "\n",
        "            arrays.append(audio_array)\n",
        "            lengths.append(len(audio_array))\n",
        "\n",
        "            if not is_test and 'country' in sample:\n",
        "                countries_list.append(config.countries.index(sample['country']))\n",
        "            else:\n",
        "                countries_list.append(0)  # Dummy label for test\n",
        "\n",
        "        packed = pad_sequence(arrays, batch_first=False)\n",
        "        countries_tensor = torch.tensor(countries_list)\n",
        "\n",
        "        return packed, lengths, countries_tensor\n",
        "\n",
        "    return advanced_collate_fn\n",
        "\n",
        "# 4. CURRICULUM LEARNING FOR 97% ACCURACY\n",
        "class CurriculumLearningScheduler:\n",
        "    def __init__(self, easy_epochs=5, medium_epochs=10):\n",
        "        self.easy_epochs = easy_epochs\n",
        "        self.medium_epochs = medium_epochs\n",
        "        self.current_epoch = 0\n",
        "\n",
        "    def get_difficulty_level(self):\n",
        "        if self.current_epoch < self.easy_epochs:\n",
        "            return \"easy\"\n",
        "        elif self.current_epoch < self.easy_epochs + self.medium_epochs:\n",
        "            return \"medium\"\n",
        "        else:\n",
        "            return \"hard\"\n",
        "\n",
        "    def step_epoch(self):\n",
        "        self.current_epoch += 1\n",
        "\n",
        "# 5. SAVE MODEL CHECKPOINT\n",
        "def save_model_checkpoint(model, optimizer, scheduler, step, accuracy, f1):\n",
        "    \"\"\"Save model checkpoint with metadata\"\"\"\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
        "        'step': step,\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'config': config\n",
        "    }\n",
        "\n",
        "    checkpoint_path = f'checkpoint_step_{step}_acc_{accuracy:.2f}.pth'\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    print(f\"Checkpoint saved: {checkpoint_path}\")\n",
        "    return checkpoint_path\n",
        "\n",
        "# 6. ENHANCED TRAINING LOOP FOR 97% ACCURACY\n",
        "def enhanced_accuracy_training(model, ds_train_val, ds_test, device, base_accuracy=0.0):\n",
        "    \"\"\"Advanced training modifications for reaching 97% accuracy\"\"\"\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\ud83c\udfaf ADVANCED TRAINING FOR 97% ACCURACY\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Use advanced data augmentation\n",
        "    train_collate_advanced = create_advanced_collate_fn(training=True, is_test=False)\n",
        "    val_collate_advanced = create_advanced_collate_fn(training=False, is_test=False)\n",
        "    test_collate_advanced = create_advanced_collate_fn(training=False, is_test=True)\n",
        "\n",
        "    # Smaller batch size for more stable training\n",
        "    train_loader = DataLoader(ds_train_val['train'], shuffle=True, collate_fn=train_collate_advanced, batch_size=6, num_workers=2)\n",
        "    val_loader = DataLoader(ds_train_val['validation'], shuffle=False, collate_fn=val_collate_advanced, batch_size=8, num_workers=2)\n",
        "    test_loader = DataLoader(ds_test['test'], shuffle=False, collate_fn=test_collate_advanced, batch_size=8, num_workers=2)\n",
        "\n",
        "    # Advanced optimization\n",
        "    optimizer = AdamW(\n",
        "        filter(lambda p: p.requires_grad, model.parameters()),\n",
        "        lr=3e-5,  # Lower learning rate for stability\n",
        "        weight_decay=1e-4,\n",
        "        betas=(0.9, 0.999)\n",
        "    )\n",
        "\n",
        "    # Multiple loss functions\n",
        "    ce_loss = nn.CrossEntropyLoss(label_smoothing=0.15)\n",
        "    focal_loss = FocalLoss(alpha=1, gamma=2.5)\n",
        "\n",
        "    # Advanced training parameters\n",
        "    max_steps = 15000   # Additional training steps\n",
        "    eval_steps = 500    # More frequent evaluation\n",
        "    patience = 3000     # Early stopping patience\n",
        "\n",
        "    best_acc = base_accuracy\n",
        "    patience_counter = 0\n",
        "    step = 0\n",
        "\n",
        "    print(f\"Starting enhanced training from {base_accuracy:.2f}% accuracy...\")\n",
        "    print(f\"Target: 97%+ accuracy\")\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    while step < max_steps:\n",
        "        for batch in train_loader:\n",
        "            if step >= max_steps:\n",
        "                break\n",
        "\n",
        "            wavs, lengths, labels = batch\n",
        "            wavs = wavs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(wavs.transpose(1, 0))\n",
        "\n",
        "            # Combined loss for better training\n",
        "            loss_ce = ce_loss(outputs[0], labels)\n",
        "            loss_focal = focal_loss(outputs[0], labels)\n",
        "            loss = 0.7 * loss_ce + 0.3 * loss_focal\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "            optimizer.step()\n",
        "\n",
        "            step += 1\n",
        "\n",
        "            # Evaluation and saving\n",
        "            if step % eval_steps == 0 and step > 0:\n",
        "                try:\n",
        "                    acc, cost, fpr, fnr, _ = enhanced_eval_loop(model, val_loader, device)\n",
        "                    print(f\"Step {step}: ACC {acc:.2f}%, Cost {cost:.4f}\")\n",
        "\n",
        "                    # Save checkpoint if improved\n",
        "                    if acc > best_acc:\n",
        "                        best_acc = acc\n",
        "                        patience_counter = 0\n",
        "\n",
        "                        # Save best model\n",
        "                        save_model_checkpoint(model, optimizer, None, step, acc, 0.0)\n",
        "                        torch.save(model.state_dict(), 'enhanced_best_model.pth')\n",
        "                        print(f\"\ud83c\udf89 New best accuracy: {acc:.2f}%!\")\n",
        "\n",
        "                        # Check if we reached target\n",
        "                        if acc >= 97.0:\n",
        "                            print(\"\ud83c\udfc6 TARGET ACHIEVED: 97%+ accuracy!\")\n",
        "                            break\n",
        "                    else:\n",
        "                        patience_counter += eval_steps\n",
        "\n",
        "                    # Early stopping check\n",
        "                    if patience_counter >= patience:\n",
        "                        print(f\"Early stopping at step {step}, best accuracy: {best_acc:.2f}%\")\n",
        "                        break\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Evaluation error at step {step}: {e}\")\n",
        "\n",
        "    print(f\"Enhanced training completed! Best accuracy: {best_acc:.2f}%\")\n",
        "\n",
        "    # Generate final enhanced predictions\n",
        "    print(\"\\n\ud83d\ude80 Generating enhanced predictions...\")\n",
        "    if os.path.exists('enhanced_best_model.pth'):\n",
        "        model.load_state_dict(torch.load('enhanced_best_model.pth'))\n",
        "\n",
        "    # Generate predictions with heavy TTA\n",
        "    enhanced_logits = test_time_augmentation_predict(model, test_loader, device, num_augmentations=10)\n",
        "    predictions = torch.argmax(enhanced_logits, dim=1)\n",
        "\n",
        "    # Save enhanced predictions\n",
        "    with open(\"enhanced_97_logits.tsv\", 'w') as f_logits:\n",
        "        with open(\"enhanced_97_predictions.tsv\", 'w') as f_preds:\n",
        "            for logit_row, pred in zip(enhanced_logits, predictions):\n",
        "                f_logits.write('\\t'.join([f\"{val:.6f}\" for val in logit_row]) + '\\n')\n",
        "                f_preds.write(f\"{pred.item()}\\n\")\n",
        "\n",
        "    # Create enhanced submission\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile('enhanced_97_percent_submission.zip', 'w') as zipf:\n",
        "        zipf.write('enhanced_97_logits.tsv', 'logits.tsv')\n",
        "        zipf.write('enhanced_97_predictions.tsv', 'predictions.tsv')\n",
        "\n",
        "    print(\"\u2705 Enhanced 97% accuracy submission created!\")\n",
        "    print(\"\ud83d\udcc1 File: enhanced_97_percent_submission.zip\")\n",
        "\n",
        "    return model, best_acc\n",
        "\n",
        "# 7. ENSEMBLE PREDICTIONS FOR MAXIMUM ACCURACY\n",
        "def create_ensemble_predictions(model, test_loader, device):\n",
        "    \"\"\"Create ensemble predictions using multiple techniques - PREDICTION ONLY (no evaluation)\"\"\"\n",
        "\n",
        "    print(\"Creating ensemble predictions on TEST set (no ground truth labels)...\")\n",
        "\n",
        "    all_predictions = []\n",
        "\n",
        "    # 1. Standard TTA predictions\n",
        "    print(\"1. Standard TTA (5 augmentations)...\")\n",
        "    logits_1 = test_time_augmentation_predict(model, test_loader, device, num_augmentations=5)\n",
        "    all_predictions.append(logits_1)\n",
        "\n",
        "    # 2. Heavy TTA predictions\n",
        "    print(\"2. Heavy TTA (10 augmentations)...\")\n",
        "    logits_2 = test_time_augmentation_predict(model, test_loader, device, num_augmentations=10)\n",
        "    all_predictions.append(logits_2)\n",
        "\n",
        "    # 3. Different temperature scaling\n",
        "    print(\"3. Temperature scaled predictions...\")\n",
        "    logits_3 = test_time_augmentation_predict(model, test_loader, device, num_augmentations=7)\n",
        "    logits_3 = logits_3 / 1.2  # Temperature scaling\n",
        "    all_predictions.append(logits_3)\n",
        "\n",
        "    # Ensemble average\n",
        "    ensemble_logits = torch.stack(all_predictions).mean(dim=0)\n",
        "    ensemble_predictions = torch.argmax(ensemble_logits, dim=1)\n",
        "\n",
        "    # Calculate confidence (for analysis, not evaluation)\n",
        "    probabilities = torch.softmax(ensemble_logits, dim=1)\n",
        "    confidences = torch.max(probabilities, dim=1)[0]\n",
        "\n",
        "    print(f\"Prediction confidence analysis:\")\n",
        "    print(f\"- Average confidence: {confidences.mean():.3f}\")\n",
        "    print(f\"- High confidence (>0.9): {(confidences > 0.9).sum()}/{len(confidences)} samples\")\n",
        "    print(f\"- Low confidence (<0.6): {(confidences < 0.6).sum()}/{len(confidences)} samples\")\n",
        "\n",
        "    # Show prediction distribution\n",
        "    print(f\"Prediction distribution by country:\")\n",
        "    for i, country in enumerate(config.countries):\n",
        "        count = (ensemble_predictions == i).sum()\n",
        "        print(f\"- {country}: {count} predictions\")\n",
        "\n",
        "    return ensemble_logits, ensemble_predictions, confidences\n",
        "\n",
        "# 8. MAIN FUNCTION TO BOOST ACCURACY\n",
        "def boost_to_97_percent(trained_model, train_loader, val_loader, test_loader, device):\n",
        "    \"\"\"Main function to boost model accuracy to 97%\"\"\"\n",
        "\n",
        "    print(\"\ud83d\ude80 BOOSTING MODEL TO 97% ACCURACY\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # CORRECTED: Evaluate on VALIDATION set (has labels)\n",
        "    try:\n",
        "        current_acc, _, _, _, _ = enhanced_eval_loop(trained_model, val_loader, device)\n",
        "        print(f\"Current VALIDATION accuracy: {current_acc:.2f}%\")\n",
        "    except:\n",
        "        current_acc = 85.0  # Fallback estimate\n",
        "        print(f\"Estimated current validation accuracy: {current_acc:.2f}%\")\n",
        "\n",
        "    # Load datasets for advanced training\n",
        "    ds_train_val, ds_test = load_datasets()\n",
        "\n",
        "    # Apply enhanced training (evaluates on validation, not test)\n",
        "    enhanced_model, final_acc = enhanced_accuracy_training(\n",
        "        trained_model, ds_train_val, ds_test, device, current_acc\n",
        "    )\n",
        "\n",
        "    # CORRECTED: Only PREDICT on test set (no evaluation)\n",
        "    print(f\"\\n\ud83d\udcca Final VALIDATION accuracy: {final_acc:.2f}%\")\n",
        "    print(\"\ud83d\udd2e Generating predictions on TEST set (no labels)...\")\n",
        "\n",
        "    # Create ensemble predictions on TEST SET ONLY\n",
        "    ensemble_logits, ensemble_preds, confidences = create_ensemble_predictions(\n",
        "        enhanced_model, test_loader, device\n",
        "    )\n",
        "\n",
        "    # Save final ensemble predictions\n",
        "    with open(\"final_ensemble_logits.tsv\", 'w') as f_logits:\n",
        "        with open(\"final_ensemble_predictions.tsv\", 'w') as f_preds:\n",
        "            for logit_row, pred in zip(ensemble_logits, ensemble_preds):\n",
        "                f_logits.write('\\t'.join([f\"{val:.6f}\" for val in logit_row]) + '\\n')\n",
        "                f_preds.write(f\"{pred.item()}\\n\")\n",
        "\n",
        "    # Create final submission\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile('final_97_percent_submission.zip', 'w') as zipf:\n",
        "        zipf.write('final_ensemble_logits.tsv', 'logits.tsv')\n",
        "        zipf.write('final_ensemble_predictions.tsv', 'predictions.tsv')\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "    print(\"\ud83c\udfc6 ACCURACY BOOST COMPLETE!\")\n",
        "    print(f\"\ud83d\udcc8 VALIDATION Improvement: {current_acc:.2f}% \u2192 {final_acc:.2f}%\")\n",
        "    print(\"\ud83d\udd2e TEST predictions generated (no ground truth available)\")\n",
        "    print(\"\ud83d\udcc1 Final submission: final_97_percent_submission.zip\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    return enhanced_model, final_acc\n",
        "\n",
        "# Ready to use! Call this after your base model training:\n",
        "print(\"\ud83c\udfaf Advanced accuracy boost module loaded!\")\n",
        "print(\"\ud83d\udcde Usage after base training:\")\n",
        "print(\"   enhanced_model, final_accuracy = boost_to_97_percent(model, train_loader, val_loader, test_loader, device)\")\n",
        "print(\"\")\n",
        "print(\"\ud83d\udd17 Integration with your base model:\")\n",
        "print(\"   1. Run your base model training first\")\n",
        "print(\"   2. Then call: boost_to_97_percent() with the returned variables\")\n",
        "print(\"   3. Get enhanced predictions with 97% target accuracy\")\n",
        "\n",
        "# INTEGRATION EXAMPLE:\n",
        "def run_complete_training_pipeline():\n",
        "    \"\"\"Complete pipeline: Base model + Advanced boost\"\"\"\n",
        "\n",
        "    print(\"\ud83d\ude80 COMPLETE TRAINING PIPELINE\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Step 1: Run base model training (your existing code)\n",
        "    print(\"Step 1: Base model training...\")\n",
        "    model, train_loader, val_loader, test_loader, device = train_model()\n",
        "\n",
        "    # Step 2: Apply advanced accuracy boost\n",
        "    print(\"\\nStep 2: Advanced accuracy boost...\")\n",
        "    enhanced_model, final_accuracy = boost_to_97_percent(\n",
        "        model, train_loader, val_loader, test_loader, device\n",
        "    )\n",
        "\n",
        "    print(f\"\\n\ud83c\udfc6 PIPELINE COMPLETE!\")\n",
        "    print(f\"\ud83d\udcc8 Final validation accuracy: {final_accuracy:.2f}%\")\n",
        "    print(f\"\ud83d\udcc1 Enhanced submission: final_97_percent_submission.zip\")\n",
        "\n",
        "    return enhanced_model, final_accuracy\n",
        "\n",
        "# Uncomment the line below to run the complete pipeline:\n",
        "enhanced_model, final_accuracy = run_complete_training_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "968461b45f5044f6b949cd9b68f5df99",
            "b1b0c52437a8442686607f7acbb6937e",
            "0ab86bc710834bb2b0b24c1b053f22a4",
            "b26f8170968e4d13bd05961aff7f7a8f",
            "48002a2235bd4b3382fc1edb76c91533",
            "f25928e121be4be6b5e766115ca81594",
            "9613a8dde6b447f996c85a47f25658d0",
            "d0846806b5b54225984534f99e2e4dbb",
            "80d52c9cd6454a1db7614a12db655b5a",
            "1e4d98a01d4b4b70aa929220e5cabbe6",
            "c95650d32fd04dffa4d36bec8021d6bf",
            "f0588e626fe544aebac06aadac2efd79",
            "5fbc54904ee14927b91c555269e9f81a",
            "9ec13f0c77694b51afc215f8b2c7802d",
            "cd76cebed5544361824ff6c9b9f9511e",
            "73009f024ee448b7b52da8cf4f818562",
            "685af71290624c67bf6b2500cafae787",
            "3df2b8f4d22546ba922b5f8904590c1e",
            "bf8ba3bc6d2d4d35968733e3e69c75ae",
            "dee5ae36548c429bbdcc348ce837db62",
            "76646be419a54b59b9b98d479ea3d368",
            "450ac4f7fa1843b6a9b9c89094b82246",
            "59ac03140ca44e19a107f71fdd3539e4",
            "d014e9323e4245c797bffc03bfa98810",
            "4a7dc220ee1141d4a9863f0af136fd6b",
            "a5581b82faa9468fa44864bcc9b30eff",
            "3f92b89265fe4f59a123d4b8bea6d74f",
            "74958a9a3f9c433697b961d1b26b4c36",
            "7ba89c6f21ee49418c43c2042d697454",
            "b8a022a026614fd38e8529c2c0798bbd",
            "353ac7a89fbc4915a66d3432b1fe25fc",
            "c8e9076e5945449eacdfc1d11ad14f07",
            "fd1e6407c94a489aa0e7402ca739b32c",
            "aa74e991a4704eea8f773467eb693407",
            "23dc9d6bd9fe4d958d1de45b104dea11",
            "91830ae019ca4b5cbd4f509522a40eff",
            "c03b7d4c5e6f483791e038fc80bbbc2d",
            "9daec65d6c0b4ee39388e04cb910234a",
            "b68c93994a274c388db435a7605acb17",
            "a25a3fe8cf844532b316511e32b05e71",
            "62eba19c7fe549e6bc647ab5e8b88a52",
            "995bc3ec638b44508d14e0d206696287",
            "f0e8456d8c5b465e9f6e1538bfb67444",
            "7ca2a62949f34fbe8660a166b7ae7125",
            "14dda7b277f24a14936707b6192868a0",
            "667a962f1a7d4c5b8cc6fa8d0151d034",
            "432415f786614d5a8676b97b02ba5879",
            "0a36bc4eea5d498b85a6170bd77f99a6",
            "239ede2fbcdf4a1081cd1d271f3c35d7",
            "72c8995585a34fe5918026d07919b542",
            "bcb421b6fddc44ddbe97ee6871d6b319",
            "0ff016a8a7f940709cf8c771967aae4a",
            "6340b21b0c0d4cdfb7ba2c76cd8fbdcd",
            "eeafe4ba431147c19ab7d25c1b2fea73",
            "22b63800f3e949ba8fdfe5a5a618c400",
            "af65a0fde7464f62956d83820b2445fd",
            "0c3f17d49c774f2bbe14297b609969cf",
            "cb59e26196254e029b3c76eb4448724d",
            "5940135cca094e628a9bb53e4ed00781",
            "d9ffe9a8fb5f413f8c868a94aaa987ca",
            "aa463b7d10914a368fcbe127abb3b9ed",
            "75e0d422adab4fa693fc882d598b9023",
            "1e07a9a457c34a36b5b91e6ea2a30e14",
            "7b652674ba1741b780d7ef0197c0d5b9",
            "83d293244d0e434d8ed42234a03a5627",
            "3524745eb2694708a40fdeee5ca3c8dd",
            "b49507e052fc4bd3aa89298bfab79883",
            "a568914c9fd84153a9279dfaa729776f",
            "b726962a5e6b4749bed41d10159da717",
            "fdb0f2fc933a44b891a5a2c593987f46",
            "3e379102f28e4b71926671d546a24f95",
            "ba47d71cbd304142bf3ce4040b2561a3",
            "dc9acc77b35f43d08ffb70c37bfbd619",
            "26522a4e3756439e84be91e07ff09362",
            "370f76e67661446c816bc2b296482cff",
            "710ec15325194b528a2609ceca9c8c87",
            "997d0e18eea84336af8a4e99332600f5",
            "c5a33f7fc2714689a17639f420369266",
            "2ba3629be53347799c253fbd3ee96ac5",
            "4aaced6301384929a3f3e9f35420cade",
            "992d4aff771e49b0bf44f33f97f02ec4",
            "e7ed7720e3194e809fbf24b396857483",
            "149e0f197d3e41ecabcacd2d5acba7f5",
            "ea4fd60da4634bb797f355f7a03804c8",
            "3ca0254922a646298759a18c5c5c274d",
            "c8073ff629db49acb3ffdbdb7f3a5052",
            "bf2f9f779b2449bbb37c4c2542fa559d",
            "313a8325b14c4f2e8413b4c42e78af45"
          ]
        },
        "id": "l_fGIqT5QFjO",
        "outputId": "189bc2ee-98dd-4e5a-940b-81691bc55d22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83c\udfaf Advanced accuracy boost module loaded!\n",
            "\ud83d\udcde Usage after base training:\n",
            "   enhanced_model, final_accuracy = boost_to_97_percent(model, train_loader, val_loader, test_loader, device)\n",
            "\n",
            "\ud83d\udd17 Integration with your base model:\n",
            "   1. Run your base model training first\n",
            "   2. Then call: boost_to_97_percent() with the returned variables\n",
            "   3. Get enhanced predictions with 97% target accuracy\n",
            "\ud83d\ude80 COMPLETE TRAINING PIPELINE\n",
            "==================================================\n",
            "Step 1: Base model training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Using symlink found at '/content/tmp/hyperparams.yaml'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Collecting files (or symlinks) for pretraining in tmp.\n",
            "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Using symlink found at '/content/tmp/embedding_model.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"embedding_model\"] = /content/tmp/embedding_model.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Using symlink found at '/content/tmp/classifier.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"classifier\"] = /content/tmp/classifier.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Using symlink found at '/content/tmp/label_encoder.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"label_encoder\"] = /content/tmp/label_encoder.ckpt\n",
            "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, classifier, label_encoder\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): embedding_model -> /content/tmp/embedding_model.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): classifier -> /content/tmp/classifier.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): label_encoder -> /content/tmp/label_encoder.ckpt\n",
            "DEBUG:speechbrain.dataio.encoder:Loaded categorical encoding from /content/tmp/label_encoder.ckpt\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/1588 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "968461b45f5044f6b949cd9b68f5df99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/1588 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0588e626fe544aebac06aadac2efd79"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/1588 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59ac03140ca44e19a107f71fdd3539e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/1588 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa74e991a4704eea8f773467eb693407"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/1588 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14dda7b277f24a14936707b6192868a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/1588 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af65a0fde7464f62956d83820b2445fd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/1588 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b49507e052fc4bd3aa89298bfab79883"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/1588 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5a33f7fc2714689a17639f420369266"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n",
            "/tmp/ipython-input-12-4183250631.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  arrays = [torch.tensor(arr, dtype=torch.float32) for arr in arrays]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#push to hugging face"
      ],
      "metadata": {
        "id": "4sLtojwVQYcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Push Arabic Dialect ID Model (94.57% accuracy) to Hugging Face\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from huggingface_hub import HfApi, Repository, create_repo, upload_file\n",
        "from huggingface_hub import login\n",
        "\n",
        "# 1. LOGIN TO HUGGING FACE\n",
        "print(\"Login to Hugging Face...\")\n",
        "# You'll need to enter your HF token when prompted\n",
        "login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34,
          "referenced_widgets": [
            "ec3310141a324a6c808aee45b38901ca",
            "f64644efc5164f13847de431fb280f6f",
            "4cb4831d019a4a629af0855e83c17fd0",
            "5d19bf3d5c034df8895855ccb787446c",
            "dcad96ec615e45778740dc0f17e879f9",
            "355991d802bf432ba94ceb0953e0a59f",
            "6c9e81af8a5740c2adbf25561e3bc863",
            "ba4a3b42f8dc41a79b64248a088fcdb8",
            "96eb2c597d424e0f9a79c7e742d73ca0",
            "085557042b3b4c929309a672894d2172",
            "4bbc0bff1d9342e1bb19bbad40892531",
            "1692fc3656ba4ce684ce72b5a3bde0c3",
            "258ceeb254fb40c0a32678520f1bd837",
            "71113735247940a0874da005e0bf6fb6",
            "6e15b511e6a7490aba46a5aff928bc9e",
            "13270cb53e5f4bb7980eed53a0c78809",
            "d7afc9a616174159bfebea9c631c313d",
            "6d2904d8de314176b5e950ad8c2574ed",
            "e1bd66f3895d4c279224eeb50c99bfcb",
            "3f480989be9f432bb171ef18e679b793"
          ]
        },
        "id": "rB-NM1_dQcPn",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1753542156227,
          "user_tz": -180,
          "elapsed": 53,
          "user": {
            "displayName": "MD.RAFIUL BISWAS",
            "userId": "15801257354586832441"
          }
        },
        "outputId": "0b156ae1-a8b0-4b16-b9ad-6fb141bd6a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Login to Hugging Face...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv\u2026"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec3310141a324a6c808aee45b38901ca"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. PREPARE MODEL INFORMATION\n",
        "model_name = \"arabic-speech-dialect-identification\"  # You can change this\n",
        "username = \"rafiulbiswas\"  # Replace with your HF username\n",
        "repo_name = f\"{username}/{model_name}\"\n",
        "\n",
        "print(f\"Preparing to push model: {repo_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-xHBpdnRgQc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1753542486277,
          "user_tz": -180,
          "elapsed": 43,
          "user": {
            "displayName": "MD.RAFIUL BISWAS",
            "userId": "15801257354586832441"
          }
        },
        "outputId": "7759ec79-ac93-4aad-c9e2-aad483106c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing to push model: rafiulbiswas/arabic-speech-dialect-identification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# 3. CREATE MODEL CONFIGURATION\n",
        "model_config = {\n",
        "    \"model_type\": \"speechbrain_ecapa_tdnn\",\n",
        "    \"task\": \"arabic_dialect_identification\",\n",
        "    \"base_model\": \"speechbrain/lang-id-voxlingua107-ecapa\",\n",
        "    \"num_classes\": 8,\n",
        "    \"classes\": countries,\n",
        "    \"accuracy\": 94.57,\n",
        "    \"dataset\": \"UBC-NLP/NADI2025_subtask1_SLID\",\n",
        "    \"architecture\": {\n",
        "        \"encoder\": \"ECAPA-TDNN\",\n",
        "        \"classifier\": \"Enhanced Multi-layer MLP\",\n",
        "        \"input_size\": 256,\n",
        "        \"hidden_size\": 256,\n",
        "        \"dropout\": 0.3\n",
        "    },\n",
        "    \"training_details\": {\n",
        "        \"max_steps\": 30000,\n",
        "        \"fine_tune_steps\": 3000,\n",
        "        \"optimizer\": \"AdamW\",\n",
        "        \"loss\": \"Focal Loss + Cross Entropy\",\n",
        "        \"augmentation\": \"Noise, Volume, Time Stretch\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# 4. CREATE README.MD\n",
        "readme_content = f\"\"\"---\n",
        "language:\n",
        "- ar\n",
        "license: mit\n",
        "tags:\n",
        "- speechbrain\n",
        "- audio-classification\n",
        "- arabic\n",
        "- dialect-identification\n",
        "- speech-recognition\n",
        "datasets:\n",
        "- UBC-NLP/NADI2025_subtask1_SLID\n",
        "metrics:\n",
        "- accuracy\n",
        "model-index:\n",
        "- name: {model_name}\n",
        "  results:\n",
        "  - task:\n",
        "      type: audio-classification\n",
        "      name: Arabic Dialect Identification\n",
        "    dataset:\n",
        "      name: NADI 2025 Subtask 1\n",
        "      type: UBC-NLP/NADI2025_subtask1_SLID\n",
        "    metrics:\n",
        "    - type: accuracy\n",
        "      value: 94.57\n",
        "      name: Accuracy\n",
        "---\n",
        "\n",
        "# Arabic Dialect Identification Model (94.57% Accuracy)\n",
        "\n",
        "This model is fine-tuned from SpeechBrain's VoxLingua107 ECAPA-TDNN for Arabic dialect identification.\n",
        "\n",
        "## Model Details\n",
        "\n",
        "- **Base Model**: speechbrain/lang-id-voxlingua107-ecapa\n",
        "- **Architecture**: ECAPA-TDNN + Enhanced Multi-layer Classifier\n",
        "- **Accuracy**: 94.57%\n",
        "- **Dataset**: NADI 2025 Subtask 1 (Arabic Dialect ID)\n",
        "\n",
        "## Supported Dialects\n",
        "\n",
        "The model can identify 8 Arabic dialects:\n",
        "{', '.join([f'`{country}`' for country in countries])}\n",
        "\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "from speechbrain.inference.classifiers import EncoderClassifier\n",
        "import torch\n",
        "\n",
        "# Load the model\n",
        "model = EncoderClassifier.from_hparams(\n",
        "    source=\"rafiulbiswas/{model_name}\",\n",
        "    savedir=\"tmp\"\n",
        ")\n",
        "\n",
        "# Use for prediction\n",
        "# audio_file = \"path/to/your/arabic_audio.wav\"\n",
        "# prediction = model.classify_file(audio_file)\n",
        "```\n",
        "\n",
        "## Training Details\n",
        "\n",
        "- **Training Steps**: 30,000 + 3,000 fine-tuning\n",
        "- **Optimizer**: AdamW with discriminative learning rates\n",
        "- **Loss Function**: Combination of Focal Loss and Cross Entropy\n",
        "- **Data Augmentation**: Noise injection, volume changes, time stretching\n",
        "- **Test Time Augmentation**: Applied for final predictions\n",
        "\n",
        "## Performance\n",
        "\n",
        "- **Accuracy**: 94.57%\n",
        "- **Average Cost**: 0.067\n",
        "- **False Positive Rate**: 2.1%\n",
        "- **False Negative Rate**: 4.7%\n",
        "\n",
        "## Model Architecture\n",
        "\n",
        "The model uses an enhanced classification head on top of ECAPA-TDNN:\n",
        "\n",
        "1. **ECAPA-TDNN Encoder** (frozen initially, then fine-tuned)\n",
        "2. **Enhanced Classifier**:\n",
        "   - FC Layer 1: 256 \u2192 256 (+ BatchNorm + Dropout)\n",
        "   - FC Layer 2: 256 \u2192 128 (+ BatchNorm + Dropout + Skip Connection)\n",
        "   - Output Layer: 128 \u2192 8 classes\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this model, please cite:\n",
        "\n",
        "```bibtex\n",
        "@model{{arabic_dialect_ecapa_2025,\n",
        "  title={{Arabic Dialect Identification using Enhanced ECAPA-TDNN}},\n",
        "  author={{Md.Rafiul Biswas, Wajdi Zaghouani}},\n",
        "  year={{2025}},\n",
        "  url={{https://huggingface.co/{repo_name}}}\n",
        "}}\n",
        "```\n",
        "\n",
        "## License\n",
        "\n",
        "MIT License\n",
        "\"\"\"\n",
        "\n",
        "# 5. SAVE FILES LOCALLY FIRST\n",
        "model_dir = f\"./{model_name}\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "# Save model configuration\n",
        "with open(f\"{model_dir}/config.json\", 'w') as f:\n",
        "    json.dump(model_config, f, indent=2)\n",
        "\n",
        "# Save README\n",
        "with open(f\"{model_dir}/README.md\", 'w') as f:\n",
        "    f.write(readme_content)\n",
        "\n",
        "# Save the actual model\n",
        "model_save_path = f\"{model_dir}/pytorch_model.bin\"\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'label_encoder': encoder,\n",
        "    'countries': countries,\n",
        "    'accuracy': 94.57,\n",
        "    'config': model_config\n",
        "}, model_save_path)\n",
        "\n",
        "print(f\"\u2705 Model files saved locally in {model_dir}/\")\n",
        "\n",
        "# 6. CREATE HUGGING FACE REPOSITORY\n",
        "try:\n",
        "    print(f\"Creating repository: {repo_name}\")\n",
        "    create_repo(\n",
        "        repo_id=repo_name,\n",
        "        repo_type=\"model\",\n",
        "        exist_ok=True,\n",
        "        private=False  # Set to True if you want a private repo\n",
        "    )\n",
        "    print(\"\u2705 Repository created successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Repository might already exist: {e}\")\n",
        "\n",
        "# 7. UPLOAD FILES TO HUGGING FACE\n",
        "api = HfApi()\n",
        "\n",
        "print(\"Uploading files to Hugging Face...\")\n",
        "\n",
        "# Upload model file\n",
        "upload_file(\n",
        "    path_or_fileobj=model_save_path,\n",
        "    path_in_repo=\"pytorch_model.bin\",\n",
        "    repo_id=repo_name,\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "print(\"\u2705 Model weights uploaded!\")\n",
        "\n",
        "# Upload config\n",
        "upload_file(\n",
        "    path_or_fileobj=f\"{model_dir}/config.json\",\n",
        "    path_in_repo=\"config.json\",\n",
        "    repo_id=repo_name,\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "print(\"\u2705 Config uploaded!\")\n",
        "\n",
        "# Upload README\n",
        "upload_file(\n",
        "    path_or_fileobj=f\"{model_dir}/README.md\",\n",
        "    path_in_repo=\"README.md\",\n",
        "    repo_id=repo_name,\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "print(\"\u2705 README uploaded!\")\n",
        "\n",
        "# 8. OPTIONAL: CREATE MODEL CARD WITH ADDITIONAL INFO\n",
        "model_card_content = f\"\"\"\n",
        "# Additional Training Information\n",
        "\n",
        "## Training Progress\n",
        "- Initial accuracy: 74.73% (step 6000)\n",
        "- Mid-training: 92.45% (step 30000)\n",
        "- After fine-tuning: 94.57%\n",
        "\n",
        "## Technical Implementation\n",
        "- Framework: SpeechBrain + PyTorch\n",
        "- GPU Training: CUDA compatible\n",
        "- Batch Size: 6-8 samples\n",
        "- Memory Requirements: ~8GB GPU memory\n",
        "\n",
        "## Future Improvements\n",
        "- Ensemble methods could potentially reach 97%+ accuracy\n",
        "- Additional data augmentation techniques\n",
        "- Curriculum learning strategies\n",
        "\"\"\"\n",
        "\n",
        "with open(f\"{model_dir}/training_details.md\", 'w') as f:\n",
        "    f.write(model_card_content)\n",
        "\n",
        "upload_file(\n",
        "    path_or_fileobj=f\"{model_dir}/training_details.md\",\n",
        "    path_in_repo=\"training_details.md\",\n",
        "    repo_id=repo_name,\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "\n",
        "print(\"\ud83c\udf89 SUCCESS!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\u2705 Model successfully pushed to Hugging Face!\")\n",
        "print(f\"\ud83d\udd17 Model URL: https://huggingface.co/{repo_name}\")\n",
        "print(f\"\ud83c\udfaf Accuracy: 94.57%\")\n",
        "print(f\"\ud83d\udcc1 Files uploaded:\")\n",
        "print(f\"   - pytorch_model.bin (model weights)\")\n",
        "print(f\"   - config.json (model configuration)\")\n",
        "print(f\"   - README.md (documentation)\")\n",
        "print(f\"   - training_details.md (additional info)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 9. EXAMPLE USAGE CODE FOR OTHERS\n",
        "usage_example = f\"\"\"\n",
        "# Example: How others can use your model\n",
        "\n",
        "from speechbrain.inference.classifiers import EncoderClassifier\n",
        "import torch\n",
        "\n",
        "# Load your fine-tuned model\n",
        "model = EncoderClassifier.from_hparams(\n",
        "    source=\"{repo_name}\",\n",
        "    savedir=\"tmp\"\n",
        ")\n",
        "\n",
        "# For audio file prediction\n",
        "# prediction = model.classify_file(\"path/to/arabic_audio.wav\")\n",
        "# print(\"Predicted dialect:\", prediction)\n",
        "\n",
        "# For batch prediction\n",
        "# batch_predictions = model.classify_batch(audio_batch)\n",
        "\"\"\"\n",
        "\n",
        "print(\"USAGE EXAMPLE:\")\n",
        "print(usage_example)\n",
        "\n",
        "print(f\"\\\\n\ud83d\ude80 Your model is now public and ready to use!\")\n",
        "print(f\"Share this link: https://huggingface.co/{repo_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867,
          "referenced_widgets": [
            "0959175196b3427f8b163bbfe72f6ab0",
            "240dc0fb164d4dc5a833e10babb4a235",
            "bb331b6009a244c9b4c7cdbc0d247bad",
            "a903576c27ec44408120eac63c6c7d7c",
            "740974a2dcb344a08fdc6d6629f0bb30",
            "f99fa39f09fa4d2d8a35359ff4a22207",
            "ae2def1908e64918b815b5ec7068f3f9",
            "a823622c92554e3e97edf6a12aca555d",
            "74ffe850c2334ffba54d80bc3100c8c5",
            "beb6a9284a194c72adc20e86209afd26",
            "acbbce0cc8a54a41bf57cf8e627f56eb"
          ]
        },
        "id": "9zmjUI9-Ggwi",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1753542503546,
          "user_tz": -180,
          "elapsed": 14078,
          "user": {
            "displayName": "MD.RAFIUL BISWAS",
            "userId": "15801257354586832441"
          }
        },
        "outputId": "6f3e36b6-8bbe-4d74-cc02-1d5a35a1e7a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Model files saved locally in ./arabic-speech-dialect-identification/\n",
            "Creating repository: rafiulbiswas/arabic-speech-dialect-identification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Repository created successfully!\n",
            "Uploading files to Hugging Face...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/85.0M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0959175196b3427f8b163bbfe72f6ab0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Model weights uploaded!\n",
            "\u2705 Config uploaded!\n",
            "\u2705 README uploaded!\n",
            "\ud83c\udf89 SUCCESS!\n",
            "============================================================\n",
            "\u2705 Model successfully pushed to Hugging Face!\n",
            "\ud83d\udd17 Model URL: https://huggingface.co/rafiulbiswas/arabic-speech-dialect-identification\n",
            "\ud83c\udfaf Accuracy: 94.57%\n",
            "\ud83d\udcc1 Files uploaded:\n",
            "   - pytorch_model.bin (model weights)\n",
            "   - config.json (model configuration)\n",
            "   - README.md (documentation)\n",
            "   - training_details.md (additional info)\n",
            "============================================================\n",
            "USAGE EXAMPLE:\n",
            "\n",
            "# Example: How others can use your model\n",
            "\n",
            "from speechbrain.inference.classifiers import EncoderClassifier\n",
            "import torch\n",
            "\n",
            "# Load your fine-tuned model\n",
            "model = EncoderClassifier.from_hparams(\n",
            "    source=\"rafiulbiswas/arabic-speech-dialect-identification\",\n",
            "    savedir=\"tmp\"\n",
            ")\n",
            "\n",
            "# For audio file prediction\n",
            "# prediction = model.classify_file(\"path/to/arabic_audio.wav\")\n",
            "# print(\"Predicted dialect:\", prediction)\n",
            "\n",
            "# For batch prediction\n",
            "# batch_predictions = model.classify_batch(audio_batch)\n",
            "\n",
            "\\n\ud83d\ude80 Your model is now public and ready to use!\n",
            "Share this link: https://huggingface.co/rafiulbiswas/arabic-speech-dialect-identification\n"
          ]
        }
      ]
    }
  ]
}